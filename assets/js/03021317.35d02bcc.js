"use strict";(self.webpackChunklinkis_web_apache=self.webpackChunklinkis_web_apache||[]).push([[73464],{3905:(e,n,t)=>{t.d(n,{Zo:()=>c,kt:()=>m});var i=t(67294);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);n&&(i=i.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,i)}return t}function r(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?o(Object(t),!0).forEach((function(n){a(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,i,a=function(e,n){if(null==e)return{};var t,i,a={},o=Object.keys(e);for(i=0;i<o.length;i++)t=o[i],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(i=0;i<o.length;i++)t=o[i],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var l=i.createContext({}),p=function(e){var n=i.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):r(r({},n),e)),t},c=function(e){var n=p(e.components);return i.createElement(l.Provider,{value:n},e.children)},d="mdxType",h={inlineCode:"code",wrapper:function(e){var n=e.children;return i.createElement(i.Fragment,{},n)}},u=i.forwardRef((function(e,n){var t=e.components,a=e.mdxType,o=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),d=p(t),u=a,m=d["".concat(l,".").concat(u)]||d[u]||h[u]||o;return t?i.createElement(m,r(r({ref:n},c),{},{components:t})):i.createElement(m,r({ref:n},c))}));function m(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var o=t.length,r=new Array(o);r[0]=u;var s={};for(var l in n)hasOwnProperty.call(n,l)&&(s[l]=n[l]);s.originalType=e,s[d]="string"==typeof e?e:a,r[1]=s;for(var p=2;p<o;p++)r[p]=t[p];return i.createElement.apply(null,r)}return i.createElement.apply(null,t)}u.displayName="MDXCreateElement"},85826:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>p});var i=t(87462),a=(t(67294),t(3905));const o={title:"Linkis Deployment Troubleshooting"},r=void 0,s={permalink:"/blog/2022/02/21/linkis-deploy",editUrl:"https://github.com/apache/linkis-website/edit/dev/blog/2022-02-21-linkis-deploy.md",source:"@site/blog/2022-02-21-linkis-deploy.md",title:"Linkis Deployment Troubleshooting",description:"Linkis deployment instructions and precautions",date:"2022-02-21T00:00:00.000Z",formattedDate:"February 21, 2022",tags:[],readingTime:14.57,hasTruncateMarker:!1,authors:[],frontMatter:{title:"Linkis Deployment Troubleshooting"},prevItem:{title:"Implementation of OpenLookEng Engine",permalink:"/blog/2022/03/20/openlookeng"},nextItem:{title:"How to Write a Blog",permalink:"/blog/2022/02/08/how-to-user-blog"}},l={authorsImageUrls:[]},p=[{value:"1. Precautions for preparation",id:"1-precautions-for-preparation",level:2},{value:"1.1 linux server",id:"11-linux-server",level:3},{value:"1.2 Add deployment user",id:"12-add-deployment-user",level:3},{value:"1.3 Installation package preparation",id:"13-installation-package-preparation",level:3},{value:"1.4 Low-level dependency checking",id:"14-low-level-dependency-checking",level:3},{value:"1.5 Resource dependencies",id:"15-resource-dependencies",level:3},{value:"2. Install",id:"2-install",level:2},{value:"2.1 Unzip the installation package",id:"21-unzip-the-installation-package",level:3},{value:"2.2 Configure linkis database information",id:"22-configure-linkis-database-information",level:3},{value:"2.3 Configure basic environment variables",id:"23-configure-basic-environment-variables",level:3},{value:"Basic directory configuration",id:"basic-directory-configuration",level:4},{value:"META configuration for HIVE",id:"meta-configuration-for-hive",level:4},{value:"Yarn&#39;s ResourceManager address",id:"yarns-resourcemanager-address",level:4},{value:"LDAP login authentication",id:"ldap-login-authentication",level:4},{value:"Basic component environment information",id:"basic-component-environment-information",level:4},{value:"Engine version information",id:"engine-version-information",level:4},{value:"jvm memory configuration",id:"jvm-memory-configuration",level:4},{value:"Installation directory configuration",id:"installation-directory-configuration",level:4},{value:"3. Deployment process",id:"3-deployment-process",level:2},{value:"3.1 Execute the deployment script",id:"31-execute-the-deployment-script",level:3},{value:"3.2 Possible problems",id:"32-possible-problems",level:3},{value:"1.Permission problem mkdir: cannot create directory \u2018xxxx\u2019: Permission denied",id:"1permission-problem-mkdir-cannot-create-directory-xxxx-permission-denied",level:4},{value:"3.3 Configuration modification",id:"33-configuration-modification",level:3},{value:"3.4 Add mysql driver (&gt;=1.0.3) version",id:"34-add-mysql-driver-103-version",level:3},{value:"3.5 Start the service",id:"35-start-the-service",level:3},{value:"3.6 Check whether the service starts normally",id:"36-check-whether-the-service-starts-normally",level:3},{value:"4. Install the web frontend",id:"4-install-the-web-frontend",level:2},{value:"1 Check whether the yarn address is configured correctly",id:"1-check-whether-the-yarn-address-is-configured-correctly",level:4},{value:"2 Check whether the yarn queue is correct",id:"2-check-whether-the-yarn-queue-is-correct",level:4},{value:"5. Check if the engine material resource is uploaded successfully",id:"5-check-if-the-engine-material-resource-is-uploaded-successfully",level:2},{value:"6. Troubleshooting installation and deployment common problems",id:"6-troubleshooting-installation-and-deployment-common-problems",level:2},{value:"6.1 Version compatibility issues",id:"61-version-compatibility-issues",level:3},{value:"6.2 How to locate the server exception log",id:"62-how-to-locate-the-server-exception-log",level:3},{value:"6.3 Execution of abnormal troubleshooting of engine tasks",id:"63-execution-of-abnormal-troubleshooting-of-engine-tasks",level:3},{value:"6.4 Notes on CDH adaptation version",id:"64-notes-on-cdh-adaptation-version",level:3},{value:"6.6 Debugging of Http interface",id:"66-debugging-of-http-interface",level:3},{value:"6.7 Troubleshooting process for abnormal problems",id:"67-troubleshooting-process-for-abnormal-problems",level:3},{value:"7. How to obtain relevant information",id:"7-how-to-obtain-relevant-information",level:2}],c={toc:p},d="wrapper";function h(e){let{components:n,...t}=e;return(0,a.kt)(d,(0,i.Z)({},c,t,{components:n,mdxType:"MDXLayout"}),(0,a.kt)("blockquote",null,(0,a.kt)("p",{parentName:"blockquote"},"Linkis deployment instructions and precautions")),(0,a.kt)("h2",{id:"1-precautions-for-preparation"},"1. Precautions for preparation"),(0,a.kt)("h3",{id:"11-linux-server"},"1.1 linux server"),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Hardware requirements"),(0,a.kt)("br",{parentName:"p"}),"\n","Install nearly 10 linkis microservices. The default configuration of each microservice is to start the jvm-Xmx memory size of 512M (if the memory is not enough, you can try to reduce it to 256/128M, and you can also increase it if the memory is sufficient)"),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Software Requirements"),(0,a.kt)("br",{parentName:"p"}),"\n","basic software environment\nUse the following command to check whether the corresponding software has been installed, if not, please install it first"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell"},"#java environment recommended >=1.0.8\ncommand -v java\n\n#yum package management tool, mainly when the linkis web management console installation script is executed, the nginx/policycoreutils-python software will be installed through yum\ncommand -v yum\n\n#When the database of linkis is initialized, it will use the mysql client to connect to the database and execute the sql statement\ncommand -v mysql\n\ncommand -v telnet\n\n#Related installation packages for decompressing linkis\ncommand -v tar\n\n#The initial service configuration file linkis-*.properties is replaced by sed in the properties file according to the data configured by `deploy-config/*sh` when the installation script install.sh is executed\ncommand -v sed\n\n")),(0,a.kt)("h3",{id:"12-add-deployment-user"},"1.2 Add deployment user"),(0,a.kt)("p",null,"Deployment user: the startup user of the linkis core process, and this user will be granted administrator privileges by default, and the corresponding administrator login password will be generated during the deployment process, which is located in the conf/linkis-mg-gateway.properties file"),(0,a.kt)("p",null,"linkis supports specifying the user who submits and executes. The main process service of linkis will switch to the corresponding execution user through sudo -u ${linkis-user}, and execute the corresponding engine startup command, so the process user of the engine process linkis-engine is the execution owner user belonging to the task"),(0,a.kt)("p",null,"Take hadoop user as an example:"),(0,a.kt)("p",null,"First check whether there is already a hadoop user in the system. If it already exists, you can directly authorize it; if not, create a user first, and then authorize."),(0,a.kt)("p",null,"Check if a hadoop user already exists"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:"script",script:!0},"$ id hadoop\nuid=2001(hadoop) gid=2001(hadoop) groups=2001(hadoop)\n")),(0,a.kt)("p",null,"If it does not exist, you need to create a hadoop user and join the hadoop user group"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:"script",script:!0},"$ sudo useradd hadoop -g hadoop\n$ vi /etc/sudoers\n#Add configuration\nhadoop ALL=(ALL) NOPASSWD: NOPASSWD: ALL\n")),(0,a.kt)("p",null,"Modify the environment variables of the installation user, ",(0,a.kt)("inlineCode",{parentName:"p"},"vim /home/hadoop/.bash_rc")," configure the environment variables, the environment variables are as follows:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:"script",script:!0},"export JAVA_HOME=/opt/jdk1.8\nexport HADOOP_HOME=/opt/install/hadoop\nexport HADOOP_CONF_DIR=/etc/conf/hadoop\nexport HIVE_HOME=/opt/install/hive\nexport HIVE_CONF_DIR=/etc/conf/conf\nexport SPARK_HOME=/opt/install/spark\nexport SPARK_CONF_DIR=/etc/spark/conf\nexport PYSPARK_ALLOW_INSECURE_GATEWAY=1 # If it is Pyspark, you must add a second parameter\n")),(0,a.kt)("p",null,"refresh configuration"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:"script",script:!0},"$ source /home/hadoop/.bash_rc\n")),(0,a.kt)("p",null,"Check if it works"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:"script",script:!0},"$ sudo -su hadoop\n$ echo $JAVA_HOME\n$ echo $HADOOP_HOME\n")),(0,a.kt)("font",{color:"red"},"The following operations are performed under the hadoop user"),(0,a.kt)("h3",{id:"13-installation-package-preparation"},"1.3 Installation package preparation"),(0,a.kt)("p",null,"linkis installation package, it is recommended to use version 1.X and above\nThe versions of 0.X and 1.X are quite different. Before 1.0.3, it was the package name of com.webank.wedatasphere.linkis, and linkis>=1.0.3 was the package name of org.apache.linkis."),(0,a.kt)("p",null,(0,a.kt)("a",{parentName:"p",href:"https://linkis.apache.org/download/main/"},"Download address"),": ",(0,a.kt)("a",{parentName:"p",href:"https://linkis.apache.org/download/main/"},"https://linkis.apache.org/download/main/")),(0,a.kt)("h3",{id:"14-low-level-dependency-checking"},"1.4 Low-level dependency checking"),(0,a.kt)("p",null,"You can execute the corresponding command to see if it is supported"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:"script",script:!0},"spark/hive/hdfs/python/\n$ spark-submit --version //spark tasks will be submitted to YARN for execution through this command\n$ python --version\n$ hdfs version\n$ hive --version\n")),(0,a.kt)("h3",{id:"15-resource-dependencies"},"1.5 Resource dependencies"),(0,a.kt)("p",null,"Accessible mysql database resources Database used to store business data of linkis itself",(0,a.kt)("br",{parentName:"p"}),"\n","Accessible yarn resource queues The execution of spark/hive/flink engines requires yarn queue resources",(0,a.kt)("br",{parentName:"p"}),"\n","Accessible hive matedata database resources (mysql as an example) Required for hive engine execution  "),(0,a.kt)("p",null,"Note: If the version of hive spark is quite different from the default version, it is best to re-edit the relevant hive/spark version that linkis depends on for compilation"),(0,a.kt)("h2",{id:"2-install"},"2. Install"),(0,a.kt)("h3",{id:"21-unzip-the-installation-package"},"2.1 Unzip the installation package"),(0,a.kt)("p",null,"After uploading the installation package ",(0,a.kt)("inlineCode",{parentName:"p"},"apache-linkis-1.0.3-incubating-bin.tar.gz"),", decompress the installation package  "),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:"script",script:!0},"$ tar -xvf apache-linkis-1.0.3-incubating-bin.tar.gz\n$ pwd\n/data/Install/1.0.3\n")),(0,a.kt)("p",null,"The unzipped directory structure is as follows"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:"script",script:!0},"-rw-r--r-- 1 hadoop hadoop 531847342 Feb 21 10:10 apache-linkis-1.0.3-incubating-bin.tar.gz\ndrwxrwxr-x 2 hadoop hadoop 4096 Feb 21 10:13 bin //Script to perform environment check and install\ndrwxrwxr-x 2 hadoop hadoop 4096 Feb 21 10:13 deploy-config // Environment configuration information such as DB that depends on deployment\n-rw-r--r-- 1 hadoop hadoop 1707 Jan 22 2020 DISCLAIMER-WIP\n-rw-r--r-- 1 hadoop hadoop 66058 Jan 22 2020 LICENSE\ndrwxrwxr-x 2 hadoop hadoop 16384 Feb 21 10:13 licenses\ndrwxrwxr-x 7 hadoop hadoop 4096 Feb 21 10:13 linkis-package // The actual package, including lib/service startup script tool/db initialization script/microservice configuration file, etc.\n-rw-r--r-- 1 hadoop hadoop 83126 Jan 22 2020 NOTICE\n-rw-r--r-- 1 hadoop hadoop 7900 Jan 22 2020 README_CN.md\n-rw-r--r-- 1 hadoop hadoop 8184 Jan 22 2020 README.md\n\n")),(0,a.kt)("h3",{id:"22-configure-linkis-database-information"},"2.2 Configure linkis database information"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:"script",script:!0},"vim deploy-config/db.sh\n\nExample:\nMYSQL_HOST=xx.xx.xx.xx\nMYSQL_PORT=3306\nMYSQL_DB=linkis_test\nMYSQL_USER=test\nMYSQL_PASSWORD=xxxxx\n\n")),(0,a.kt)("h3",{id:"23-configure-basic-environment-variables"},"2.3 Configure basic environment variables"),(0,a.kt)("p",null,"The file is located at ",(0,a.kt)("inlineCode",{parentName:"p"},"deploy-config/linkis-env.sh")),(0,a.kt)("h4",{id:"basic-directory-configuration"},"Basic directory configuration"),(0,a.kt)("blockquote",null,(0,a.kt)("p",{parentName:"blockquote"},"Please confirm that the deployment user deployUser has read and write permissions to these configuration directories")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:"script",script:!0},"deployUser=hadoop #The user who executes the deployment, the previously created user hadoop\n\nWORKSPACE_USER_ROOT_PATH=file:///tmp/linkis # Specify the directory path used by the user, which is generally used to store the user's script files and log files, and is the user's workspace. The corresponding configuration file configuration item is wds.linkis.filesystem.root.path(linkis.properties)\n\nRESULT_SET_ROOT_PATH=file:///tmp/linkis # Result set log and other file paths, used to store the Job result set file wds.linkis.resultSet.store.path(linkis-cg-entrance.properties) //If not configured, use Configuration of HDFS_USER_ROOT_PATH\n\nHDFS_USER_ROOT_PATH=hdfs:///tmp/linkis # Result set log and other file paths, used to store Job result set files wds.linkis.filesystem.hdfs.root.path(linkis.properties)\n\nENGINECONN_ROOT_PATH=/appcom/tmp #Store the working path of the execution engine, a local directory with write permissions for the deployment user wds.linkis.engineconn.root.dir(linkis-cg-engineconnmanager.properties)\n")),(0,a.kt)("p",null,"Note: Confirm whether the deployment user has read and write permissions for the corresponding file directory"),(0,a.kt)("h4",{id:"meta-configuration-for-hive"},"META configuration for HIVE"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:"script",script:!0},"HIVE_META_URL=jdbc:mysql://127.0.0.1:3306/hive_meta_demo?useUnicode=true&amp;characterEncoding=UTF-8 # URL of HiveMeta meta database\nHIVE_META_USER=demo # User of HiveMeta Metabase\nHIVE_META_PASSWORD=demo123 # HiveMeta metabase password\n")),(0,a.kt)("h4",{id:"yarns-resourcemanager-address"},"Yarn's ResourceManager address"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:"script",script:!0},"#You can confirm whether it is normal by visiting http://xx.xx.xx.xx:8088/ws/v1/cluster/scheduler interface  \nYARN_RESTFUL_URL=http://xx.xx.xx.xx:8088  \n")),(0,a.kt)("p",null,"When executing spark tasks, you need to use the ResourceManager of yarn. Linkis does not enable permission verification by default. If password permission verification is enabled for ResourceManager, please modify the ",(0,a.kt)("inlineCode",{parentName:"p"},"linkis_cg_engine_conn_plugin_bml_resources")," table data after installation and deployment (or see (#todo))"),(0,a.kt)("h4",{id:"ldap-login-authentication"},"LDAP login authentication"),(0,a.kt)("blockquote",null,(0,a.kt)("p",{parentName:"blockquote"},"Linkis uses static users and passwords by default. Static users are deployment users. Static passwords will randomly generate a password string during deployment and store them in {InstallPath}/conf/linkis-mg-gateway.properties (>=1.0.3 version)."),(0,a.kt)("pre",{parentName:"blockquote"},(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:"script",script:!0},"#LDAP configuration, Linkis only supports deployment user login by default, if you need to support multi-user login, you can use LDAP, you need to configure the following parameters\nnumber:\n#LDAP_URL=ldap://localhost:1389/\n#LDAP_BASEDN=dc=webank,dc=com\n"))),(0,a.kt)("h4",{id:"basic-component-environment-information"},"Basic component environment information"),(0,a.kt)("blockquote",null,(0,a.kt)("p",{parentName:"blockquote"},"It is best to configure it through the user's system environment variables (step 1.2 Adding a deployment user has been explained), you can directly comment it out without configuring in the deploy-config/linkis-env.sh configuration file"),(0,a.kt)("pre",{parentName:"blockquote"},(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:"script",script:!0},"###HADOOP CONF DIR\n#HADOOP_CONF_DIR=/appcom/config/hadoop-config\n###HIVE CONF DIR\n#HIVE_CONF_DIR=/appcom/config/hive-config\n###SPARK CONF DIR\n#SPARK_CONF_DIR=/appcom/config/spark-config\n"))),(0,a.kt)("h4",{id:"engine-version-information"},"Engine version information"),(0,a.kt)("admonition",{type:"caution"},(0,a.kt)("p",{parentName:"admonition"},"If the official release package used does not need to be modified, if it is compiled by modifying the Spark/Hive engine version, it needs to be modified.")),(0,a.kt)("p",null,"If spark is not version 2.4.3, you need to modify the parameters:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:"script",script:!0},"## Engine version conf\n#SPARK_VERSION, If the installed Spark version is not 2.4.3, it needs to be modified to the corresponding version, such as 3.1.1\nSPARK_VERSION=3.1.1\n")),(0,a.kt)("p",null,"If hive is not version 2.3.3, you need to modify the parameters:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:"script",script:!0},"## Engine version conf\n##HIVE_VERSION, If the installed Hive version is not 2.3.3, it needs to be modified to the corresponding version, such as 2.3.4\nHIVE_VERSION=2.3.4\n")),(0,a.kt)("p",null,"If configured, it will actually be updated in the ",(0,a.kt)("inlineCode",{parentName:"p"},"{linkisInstallPath}/conf/linkis.properties")," file after the installation and deployment are performed"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:"script",script:!0},"#wds.linkis.spark.engine.version=\n#wds.linkis.hive.engine.version=\n#wds.linkis.python.engine.version=\n")),(0,a.kt)("h4",{id:"jvm-memory-configuration"},"jvm memory configuration"),(0,a.kt)("blockquote",null,(0,a.kt)("p",{parentName:"blockquote"},"The microservice starts the jvm memory configuration, which can be adjusted according to the actual situation of the machine. If the machine memory resources are few, you can try to adjust it to 256/128M"),(0,a.kt)("pre",{parentName:"blockquote"},(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:"script",script:!0},'## java application default jvm memory\nexport SERVER_HEAP_SIZE="512M"\n'))),(0,a.kt)("h4",{id:"installation-directory-configuration"},"Installation directory configuration"),(0,a.kt)("blockquote",null,(0,a.kt)("p",{parentName:"blockquote"},"linkis will eventually be installed in this directory, if not configured, the default is the same level directory as the current installation package"),(0,a.kt)("pre",{parentName:"blockquote"},(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:"script",script:!0},"##The decompression directory and the installation directory need to be inconsistent\nLINKIS_HOME=/appcom/Install/LinkisInstall\n"))),(0,a.kt)("h2",{id:"3-deployment-process"},"3. Deployment process"),(0,a.kt)("h3",{id:"31-execute-the-deployment-script"},"3.1 Execute the deployment script"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:"script",script:!0},"sh bin/install.sh\n")),(0,a.kt)("p",null,"tip: If an error occurs and you are not sure what command to execute to report the error, you can add the -v parameter ",(0,a.kt)("inlineCode",{parentName:"p"},"sh -v bin/install.sh")," to print the shell script execution process log, which is convenient for locating the problem."),(0,a.kt)("h3",{id:"32-possible-problems"},"3.2 Possible problems"),(0,a.kt)("h4",{id:"1permission-problem-mkdir-cannot-create-directory-xxxx-permission-denied"},"1.Permission problem mkdir: cannot create directory \u2018xxxx\u2019: Permission denied"),(0,a.kt)("p",null,"The prompt for successful execution is as follows\uff1a",(0,a.kt)("br",{parentName:"p"}),"\n","Congratulations! You have installed Linkis 1.0.3 successfully, please use sh /data/Install/linkis/sbin/linkis-start-all.sh to start it!",(0,a.kt)("br",{parentName:"p"}),"\n","Your default account password is ","[","hadoop/5e8e312b4]    "),(0,a.kt)("h3",{id:"33-configuration-modification"},"3.3 Configuration modification"),(0,a.kt)("p",null,"After the installation is complete, if you need to modify the configuration, you can re-execute the installation, or modify the corresponding ${InstallPath}/conf/*properties file and restart the corresponding service"),(0,a.kt)("h3",{id:"34-add-mysql-driver-103-version"},"3.4 Add mysql driver (>=1.0.3) version"),(0,a.kt)("p",null,"Because of the license, mysql-connector-java is removed from the release package of linkis itself (the family bucket integrated by dss will be included, no need to manually add it), which needs to be added manually.",(0,a.kt)("br",{parentName:"p"}),"\n","For details, see ","[Add mysql driver package]","(docs/1.0.3/deployment/quick-deploy#-44-Add mysql driver package)"),(0,a.kt)("h3",{id:"35-start-the-service"},"3.5 Start the service"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:"script",script:!0},"sh sbin/linkis-start-all.sh\n")),(0,a.kt)("h3",{id:"36-check-whether-the-service-starts-normally"},"3.6 Check whether the service starts normally"),(0,a.kt)("p",null,"Visit the eureka service page (http://eurekaip:20303), version 1.0.x, the following services must be started normally"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:"script",script:!0},"LINKIS-CG-ENGINECONNMANAGER\nLINKIS-CG-ENGINEPLUGIN\nLINKIS-CG-ENTRANCE\nLINKIS-CG-LINKISMANAGER\xa0\xa0\xa0\nLINKIS-MG-EUREKA\xa0\nLINKIS-MG-GATEWAY\nLINKIS-PS-CS\nLINKIS-PS-PUBLICSERVICE\n")),(0,a.kt)("p",null,"If any services are not started, you can view detailed exception logs in the corresponding log/${service name}.log file."),(0,a.kt)("h2",{id:"4-install-the-web-frontend"},"4. Install the web frontend"),(0,a.kt)("blockquote",null,(0,a.kt)("p",{parentName:"blockquote"},"Mainly perform YARN related configuration")),(0,a.kt)("p",null,"Download the front-end installation package and unzip it\ntar -xvf apache-linkis-1.0.3-incubating-web-bin.tar.gz"),(0,a.kt)("p",null,"Modify configuration config.sh"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:"script",script:!0},'#Port for console access http://localhost:8088\nlinkis_port="8088"\n#linkis-mg-gatewayService Address\nlinkis_url="http://localhost:9020"\n')),(0,a.kt)("p",null,"Perform front-end deployment"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:"script",script:!0},"sudo sh install\n")),(0,a.kt)("p",null,"After installation, the nginx configuration file of linkis defaults to /etc/nginx/conf.d/linkis.conf\nnginx log files are in /var/log/nginx/access.log and /var/log/nginx/error.log"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-nginx"},"\n        server {\n            listen 8188;# access port\n            server_name localhost;\n            #charset koi8-r;\n            #access_log /var/log/nginx/host.access.log main;\n            location /linkis/visualis {\n            root /appcom/Install/linkis-web/linkis/visualis; # static file directory\n            autoindex on;\n            }\n            location / {\n            root /appcom/Install/linkis-web/dist; # static file directory\n            index index.html index.html;\n            }\n            location /ws {\n            proxy_pass http://localhost:9020;#Address of backend Linkis\n            proxy_http_version 1.1;\n            proxy_set_header Upgrade $http_upgrade;\n            proxy_set_header Connection upgrade;\n            }\n\n            location /api {\n            proxy_pass http://localhost:9020; #Address of backend Linkis\n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n            proxy_set_header x_real_ipP $remote_addr;\n            proxy_set_header remote_addr $remote_addr;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_http_version 1.1;\n            proxy_connect_timeout 4s;\n            proxy_read_timeout 600s;\n            proxy_send_timeout 12s;\n            proxy_set_header Upgrade $http_upgrade;\n            proxy_set_header Connection upgrade;\n            }\n\n            #error_page 404 /404.html;\n            # redirect server error pages to the static page /50x.html\n            #\n            error_page 500 502 503 504 /50x.html;\n            location = /50x.html {\n            root /usr/share/nginx/html;\n            }\n        }\n")),(0,a.kt)("p",null,"If you need to modify the port or static resource directory, etc., please modify the /etc/nginx/conf.d/linkis.conf file and execute the ",(0,a.kt)("inlineCode",{parentName:"p"},"sudo nginx -s reload")," command"),(0,a.kt)("p",null,"Log in to the web terminal to view information\n",(0,a.kt)("a",{parentName:"p",href:"http://xx.xx.xx.xx:8188/#/login"},"http://xx.xx.xx.xx:8188/#/login"),"\nUsername/Password (check in {InstallPath}/conf/linkis-mg-gateway.properties)"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:"script",script:!0},"#When not using LDAP configuration\nwds.linkis.admin.user= #User\nwds.linkis.admin.password= #Password\n")),(0,a.kt)("p",null,"After logging in, check whether the yarn queue resources can be displayed normally (if you want to use the spark/hive/flink engine)\nNormally as shown below:",(0,a.kt)("br",{parentName:"p"}),"\n",(0,a.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/7869972/159955494-2f305a38-a3d6-4798-83aa-58cde23bc436.png",alt:"yarn-normal"})),(0,a.kt)("p",null,"If it cannot be displayed:  "),(0,a.kt)("h4",{id:"1-check-whether-the-yarn-address-is-configured-correctly"},"1 Check whether the yarn address is configured correctly"),(0,a.kt)("p",null,"Database table linkis_cg_rm_external_resource_provider\nInsert yarn data information"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-sql"},'INSERT INTO `linkis_cg_rm_external_resource_provider`\n(`resource_type`, `name`, `labels`, `config`) VALUES\n(\'Yarn\', \'default\', NULL,\n\'{\\r\\n"rmWebAddress": "http://xx.xx.xx.xx:8088",\\r\\n"hadoopVersion": "2.7.2",\\r\\n"authorEnable":false, \\r\\n"user":"hadoop",\\r\\n"pwd":"1234\n56"\\r\\n}\'\n);\n\nconfig field properties\n\n"rmWebAddress": "http://xx.xx.xx.xx:8088", #need to bring http and port\n"hadoopVersion": "2.7.2",\n"authorEnable":true, //Whether authentication is required You can verify the username and password by visiting http://xx.xx.xx.xx:8088 in the browser\n"user":"user",//username\n"pwd":"pwd"//Password\n\n')),(0,a.kt)("p",null,"After the update, because the cache is used in the program, if you want to take effect immediately, you need to restart the linkis-cg-linkismanager service"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:"script",script:!0},"sh sbin/linkis-daemon.sh restart cg-linkismanager\n")),(0,a.kt)("h4",{id:"2-check-whether-the-yarn-queue-is-correct"},"2 Check whether the yarn queue is correct"),(0,a.kt)("p",null,"Exception message: desc: queue ide is not exists in YARN.",(0,a.kt)("br",{parentName:"p"}),"\n","The configuration yarn queue does not exist and needs to be adjusted  "),(0,a.kt)("p",null,"Modification method: linkis management console/parameter configuration > global settings > yarn queue name ","[wds.linkis.rm.yarnqueue]"," Modify a usable yarn queue",(0,a.kt)("br",{parentName:"p"}),"\n","Available yarn queues can be viewed at rmWebAddress:",(0,a.kt)("a",{parentName:"p",href:"http://xx.xx.xx.xx:8088"},"http://xx.xx.xx.xx:8088")),(0,a.kt)("h2",{id:"5-check-if-the-engine-material-resource-is-uploaded-successfully"},"5. Check if the engine material resource is uploaded successfully"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-sql"},"#Login to the linkis database\nselect * from linkis_cg_engine_conn_plugin_bml_resources\n")),(0,a.kt)("p",null,"normal as follows",(0,a.kt)("br",{parentName:"p"}),"\n",(0,a.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/29391030/156343249-9f6dca8f-4e0d-438b-995f-4f469270a22d.png",alt:"bml"})),(0,a.kt)("p",null,"Check whether the material record of the engine exists (if there is an update, check whether the update time is correct).",(0,a.kt)("br",{parentName:"p"}),"\n","If it does not exist or is not updated, first try to manually refresh the material resource (for details, see ","[Engine Material Resource Refresh]","(docs/latest/deployment/install-engineconn#23-Engine Refresh)). Check the ",(0,a.kt)("inlineCode",{parentName:"p"},"log/linkis-cg-engineplugin.log")," log to check the specific reasons for the failure of the material. In many cases, it may be caused by the lack of permissions in the hdfs directory. Check whether the gateway address configuration is correct ",(0,a.kt)("inlineCode",{parentName:"p"},"conf/linkis.properties:wds.linkis.gateway.url")),(0,a.kt)("p",null,"The material resources of the engine are uploaded to the hdfs directory by default as ",(0,a.kt)("inlineCode",{parentName:"p"},"/apps-data/${deployUser}/bml")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:"script",script:!0},"hdfs dfs -ls /apps-data/hadoop/bml\n#If there is no such directory, please manually create the directory and grant ${deployUser} read and write permissions\nhdfs dfs -mkdir /apps-data\nhdfs dfs -chown hadoop:hadoop/apps-data\n")),(0,a.kt)("p",null,"##Verify basic functions"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},'#The version number of the engineType of the engine must match the actual one\n\nsh bin/linkis-cli -submitUser hadoop -engineType shell-1 -codeType shell -code "whoami"\nsh bin/linkis-cli -submitUser hadoop -engineType hive-2.3.3 -codeType hql -code "show tables"\nsh bin/linkis-cli -submitUser hadoop -engineType spark-2.4.3 -codeType sql -code "show tables"\nsh bin/linkis-cli -submitUser hadoop -engineType python-python2 -codeType python -code \'print("hello, world!")\'\n')),(0,a.kt)("p",null,"View supported versions of each engine"),(0,a.kt)("p",null,"Method 1: View the directory packaged by the engine"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"$ tree linkis-package/lib/linkis-engineconn-plugins/ -L 3\nlinkis-package/lib/linkis-engineconn-plugins/\n\u251c\u2500\u2500 hive\n\u2502 \u251c\u2500\u2500 dist\n\u2502 \u2502 \u2514\u2500\u2500 v2.3.3 #version is 2.3.3 engineType is hive-2.3.3\n\u2502 \u2514\u2500\u2500 plugin\n\u2502 \u2514\u2500\u2500 2.3.3\n\u251c\u2500\u2500 python\n\u2502 \u251c\u2500\u2500 dist\n\u2502 \u2502 \u2514\u2500\u2500 vpython2\n\u2502 \u2514\u2500\u2500 plugin\n\u2502 \u2514\u2500\u2500 python2 #version is python2 engineType is python-python2\n\u251c\u2500\u2500 shell\n\u2502 \u251c\u2500\u2500 dist\n\u2502 \u2502 \u2514\u2500\u2500 v1\n\u2502 \u2514\u2500\u2500 plugin\n\u2502 \u2514\u2500\u2500 1\n\u2514\u2500\u2500 spark\n    \u251c\u2500\u2500 dist\n    \u2502 \u2514\u2500\u2500 v2.4.3\n    \u2514\u2500\u2500 plugin\n        \u2514\u2500\u2500 2.4.3\n")),(0,a.kt)("p",null,"Method 2: View the database table of linkis\nselect * from linkis_cg_engine_conn_plugin_bml_resources"),(0,a.kt)("h2",{id:"6-troubleshooting-installation-and-deployment-common-problems"},"6. Troubleshooting installation and deployment common problems"),(0,a.kt)("h3",{id:"61-version-compatibility-issues"},"6.1 Version compatibility issues"),(0,a.kt)("p",null," The engine supported by linkis by default, for compatibility with dss, you can view this document ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/apache/linkis/blob/master/README.md"},"https://github.com/apache/linkis/blob/master/README.md")),(0,a.kt)("h3",{id:"62-how-to-locate-the-server-exception-log"},"6.2 How to locate the server exception log"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"Linkis has many microservices. If you are unfamiliar with the system, sometimes you cannot locate the specific module that has an exception. You can search through the global log.  \ntail -f log/* |grep -5n exception (or tail -f log/* |grep -5n ERROR)  \nless log/* |grep -5n exception (or less log/* |grep -5n ERROR)  \n")),(0,a.kt)("h3",{id:"63-execution-of-abnormal-troubleshooting-of-engine-tasks"},"6.3 Execution of abnormal troubleshooting of engine tasks"),(0,a.kt)("p",null,"  step1: Find the startup deployment directory of the engine",(0,a.kt)("br",{parentName:"p"}),"\n","Method 1: If it is displayed in the execution log, you can view it on the management console as shown below:",(0,a.kt)("br",{parentName:"p"}),"\n","",(0,a.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/29391030/156343802-9d47fa98-dc70-4206-b07f-df439b291028.png",alt:"engine-log"}),(0,a.kt)("br",{parentName:"p"}),"\n","Method 2: If it is not found in method 1, you can find the parameter of ",(0,a.kt)("inlineCode",{parentName:"p"},"wds.linkis.engineconn.root.dir")," configured in ",(0,a.kt)("inlineCode",{parentName:"p"},"conf/linkis-cg-engineconnmanager.properties"),", which is the directory where the engine is started and deployed. The user of the execution engine is isolated (taskId). If you do not know the taskid, you can select it after sorting by time. ll -rt /appcom/tmp/${executing user}/workDir"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"cd /appcom/tmp/${executing user}/workDir/${taskId}  \n# The directory is roughly as follows  \nconf -> /appcom/tmp/engineConnPublickDir/6a09d5fb-81dd-41af-a58b-9cb5d5d81b5a/v000002/conf #engine configuration file  \nengineConnExec.sh #Generated engine startup script  \nlib -> /appcom/tmp/engineConnPublickDir/45bf0e6b-0fa5-47da-9532-c2a9f3ec764d/v000003/lib #Engine dependent packages  \nlogs #Engine startup and execution related logs  \n")),(0,a.kt)("p",null,"  step2: View the log of the engine",(0,a.kt)("br",{parentName:"p"}),"\n","less logs/stdout"),(0,a.kt)("p",null,"  step3: try to execute the script manually (if needed)",(0,a.kt)("br",{parentName:"p"}),"\n","Debugging can be done by trying to execute the script manually",(0,a.kt)("br",{parentName:"p"}),"\n","sh engineConnExec.sh  "),(0,a.kt)("h3",{id:"64-notes-on-cdh-adaptation-version"},"6.4 Notes on CDH adaptation version"),(0,a.kt)("p",null,"  CDH itself is not an official standard hive/spark package. When adapting, it is best to modify the hive/spark version dependencies in the source code of linkis to recompile and deploy.\nFor details, please refer to the CDH adaptation blog post",(0,a.kt)("br",{parentName:"p"}),"\n","",(0,a.kt)("a",{parentName:"p",href:"https://mp.weixin.qq.com/s/__QxC1NoLQFwme1yljy-Nw"},"[Linkis1.0 - Installation and Stepping in the CDH5 Environment]"),(0,a.kt)("br",{parentName:"p"}),"\n","",(0,a.kt)("a",{parentName:"p",href:"https://mp.weixin.qq.com/s/9Pl9P0hizDWbbTBf1yzGJA"},"[DSS1.0.0+Linkis1.0.2\u2014\u2014Trial record in CDH5 environment]"),(0,a.kt)("br",{parentName:"p"}),"\n","",(0,a.kt)("a",{parentName:"p",href:"https://mp.weixin.qq.com/s/vcFge4BNiEuW-7OC3P-yaw"},"[DSS1.0.0 and Linkis1.0.2\u2014\u2014Summary of JDBC engine related issues]"),(0,a.kt)("br",{parentName:"p"}),"\n","",(0,a.kt)("a",{parentName:"p",href:"https://mp.weixin.qq.com/s/VxZ16IPMd1CvcrvHFuU4RQ"},"[DSS1.0.0 and Linkis1.0.2\u2014\u2014Summary of Flink engine related issues]"),"  "),(0,a.kt)("h3",{id:"66-debugging-of-http-interface"},"6.6 Debugging of Http interface"),(0,a.kt)("p",null,"  Method 1 can open the ","[Login-Free Mode Guide]"," (docs/latest/api/login-api#2 Login-Free Configuration)",(0,a.kt)("br",{parentName:"p"}),"\n","Method 2: Add a static Token to the http request header"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell",metastring:"script",script:!0},"Token-User:hadoop\nToken-Code: BML-AUTH\n")),(0,a.kt)("h3",{id:"67-troubleshooting-process-for-abnormal-problems"},"6.7 Troubleshooting process for abnormal problems"),(0,a.kt)("p",null,"  First, follow the above steps to check whether the service/environment, etc. are all started normally",(0,a.kt)("br",{parentName:"p"}),"\n","Troubleshoot basic problems according to some of the scenarios listed above",(0,a.kt)("br",{parentName:"p"}),"\n","",(0,a.kt)("a",{parentName:"p",href:"https://docs.qq.com/doc/DSGZhdnpMV3lTUUxq"},"QA documentation")," Find out if there is a solution, link: ",(0,a.kt)("a",{parentName:"p",href:"https://docs.qq.com/doc/DSGZhdnpMV3lTUUxq"},"https://docs.qq.com/doc/DSGZhdnpMV3lTUUxq"),(0,a.kt)("br",{parentName:"p"}),"\n","See if you can find a solution by searching the content in the issue",(0,a.kt)("br",{parentName:"p"}),"\n","",(0,a.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/29391030/156343419-81cc25e0-aa94-4c06-871c-bb036eb6d4ff.png",alt:"issues"}),(0,a.kt)("br",{parentName:"p"}),"\n",'Through the official website document search, for some problems, you can search for keywords through the official website, such as searching for "deployment". (If 404 appears,   please refresh your browser)',(0,a.kt)("br",{parentName:"p"}),"\n","",(0,a.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/29391030/156343459-7911bd05-4d8d-4a7b-b9f8-35c152d52c41.png",alt:"search"}),"     "),(0,a.kt)("h2",{id:"7-how-to-obtain-relevant-information"},"7. How to obtain relevant information"),(0,a.kt)("p",null,"Linkis official website documents are constantly improving, you can view/keyword search related documents on this official website."),(0,a.kt)("p",null,"Related blog post links  "),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Linkis technical blog collection ",(0,a.kt)("a",{parentName:"li",href:"https://github.com/apache/linkis/issues/1233"},"https://github.com/apache/linkis/issues/1233"),"  "),(0,a.kt)("li",{parentName:"ul"},"Official account technical blog post ",(0,a.kt)("a",{parentName:"li",href:"https://mp.weixin.qq.com/mp/homepage?__biz=MzI4MDkxNzUxMg==&hid=1&sn=088cbf2bbed1c80d003c5865bc92ace8&scene=18"},"https://mp.weixin.qq.com/mp/homepage?__biz=MzI4MDkxNzUxMg==&hid=1&sn=088cbf2bbed1c80d003c5865bc92ace8&scene=18"),"  "),(0,a.kt)("li",{parentName:"ul"},"Official website documentation ",(0,a.kt)("a",{parentName:"li",href:"https://linkis.apache.org/docs/latest/about/introduction/"},"https://linkis.apache.org/docs/latest/about/introduction/"),"  "),(0,a.kt)("li",{parentName:"ul"},"bilibili technology sharing video ",(0,a.kt)("a",{parentName:"li",href:"https://space.bilibili.com/598542776?spm_id_from=333.788.b_765f7570696e666f.2"},"https://space.bilibili.com/598542776?spm_id_from=333.788.b_765f7570696e666f.2"))))}h.isMDXComponent=!0}}]);