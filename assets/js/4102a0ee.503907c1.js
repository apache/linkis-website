"use strict";(self.webpackChunklinkis_web_apache=self.webpackChunklinkis_web_apache||[]).push([[41559],{3905:(e,n,t)=>{t.d(n,{Zo:()=>p,kt:()=>m});var i=t(67294);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function r(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);n&&(i=i.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,i)}return t}function l(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?r(Object(t),!0).forEach((function(n){a(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function o(e,n){if(null==e)return{};var t,i,a=function(e,n){if(null==e)return{};var t,i,a={},r=Object.keys(e);for(i=0;i<r.length;i++)t=r[i],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(i=0;i<r.length;i++)t=r[i],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var s=i.createContext({}),d=function(e){var n=i.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):l(l({},n),e)),t},p=function(e){var n=d(e.components);return i.createElement(s.Provider,{value:n},e.children)},g="mdxType",u={inlineCode:"code",wrapper:function(e){var n=e.children;return i.createElement(i.Fragment,{},n)}},c=i.forwardRef((function(e,n){var t=e.components,a=e.mdxType,r=e.originalType,s=e.parentName,p=o(e,["components","mdxType","originalType","parentName"]),g=d(t),c=a,m=g["".concat(s,".").concat(c)]||g[c]||u[c]||r;return t?i.createElement(m,l(l({ref:n},p),{},{components:t})):i.createElement(m,l({ref:n},p))}));function m(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var r=t.length,l=new Array(r);l[0]=c;var o={};for(var s in n)hasOwnProperty.call(n,s)&&(o[s]=n[s]);o.originalType=e,o[g]="string"==typeof e?e:a,l[1]=o;for(var d=2;d<r;d++)l[d]=t[d];return i.createElement.apply(null,l)}return i.createElement.apply(null,t)}c.displayName="MDXCreateElement"},27146:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>l,default:()=>u,frontMatter:()=>r,metadata:()=>o,toc:()=>d});var i=t(87462),a=(t(67294),t(3905));const r={title:"Hive Engine",sidebar_position:2},l=void 0,o={unversionedId:"engine-usage/hive",id:"engine-usage/hive",title:"Hive Engine",description:"This article mainly introduces the installation, usage and configuration of the Hive engine plugin in Linkis.",source:"@site/docs/engine-usage/hive.md",sourceDirName:"engine-usage",slug:"/engine-usage/hive",permalink:"/docs/1.5.0/engine-usage/hive",draft:!1,editUrl:"https://github.com/apache/linkis-website/edit/dev/docs/engine-usage/hive.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{title:"Hive Engine",sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"Spark Engine",permalink:"/docs/1.5.0/engine-usage/spark"},next:{title:"Python Engine",permalink:"/docs/1.5.0/engine-usage/python"}},s={},d=[{value:"1. Preliminary work",id:"1-preliminary-work",level:2},{value:"1.1 Environment configuration before engine use",id:"11-environment-configuration-before-engine-use",level:3},{value:"1.1 Environment verification",id:"11-environment-verification",level:3},{value:"2. Engine plugin installation default engine",id:"2-engine-plugin-installation-default-engine",level:2},{value:"3. Engine usage",id:"3-engine-usage",level:2},{value:"3.1 Submitting tasks via <code>Linkis-cli</code>",id:"31-submitting-tasks-via-linkis-cli",level:3},{value:"3.2 Submit tasks through Linkis SDK",id:"32-submit-tasks-through-linkis-sdk",level:3},{value:"4. Engine configuration instructions",id:"4-engine-configuration-instructions",level:2},{value:"4.1 Default Configuration Description",id:"41-default-configuration-description",level:3},{value:"4.2 Queue resource configuration",id:"42-queue-resource-configuration",level:3},{value:"4.3 Configuration modification",id:"43-configuration-modification",level:3},{value:"4.3.1 Management Console Configuration",id:"431-management-console-configuration",level:4},{value:"4.3.2 Task interface configuration",id:"432-task-interface-configuration",level:4},{value:"4.4 Engine related data table",id:"44-engine-related-data-table",level:3},{value:"5. Hive modification log display",id:"5-hive-modification-log-display",level:2}],p={toc:d},g="wrapper";function u(e){let{components:n,...r}=e;return(0,a.kt)(g,(0,i.Z)({},p,r,{components:n,mdxType:"MDXLayout"}),(0,a.kt)("p",null,"This article mainly introduces the installation, usage and configuration of the ",(0,a.kt)("inlineCode",{parentName:"p"},"Hive")," engine plugin in ",(0,a.kt)("inlineCode",{parentName:"p"},"Linkis"),"."),(0,a.kt)("h2",{id:"1-preliminary-work"},"1. Preliminary work"),(0,a.kt)("h3",{id:"11-environment-configuration-before-engine-use"},"1.1 Environment configuration before engine use"),(0,a.kt)("p",null,"If you want to use the ",(0,a.kt)("inlineCode",{parentName:"p"},"hive")," engine on your server, you need to ensure that the following environment variables have been set correctly and the engine startup user has these environment variables."),(0,a.kt)("p",null,"It is strongly recommended that you check these environment variables for the executing user before executing ",(0,a.kt)("inlineCode",{parentName:"p"},"hive")," tasks."),(0,a.kt)("table",null,(0,a.kt)("thead",{parentName:"table"},(0,a.kt)("tr",{parentName:"thead"},(0,a.kt)("th",{parentName:"tr",align:null},"Environment variable name"),(0,a.kt)("th",{parentName:"tr",align:null},"Environment variable content"),(0,a.kt)("th",{parentName:"tr",align:null},"Remarks"))),(0,a.kt)("tbody",{parentName:"table"},(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},"JAVA_HOME"),(0,a.kt)("td",{parentName:"tr",align:null},"JDK installation path"),(0,a.kt)("td",{parentName:"tr",align:null},"Required")),(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},"HADOOP_HOME"),(0,a.kt)("td",{parentName:"tr",align:null},"Hadoop installation path"),(0,a.kt)("td",{parentName:"tr",align:null},"Required")),(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},"HADOOP_CONF_DIR"),(0,a.kt)("td",{parentName:"tr",align:null},"Hadoop configuration path"),(0,a.kt)("td",{parentName:"tr",align:null},"required")),(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},"HIVE_CONF_DIR"),(0,a.kt)("td",{parentName:"tr",align:null},"Hive configuration path"),(0,a.kt)("td",{parentName:"tr",align:null},"required")))),(0,a.kt)("h3",{id:"11-environment-verification"},"1.1 Environment verification"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"# link hive\nbin/hive\n\n# test command\nshow databases;\n\n# Being able to link successfully and output database information normally means that the environment configuration is successful\nhive (default)> show databases;\nOK\ndatabases_name\ndefault\n")),(0,a.kt)("h2",{id:"2-engine-plugin-installation-default-engine"},"2. Engine plugin installation ",(0,a.kt)("a",{parentName:"h2",href:"/docs/1.5.0/engine-usage/overview"},"default engine")),(0,a.kt)("p",null,"The binary installation package released by ",(0,a.kt)("inlineCode",{parentName:"p"},"linkis")," includes the ",(0,a.kt)("inlineCode",{parentName:"p"},"Hive")," engine plug-in by default, and users do not need to install it additionally."),(0,a.kt)("p",null,"The version of ",(0,a.kt)("inlineCode",{parentName:"p"},"Hive")," supports ",(0,a.kt)("inlineCode",{parentName:"p"},"hive1.x")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"hive2.x"),". The default is to support ",(0,a.kt)("inlineCode",{parentName:"p"},"hive on MapReduce"),". If you want to change to ",(0,a.kt)("inlineCode",{parentName:"p"},"Hive on Tez"),", you need to modify it according to this ",(0,a.kt)("inlineCode",{parentName:"p"},"pr"),"."),(0,a.kt)("p",null,(0,a.kt)("a",{parentName:"p",href:"https://github.com/apache/linkis/pull/541"},"https://github.com/apache/linkis/pull/541")),(0,a.kt)("p",null,"The ",(0,a.kt)("inlineCode",{parentName:"p"},"hive")," version supported by default is 3.1.3, if you want to modify the ",(0,a.kt)("inlineCode",{parentName:"p"},"hive")," version, you can find the ",(0,a.kt)("inlineCode",{parentName:"p"},"linkis-engineplugin-hive")," module, modify the \\<hive.version",">"," tag, and then compile this module separately Can"),(0,a.kt)("p",null,(0,a.kt)("a",{parentName:"p",href:"/docs/1.5.0/deployment/install-engineconn"},"EngineConnPlugin engine plugin installation")),(0,a.kt)("h2",{id:"3-engine-usage"},"3. Engine usage"),(0,a.kt)("h3",{id:"31-submitting-tasks-via-linkis-cli"},"3.1 Submitting tasks via ",(0,a.kt)("inlineCode",{parentName:"h3"},"Linkis-cli")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell"},'sh ./bin/linkis-cli -engineType hive-3.1.3 \\\n-codeType hql -code "show databases"  \\\n-submitUser hadoop -proxyUser hadoop\n')),(0,a.kt)("p",null,"More ",(0,a.kt)("inlineCode",{parentName:"p"},"Linkis-Cli")," command parameter reference: ",(0,a.kt)("a",{parentName:"p",href:"/docs/1.5.0/user-guide/linkiscli-manual"},(0,a.kt)("inlineCode",{parentName:"a"},"Linkis-Cli")," usage")),(0,a.kt)("h3",{id:"32-submit-tasks-through-linkis-sdk"},"3.2 Submit tasks through Linkis SDK"),(0,a.kt)("p",null,(0,a.kt)("inlineCode",{parentName:"p"},"Linkis")," provides ",(0,a.kt)("inlineCode",{parentName:"p"},"SDK")," of ",(0,a.kt)("inlineCode",{parentName:"p"},"Java")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"Scala")," to submit tasks to ",(0,a.kt)("inlineCode",{parentName:"p"},"Linkis")," server. For details, please refer to ",(0,a.kt)("a",{parentName:"p",href:"/docs/1.5.0/user-guide/sdk-manual"},"JAVA SDK Manual"),".\nFor the ",(0,a.kt)("inlineCode",{parentName:"p"},"Hive")," task, you only need to modify ",(0,a.kt)("inlineCode",{parentName:"p"},"EngineConnType")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"CodeType")," parameters in ",(0,a.kt)("inlineCode",{parentName:"p"},"Demo"),":"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-java"},'Map<String, Object> labels = new HashMap<String, Object>();\nlabels.put(LabelKeyConstant.ENGINE_TYPE_KEY, "hive-3.1.3"); // required engineType Label\nlabels.put(LabelKeyConstant.USER_CREATOR_TYPE_KEY, "hadoop-IDE");// required execute user and creator\nlabels.put(LabelKeyConstant.CODE_TYPE_KEY, "hql"); // required codeType\n')),(0,a.kt)("h2",{id:"4-engine-configuration-instructions"},"4. Engine configuration instructions"),(0,a.kt)("h3",{id:"41-default-configuration-description"},"4.1 Default Configuration Description"),(0,a.kt)("table",null,(0,a.kt)("thead",{parentName:"table"},(0,a.kt)("tr",{parentName:"thead"},(0,a.kt)("th",{parentName:"tr",align:null},"Configuration"),(0,a.kt)("th",{parentName:"tr",align:null},"Default"),(0,a.kt)("th",{parentName:"tr",align:null},"Required"),(0,a.kt)("th",{parentName:"tr",align:null},"Description"))),(0,a.kt)("tbody",{parentName:"table"},(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},"wds.linkis.rm.instance"),(0,a.kt)("td",{parentName:"tr",align:null},"10"),(0,a.kt)("td",{parentName:"tr",align:null},"no"),(0,a.kt)("td",{parentName:"tr",align:null},"engine maximum concurrency")),(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},"wds.linkis.engineconn.java.driver.memory"),(0,a.kt)("td",{parentName:"tr",align:null},"1g"),(0,a.kt)("td",{parentName:"tr",align:null},"No"),(0,a.kt)("td",{parentName:"tr",align:null},"engine initialization memory size")),(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},"wds.linkis.engineconn.max.free.time"),(0,a.kt)("td",{parentName:"tr",align:null},"1h"),(0,a.kt)("td",{parentName:"tr",align:null},"no"),(0,a.kt)("td",{parentName:"tr",align:null},"engine idle exit time")))),(0,a.kt)("h3",{id:"42-queue-resource-configuration"},"4.2 Queue resource configuration"),(0,a.kt)("p",null,"The ",(0,a.kt)("inlineCode",{parentName:"p"},"MapReduce")," task of ",(0,a.kt)("inlineCode",{parentName:"p"},"hive")," needs to use ",(0,a.kt)("inlineCode",{parentName:"p"},"yarn")," resources, so a queue needs to be set"),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"yarn",src:t(51395).Z,width:"1252",height:"653"}),"    "),(0,a.kt)("h3",{id:"43-configuration-modification"},"4.3 Configuration modification"),(0,a.kt)("p",null,"If the default parameters are not satisfied, there are the following ways to configure some basic parameters"),(0,a.kt)("h4",{id:"431-management-console-configuration"},"4.3.1 Management Console Configuration"),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"hive",src:t(60533).Z,width:"1258",height:"715"})),(0,a.kt)("p",null,"Note: After modifying the configuration under the ",(0,a.kt)("inlineCode",{parentName:"p"},"IDE")," tag, you need to specify ",(0,a.kt)("inlineCode",{parentName:"p"},"-creator IDE")," to take effect (other tags are similar), such as:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell"},'sh ./bin/linkis-cli -creator IDE \\\n-engineType hive-3.1.3 -codeType hql \\\n-code "show databases"  \\\n-submitUser hadoop -proxyUser hadoop\n')),(0,a.kt)("h4",{id:"432-task-interface-configuration"},"4.3.2 Task interface configuration"),(0,a.kt)("p",null,"Submit the task interface, configure it through the parameter ",(0,a.kt)("inlineCode",{parentName:"p"},"params.configuration.runtime")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell"},'Example of http request parameters\n{\n    "executionContent": {"code": "show databases;", "runType":  "sql"},\n    "params": {\n                    "variable": {},\n                    "configuration": {\n                            "runtime": {\n                                "wds.linkis.rm.instance":"10"\n                                }\n                            }\n                    },\n    "labels": {\n        "engineType": "hive-3.1.3",\n        "userCreator": "hadoop-IDE"\n    }\n}\n')),(0,a.kt)("h3",{id:"44-engine-related-data-table"},"4.4 Engine related data table"),(0,a.kt)("p",null,(0,a.kt)("inlineCode",{parentName:"p"},"Linkis")," is managed through engine tags, and the data table information involved is as follows."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"linkis_ps_configuration_config_key: Insert the key and default values \u200b\u200b\u200b\u200bof the configuration parameters of the engine\nlinkis_cg_manager_label: insert engine label such as: hive-3.1.3\nlinkis_ps_configuration_category: Insert the directory association of the engine\nlinkis_ps_configuration_config_value: The configuration that the insertion engine needs to display\nlinkis_ps_configuration_key_engine_relation: The relationship between the configuration item and the engine\n")),(0,a.kt)("p",null,"The initial data related to the engine in the table is as follows"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-sql"},"-- set variable\nSET @HIVE_LABEL=\"hive-3.1.3\";\nSET @HIVE_ALL=CONCAT('*-*,',@HIVE_LABEL);\nSET @HIVE_IDE=CONCAT('*-IDE,',@HIVE_LABEL);\n\n-- engine label\ninsert into `linkis_cg_manager_label` (`label_key`, `label_value`, `label_feature`, `label_value_size`, `update_time`, `create_time`) VALUES ('combined_userCreator_engineType', @HIVE_ALL, 'OPTIONAL', 2, now(), now());\ninsert into `linkis_cg_manager_label` (`label_key`, `label_value`, `label_feature`, `label_value_size`, `update_time`, `create_time`) VALUES ('combined_userCreator_engineType', @HIVE_IDE, 'OPTIONAL', 2, now(), now());\n\nselect @label_id := id from linkis_cg_manager_label where `label_value` = @HIVE_IDE;\ninsert into linkis_ps_configuration_category (`label_id`, `level`) VALUES (@label_id, 2);\n\n-- configuration key\nINSERT INTO `linkis_ps_configuration_config_key` (`key`, `description`, `name`, `default_value`, `validate_type`, `validate_range`, `is_hidden`, `is_advanced`, `level`, `treeName`, `engine_conn_type`) VALUES ('wds.linkis.rm.instance', 'range: 1-20, unit: piece', 'hive engine maximum concurrent number', '10', 'NumInterval', '[1,20]', '0 ', '0', '1', 'Queue resource', 'hive');\nINSERT INTO `linkis_ps_configuration_config_key` (`key`, `description`, `name`, `default_value`, `validate_type`, `validate_range`, `is_hidden`, `is_advanced`, `level`, `treeName`, `engine_conn_type`) VALUES ('wds.linkis.engineconn.java.driver.memory', 'Value range: 1-10, unit: G', 'hive engine initialization memory size', '1g', 'Regex', '^([ 1-9]|10)(G|g)$', '0', '0', '1', 'hive engine settings', 'hive');\nINSERT INTO `linkis_ps_configuration_config_key` (`key`, `description`, `name`, `default_value`, `validate_type`, `validate_range`, `is_hidden`, `is_advanced`, `level`, `treeName`, `engine_conn_type`) VALUES ('hive.client.java.opts', 'hive client process parameters', 'jvm parameters when the hive engine starts','', 'None', NULL, '1', '1', '1', 'hive engine settings', 'hive');\nINSERT INTO `linkis_ps_configuration_config_key` (`key`, `description`, `name`, `default_value`, `validate_type`, `validate_range`, `is_hidden`, `is_advanced`, `level`, `treeName`, `engine_conn_type`) VALUES ('mapred.reduce.tasks', 'Range: -1-10000, unit: number', 'reduce number', '-1', 'NumInterval', '[-1,10000]', '0', '1', '1', 'hive resource settings', 'hive');\nINSERT INTO `linkis_ps_configuration_config_key` (`key`, `description`, `name`, `default_value`, `validate_type`, `validate_range`, `is_hidden`, `is_advanced`, `level`, `treeName`, `engine_conn_type`) VALUES ('wds.linkis.engineconn.max.free.time', 'Value range: 3m,15m,30m,1h,2h', 'Engine idle exit time','1h', 'OFT', '[\\ \"1h\\\",\\\"2h\\\",\\\"30m\\\",\\\"15m\\\",\\\"3m\\\"]', '0', '0', '1', 'hive engine settings', ' hive');\n\n-- key engine relation\ninsert into `linkis_ps_configuration_key_engine_relation` (`config_key_id`, `engine_type_label_id`)\n(select config.id as `config_key_id`, label.id AS `engine_type_label_id` FROM linkis_ps_configuration_config_key config\nINNER JOIN linkis_cg_manager_label label ON config.engine_conn_type = 'hive' and label_value = @HIVE_ALL);\n\n-- engine default configuration\ninsert into `linkis_ps_configuration_config_value` (`config_key_id`, `config_value`, `config_label_id`)\n(select `relation`.`config_key_id` AS `config_key_id`, '' AS `config_value`, `relation`.`engine_type_label_id` AS `config_label_id` FROM linkis_ps_configuration_key_engine_relation relation\nINNER JOIN linkis_cg_manager_label label ON relation.engine_type_label_id = label.id AND label.label_value = @HIVE_ALL);\n")),(0,a.kt)("h2",{id:"5-hive-modification-log-display"},"5. Hive modification log display"),(0,a.kt)("p",null,"The default log interface does not display ",(0,a.kt)("inlineCode",{parentName:"p"},"application_id")," and the number of ",(0,a.kt)("inlineCode",{parentName:"p"},"task")," completed, users can output the log according to their needs\nThe code blocks that need to be modified in the ",(0,a.kt)("inlineCode",{parentName:"p"},"log4j2-engineconn.xml/log4j2.xml")," configuration file in the engine are as follows"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"Need to add under the ",(0,a.kt)("inlineCode",{parentName:"li"},"appenders")," component")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-xml"},'        <Send name="SendPackage" >\n            <PatternLayout pattern="%d{yyyy-MM-dd HH:mm:ss.SSS} %-5level [%t] %logger{36} %L %M - %msg%xEx%n"/>\n        </Send>\n')),(0,a.kt)("ol",{start:2},(0,a.kt)("li",{parentName:"ol"},"Need to add under ",(0,a.kt)("inlineCode",{parentName:"li"},"root")," component")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-xml"},'        <appender-ref ref="SendPackage"/>\n')),(0,a.kt)("ol",{start:3},(0,a.kt)("li",{parentName:"ol"},"Need to add under ",(0,a.kt)("inlineCode",{parentName:"li"},"loggers")," component")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-xml"},'        <logger name="org.apache.hadoop.hive.ql.exec.StatsTask" level="info" additivity="true">\n            <appender-ref ref="SendPackage"/>\n        </logger>\n')),(0,a.kt)("p",null,"After making the above related modifications, the log can add task ",(0,a.kt)("inlineCode",{parentName:"p"},"task")," progress information, which is displayed in the following style"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"2022-04-08 11:06:50.228 INFO  [Linkis-Default-Scheduler-Thread-3] SessionState 1111 printInfo - Status: Running (Executing on YARN cluster with App id application_1631114297082_432445)\n2022-04-08 11:06:50.248 INFO  [Linkis-Default-Scheduler-Thread-3] SessionState 1111 printInfo - Map 1: -/-  Reducer 2: 0/1  \n2022-04-08 11:06:52.417 INFO  [Linkis-Default-Scheduler-Thread-3] SessionState 1111 printInfo - Map 1: 0/1  Reducer 2: 0/1  \n2022-04-08 11:06:55.060 INFO  [Linkis-Default-Scheduler-Thread-3] SessionState 1111 printInfo - Map 1: 0(+1)/1  Reducer 2: 0/1  \n2022-04-08 11:06:57.495 INFO  [Linkis-Default-Scheduler-Thread-3] SessionState 1111 printInfo - Map 1: 1/1  Reducer 2: 0(+1)/1  \n2022-04-08 11:06:57.899 INFO  [Linkis-Default-Scheduler-Thread-3] SessionState 1111 printInfo - Map 1: 1/1  Reducer 2: 1/1  \n")),(0,a.kt)("p",null,"An example of a complete ",(0,a.kt)("inlineCode",{parentName:"p"},"xml")," configuration file is as follows:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-xml"},'\x3c!--\n  ~ Copyright 2019 WeBank\n  ~\n  ~ Licensed under the Apache License, Version 2.0 (the "License");\n  ~ you may not use this file except in compliance with the License.\n  ~ You may obtain a copy of the License at\n  ~\n  ~ http://www.apache.org/licenses/LICENSE-2.0\n  ~\n  ~ Unless required by applicable law or agreed to in writing, software\n  ~ distributed under the License is distributed on an "AS IS" BASIS,\n  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  ~ See the License for the specific language governing permissions and\n  ~ limitations under the License.\n  --\x3e\n  \n<configuration status="error" monitorInterval="30">\n    <appenders>\n        <Console name="Console" target="SYSTEM_OUT">\n            <ThresholdFilter level="INFO" onMatch="ACCEPT" onMismatch="DENY"/>\n            <PatternLayout pattern="%d{yyyy-MM-dd HH:mm:ss.SSS} %-5level [%t] %logger{36} %L %M - %msg%xEx%n"/>\n        </Console>\n\n        <Send name="Send" >\n            <Filters>\n                <ThresholdFilter level="WARN" onMatch="ACCEPT" onMismatch="DENY" />\n            </Filters>\n            <PatternLayout pattern="%d{yyyy-MM-dd HH:mm:ss.SSS} %-5level [%t] %logger{36} %L %M - %msg%xEx%n"/>\n        </Send>\n\n        <Send name="SendPackage" >\n            <PatternLayout pattern="%d{yyyy-MM-dd HH:mm:ss.SSS} %-5level [%t] %logger{36} %L %M - %msg%xEx%n"/>\n        </Send>\n\n        <Console name="stderr" target="SYSTEM_ERR">\n            <ThresholdFilter level="ERROR" onMatch="ACCEPT" onMismatch="DENY" />\n            <PatternLayout pattern="%d{HH:mm:ss.SSS} %-5level %class{36} %L %M - %msg%xEx%n"/>\n        </Console>\n    </appenders>\n\n    <loggers>\n      <root level="INFO">\n            <appender-ref ref="stderr"/>\n            <appender-ref ref="Console"/>\n            <appender-ref ref="Send"/>\n            <appender-ref ref="SendPackage"/>\n        </root>\n        <logger name="org.apache.hadoop.hive.ql.exec.StatsTask" level="info" additivity="true">\n            <appender-ref ref="SendPackage"/>\n        </logger>\n        <logger name="org.springframework.boot.diagnostics.LoggingFailureAnalysisReporter" level="error" additivity="true">\n            <appender-ref ref="stderr"/>\n        </logger>\n        <logger name="com.netflix.discovery" level="warn" additivity="true">\n            <appender-ref ref="Send"/>\n        </logger>\n        <logger name="org.apache.hadoop.yarn" level="warn" additivity="true">\n            <appender-ref ref="Send"/>\n        </logger>\n        <logger name="org.springframework" level="warn" additivity="true">\n            <appender-ref ref="Send"/>\n        </logger>\n        <logger name="org.apache.linkis.server.security" level="warn" additivity="true">\n            <appender-ref ref="Send"/>\n        </logger>\n        <logger name="org.apache.hadoop.hive.ql.exec.mr.ExecDriver" level="fatal" additivity="true">\n            <appender-ref ref="Send"/>\n        </logger>\n        <logger name="org.apache.hadoop.hdfs.KeyProviderCache" level="fatal" additivity="true">\n            <appender-ref ref="Send"/>\n        </logger>\n        <logger name="org.spark_project.jetty" level="ERROR" additivity="true">\n            <appender-ref ref="Send"/>\n        </logger>\n        <logger name="org.eclipse.jetty" level="ERROR" additivity="true">\n            <appender-ref ref="Send"/>\n        </logger>\n        <logger name="org.springframework" level="ERROR" additivity="true">\n            <appender-ref ref="Send"/>\n        </logger>\n        <logger name="org.reflections.Reflections" level="ERROR" additivity="true">\n            <appender-ref ref="Send"/>\n        </logger>\n\n        <logger name="org.apache.hadoop.ipc.Client" level="ERROR" additivity="true">\n            <appender-ref ref="Send"/>\n        </logger>\n\n   </loggers>\n</configuration>\n')))}u.isMDXComponent=!0},60533:(e,n,t)=>{t.d(n,{Z:()=>i});const i=t.p+"assets/images/hive-config-c10af8eb5c97fca44bd8f834ca163d9e.png"},51395:(e,n,t)=>{t.d(n,{Z:()=>i});const i=t.p+"assets/images/yarn-conf-3611575997ffb7ba32993da83d626e72.png"}}]);