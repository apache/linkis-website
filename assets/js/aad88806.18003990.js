"use strict";(self.webpackChunklinkis_web_apache=self.webpackChunklinkis_web_apache||[]).push([[7457],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>h});var a=n(67294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function s(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function r(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var l=a.createContext({}),p=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):s(s({},t),e)),n},c=function(e){var t=p(e.components);return a.createElement(l.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},d=a.forwardRef((function(e,t){var n=e.components,i=e.mdxType,o=e.originalType,l=e.parentName,c=r(e,["components","mdxType","originalType","parentName"]),d=p(n),h=i,m=d["".concat(l,".").concat(h)]||d[h]||u[h]||o;return n?a.createElement(m,s(s({ref:t},c),{},{components:n})):a.createElement(m,s({ref:t},c))}));function h(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var o=n.length,s=new Array(o);s[0]=d;var r={};for(var l in t)hasOwnProperty.call(t,l)&&(r[l]=t[l]);r.originalType=e,r.mdxType="string"==typeof e?e:i,s[1]=r;for(var p=2;p<o;p++)s[p]=n[p];return a.createElement.apply(null,s)}return a.createElement.apply(null,n)}d.displayName="MDXCreateElement"},37951:(e,t,n)=>{n.r(t),n.d(t,{contentTitle:()=>s,default:()=>c,frontMatter:()=>o,metadata:()=>r,toc:()=>l});var a=n(87462),i=(n(67294),n(3905));const o={title:"Quick Deployment",sidebar_position:1},s=void 0,r={unversionedId:"deployment/quick-deploy",id:"version-0.11.0/deployment/quick-deploy",isDocsHomePage:!1,title:"Quick Deployment",description:"Reminder DSS + Linkis + Qualitis + Visualis + Azkaban, please visit DSS One-Key Deployment",source:"@site/versioned_docs/version-0.11.0/deployment/quick-deploy.md",sourceDirName:"deployment",slug:"/deployment/quick-deploy",permalink:"/docs/0.11.0/deployment/quick-deploy",editUrl:"https://github.com/apache/incubator-linkis-website/edit/dev/versioned_docs/version-0.11.0/deployment/quick-deploy.md",tags:[],version:"0.11.0",sidebarPosition:1,frontMatter:{title:"Quick Deployment",sidebar_position:1},sidebar:"version-1.0.2/tutorialSidebar",previous:{title:"Introduction",permalink:"/docs/0.11.0/introduction"},next:{title:"Quick Start",permalink:"/docs/0.11.0/deployment/quick-start"}},l=[{value:"1 Determine the installation environment",id:"1-determine-the-installation-environment",children:[]},{value:"2 Simplified version of Linkis environment preparation",id:"2-simplified-version-of-linkis-environment-preparation",children:[{value:"2.1. Basic software installation",id:"21-basic-software-installation",children:[]},{value:"2.2 Create User",id:"22-create-user",children:[]},{value:"2.3 Installation package preparation",id:"23-installation-package-preparation",children:[]}]},{value:"3 Simple version of Linkis environment preparation",id:"3-simple-version-of-linkis-environment-preparation",children:[{value:"3.1 Basic software installation",id:"31-basic-software-installation",children:[]},{value:"3.2 Create User",id:"32-create-user",children:[]},{value:"3.3 SSH password-free configuration (required for distributed mode)",id:"33-ssh-password-free-configuration-required-for-distributed-mode",children:[]},{value:"3.4 Installation package preparation",id:"34-installation-package-preparation",children:[]}]},{value:"4 Standard Linkis Environment Preparation",id:"4-standard-linkis-environment-preparation",children:[{value:"4.1 Basic software installation",id:"41-basic-software-installation",children:[]},{value:"4.2 Create User",id:"42-create-user",children:[]},{value:"4.3 SSH password-free configuration (required for distributed mode)",id:"43-ssh-password-free-configuration-required-for-distributed-mode",children:[]},{value:"4.4 Installation package preparation",id:"44-installation-package-preparation",children:[]}]},{value:"5 Installation and deployment",id:"5-installation-and-deployment",children:[{value:"5.1 Execute the installation script:",id:"51-execute-the-installation-script",children:[]},{value:"5.2 Installation steps",id:"52-installation-steps",children:[]},{value:"5.3 Is the installation successful:",id:"53-is-the-installation-successful",children:[]},{value:"5.4 Quick start Linkis",id:"54-quick-start-linkis",children:[]}]},{value:"6. Quickly use Linkis",id:"6-quickly-use-linkis",children:[{value:"6.1 Overview",id:"61-overview",children:[]},{value:"6.2 Fast running",id:"62-fast-running",children:[]},{value:"6.3 Quick implementation",id:"63-quick-implementation",children:[]}]}],p={toc:l};function c(e){let{components:t,...o}=e;return(0,i.kt)("wrapper",(0,a.Z)({},p,o,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h4",{id:"reminder-if-you-want-to-experience-linkis-family-bucket-dss--linkis--qualitis--visualis--azkaban-please-visit-dss-one-key-deployment"},"Reminder: If you want to experience LINKIS Family Bucket: DSS + Linkis + Qualitis + Visualis + Azkaban, please visit ",(0,i.kt)("a",{parentName:"h4",href:"https://github.com/WeBankFinTech/DataSphereStudio/blob/master/docs/zh_CN/ch2/DSS-LINKIS-Quick-Install.md"},"DSS One-Key Deployment")),(0,i.kt)("h2",{id:"1-determine-the-installation-environment"},"1 Determine the installation environment"),(0,i.kt)("p",null,"\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","Linkis provides the following three installation environment preparation methods according to the difficulty of installation, the differences are as follows:"),(0,i.kt)("hr",null),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Lite"),":"),(0,i.kt)("p",null,"\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","Minimum environment dependency, single-node installation mode, only includes Python engine, and only needs the user's Linux environment to support Python."),(0,i.kt)("p",null,"\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","Please note: the lite version only allows users to submit Python scripts."),(0,i.kt)("hr",null),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Simple version"),":"),(0,i.kt)("p",null,"\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","depends on Python, Hadoop and Hive, distributed installation mode, including Python engine and Hive engine, requires the user's Linux environment to install Hadoop and Hive first."),(0,i.kt)("p",null,"\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","The simple version allows users to submit HiveQL and Python scripts."),(0,i.kt)("hr",null),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Standard Edition")),(0,i.kt)("p",null,"\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","Depends on Python, Hadoop, Hive and Spark, distributed installation mode, including Python engine, Hive engine and Spark engine, requires the user's Linux environment to install Hadoop first , Hive and Spark, Linkis machines rely on the cluster's hadoop/hive/spark configuration files, and do not need to be deployed with the DataNode and NameNode machines, but can be deployed on a separate Client machine."),(0,i.kt)("p",null,"\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","The standard version allows users to submit Spark scripts (including SparkSQL, Pyspark and Scala), HiveQL and Python scripts.\n",(0,i.kt)("strong",{parentName:"p"},"Please note: the installation of the standard version requires the machine's memory to be above 10G")," If the machine's memory is not enough, you need to add or modify the environment variable: ",(0,i.kt)("inlineCode",{parentName:"p"},'export SERVER_HEAP_SIZE="512M"')),(0,i.kt)("hr",null),(0,i.kt)("h2",{id:"2-simplified-version-of-linkis-environment-preparation"},"2 Simplified version of Linkis environment preparation"),(0,i.kt)("h3",{id:"21-basic-software-installation"},"2.1. Basic software installation"),(0,i.kt)("p",null,"\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","The following software must be installed:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"MySQL (5.5+), ",(0,i.kt)("a",{parentName:"li",href:"https://www.runoob.com/mysql/mysql-install.html"},"How to install MySQL")),(0,i.kt)("li",{parentName:"ul"},"JDK (above 1.8.0_141), ",(0,i.kt)("a",{parentName:"li",href:"https://www.runoob.com/java/java-environment-setup.html"},"How to install JDK")),(0,i.kt)("li",{parentName:"ul"},"Python (support both 2.x and 3.x), ",(0,i.kt)("a",{parentName:"li",href:"https://www.runoob.com/python/python-install.html"},"How to install Python"))),(0,i.kt)("h3",{id:"22-create-user"},"2.2 Create User"),(0,i.kt)("p",null,"\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","For example: ",(0,i.kt)("strong",{parentName:"p"},"Deployment user is hadoop account")),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Create a deployment user on the deployment machine for installation")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"    sudo useradd hadoop\n")),(0,i.kt)("ol",{start:2},(0,i.kt)("li",{parentName:"ol"},"Because the Linkis service uses sudo -u ${linux-user} to switch engines to perform operations, the deployment user needs to have sudo permissions and is password-free.")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"    vi /etc/sudoers\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"     hadoop ALL=(ALL) NOPASSWD: NOPASSWD: ALL\n")),(0,i.kt)("ol",{start:3},(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("strong",{parentName:"li"},"If your Python wants to have the drawing function, you also need to install the drawing module in the installation node"),". The command is as follows:")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"    python -m pip install matplotlib\n")),(0,i.kt)("h3",{id:"23-installation-package-preparation"},"2.3 Installation package preparation"),(0,i.kt)("p",null,"\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","from the released release of Linkis (",(0,i.kt)("a",{parentName:"p",href:"https://github.com/apache/incubator-linkis/releases"},"click here to enter the download page"),"), Download the latest installation package."),(0,i.kt)("p",null,"\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","First decompress the installation package to the installation directory, and modify the configuration of the decompressed files."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"    tar -xvf wedatasphere-linkis-x.x.x-dist.tar.gz\n")),(0,i.kt)("p",null,"   (1) Modify the basic configuration"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"    vi conf/config.sh\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-properties"},"    SSH_PORT=22 #Specify the SSH port, if the stand-alone version is installed, it may not be configured\n    deployUser=hadoop #Specify deployment user\n    LINKIS_HOME=/appcom/Install/Linkis # Specify the installation directory\n    WORKSPACE_USER_ROOT_PATH=file:///tmp/hadoop # Specify the user root directory, which is generally used to store the user's script files and log files, etc. It is the user's workspace.\n    RESULT_SET_ROOT_PATH=file:///tmp/linkis # The result set file path, used to store the result set file of the job\n    #HDFS_USER_ROOT_PATH=hdfs:///tmp/linkis #This parameter needs to be commented for the streamlined version installation\n")),(0,i.kt)("p",null,"   (2) Modify the database configuration"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"    vi conf/db.sh\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-properties"},"    # Set the connection information of the database\n    # Including IP address, database name, user name, port\n    # Mainly used to store user-defined variables, configuration parameters, UDF and small functions, and provide the underlying storage of JobHistory\n    MYSQL_HOST=\n    MYSQL_PORT=\n    MYSQL_DB=\n    MYSQL_USER=\n    MYSQL_PASSWORD=\n")),(0,i.kt)("p",null,"\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","The environment is ready, click me to enter ","[5-installation deployment]","(#5-installation deployment)"),(0,i.kt)("h2",{id:"3-simple-version-of-linkis-environment-preparation"},"3 Simple version of Linkis environment preparation"),(0,i.kt)("h3",{id:"31-basic-software-installation"},"3.1 Basic software installation"),(0,i.kt)("p",null,"\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","The following software must be installed:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"MySQL (5.5+), ",(0,i.kt)("a",{parentName:"li",href:"https://www.runoob.com/mysql/mysql-install.html"},"How to install MySQL")),(0,i.kt)("li",{parentName:"ul"},"JDK (above 1.8.0_141), ",(0,i.kt)("a",{parentName:"li",href:"https://www.runoob.com/java/java-environment-setup.html"},"How to install JDK")),(0,i.kt)("li",{parentName:"ul"},"Python (support both 2.x and 3.x), ",(0,i.kt)("a",{parentName:"li",href:"https://www.runoob.com/python/python-install.html"},"How to install Python")),(0,i.kt)("li",{parentName:"ul"},"Hadoop (",(0,i.kt)("strong",{parentName:"li"},"Community version and versions below CDH3.0 are supported"),")"),(0,i.kt)("li",{parentName:"ul"},"Hive (1.2.1, ",(0,i.kt)("strong",{parentName:"li"},"2.0 and above 2.0, there may be compatibility issues"),")")),(0,i.kt)("h3",{id:"32-create-user"},"3.2 Create User"),(0,i.kt)("p",null,"\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","For example: ",(0,i.kt)("strong",{parentName:"p"},"Deployment user is hadoop account")),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Create deployment users on all machines that need to be deployed for installation")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"    sudo useradd hadoop\n")),(0,i.kt)("ol",{start:2},(0,i.kt)("li",{parentName:"ol"},"Because the Linkis service uses sudo -u ${linux-user} to switch engines to perform operations, the deployment user needs to have sudo permissions and is password-free.")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"    vi /etc/sudoers\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"     hadoop ALL=(ALL) NOPASSWD: NOPASSWD: ALL\n     \n")),(0,i.kt)("ol",{start:3},(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("strong",{parentName:"li"},"Set the following global environment variables on each installation node so that Linkis can use Hadoop and Hive normally"))),(0,i.kt)("p",null,"\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","Modify the installation user's .bash_rc, the command is as follows:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"    vim /home/hadoop/.bash_rc\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"The following is an example of environment variables:\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"    #JDK\n    export JAVA_HOME=/nemo/jdk1.8.0_141\n    #HADOOP\n    export HADOOP_HOME=/appcom/Install/hadoop\n    export HADOOP_CONF_DIR=/appcom/config/hadoop-config\n    #Hive\n    export HIVE_HOME=/appcom/Install/hive\n    export HIVE_CONF_DIR=/appcom/config/hive-config\n")),(0,i.kt)("h3",{id:"33-ssh-password-free-configuration-required-for-distributed-mode"},"3.3 SSH password-free configuration (required for distributed mode)"),(0,i.kt)("p",null,"\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","If your Linkis are deployed on the same server, this step can be skipped."),(0,i.kt)("p",null,"\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","If your Linkis is deployed on multiple servers, then you also need to configure ssh password-free login for these servers."),(0,i.kt)("p",null,"\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0",(0,i.kt)("a",{parentName:"p",href:"https://www.jianshu.com/p/0922095f69f3"},"How to configure SSH password-free login")),(0,i.kt)("h3",{id:"34-installation-package-preparation"},"3.4 Installation package preparation"),(0,i.kt)("p",null,"\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","from the released release of Linkis (",(0,i.kt)("a",{parentName:"p",href:"https://github.com/apache/incubator-linkis/releases"},"click here to enter the download page"),"), Download the latest installation package."),(0,i.kt)("p",null,"\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","First decompress the installation package to the installation directory, and modify the configuration of the decompressed files."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"    tar -xvf wedatasphere-linkis-x.x.x-dist.tar.gz\n")),(0,i.kt)("p",null,"   (1) Modify the basic configuration"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"    vi /conf/config.sh\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-prop"},"erties\n   \n    deployUser=hadoop #Specify deployment user\n    LINKIS_HOME=/appcom/Install/Linkis # Specify the installation directory\n    WORKSPACE_USER_ROOT_PATH=file:///tmp/hadoop # Specify the user root directory, which is generally used to store the user's script files and log files, etc. It is the user's workspace.\n    HDFS_USER_ROOT_PATH=hdfs:///tmp/linkis # Specify the user's HDFS root directory, which is generally used to store the result set files of the job\n\n    # If you want to use it with Scriptis, the CDH version of Hive, you also need to configure the following parameters (the community version of Hive can ignore this configuration)\n    HIVE_META_URL=jdbc://... # HiveMeta Metadata Database URL\n    HIVE_META_USER= # HiveMeta Metadata Database User\n    HIVE_META_PASSWORD= # Password of HiveMeta Metabase\n\n    # Configure hadoop/hive/spark configuration directory\n    HADOOP_CONF_DIR=/appcom/config/hadoop-config #hadoop's conf directory\n    HIVE_CONF_DIR=/appcom/config/hive-config #hive's conf directory\n")),(0,i.kt)("p",null,"   (2) Modify the database configuration"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"       vi conf/db.sh\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-properties"},"\n    # Set the connection information of the database\n    # Including IP address, database name, user name, port\n    # Mainly used to store user-defined variables, configuration parameters, UDF and small functions, and provide the underlying storage of JobHistory\n    MYSQL_HOST=\n    MYSQL_PORT=\n    MYSQL_DB=\n    MYSQL_USER=\n    MYSQL_PASSWORD=\n")),(0,i.kt)("p",null,"\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","The environment is ready, click me to enter ","[5-installation deployment]","(#5-installation deployment)"),(0,i.kt)("h2",{id:"4-standard-linkis-environment-preparation"},"4 Standard Linkis Environment Preparation"),(0,i.kt)("h3",{id:"41-basic-software-installation"},"4.1 Basic software installation"),(0,i.kt)("p",null,"\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","The following software must be installed:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"MySQL (5.5+), ",(0,i.kt)("a",{parentName:"li",href:"https://www.runoob.com/mysql/mysql-install.html"},"How to install MySQL")),(0,i.kt)("li",{parentName:"ul"},"JDK (above 1.8.0_141), ",(0,i.kt)("a",{parentName:"li",href:"https://www.runoob.com/java/java-environment-setup.html"},"How to install JDK")),(0,i.kt)("li",{parentName:"ul"},"Python (support both 2.x and 3.x), ",(0,i.kt)("a",{parentName:"li",href:"https://www.runoob.com/python/python-install.html"},"How to install Python")),(0,i.kt)("li",{parentName:"ul"},"Hadoop (",(0,i.kt)("strong",{parentName:"li"},"Community version and versions below CDH3.0 are supported"),")"),(0,i.kt)("li",{parentName:"ul"},"Hive (1.2.1, ",(0,i.kt)("strong",{parentName:"li"},"2.0 and above 2.0, there may be compatibility issues"),")"),(0,i.kt)("li",{parentName:"ul"},"Spark (",(0,i.kt)("strong",{parentName:"li"},"Start from Linkis release 0.7.0, support all versions above Spark 2.0"),")")),(0,i.kt)("h3",{id:"42-create-user"},"4.2 Create User"),(0,i.kt)("p",null,"\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","For example: ",(0,i.kt)("strong",{parentName:"p"},"Deployment user is hadoop account")),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Create deployment users on all machines that need to be deployed for installation")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"    sudo useradd hadoop\n")),(0,i.kt)("ol",{start:2},(0,i.kt)("li",{parentName:"ol"},"Because the Linkis service uses sudo -u ${linux-user} to switch engines to perform operations, the deployment user needs to have sudo permissions and is password-free.")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"    vi /etc/sudoers\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-properties"},"    hadoop ALL=(ALL) NOPASSWD: NOPASSWD: ALL\n")),(0,i.kt)("ol",{start:3},(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Set the following global environment variables on each installation node so that Linkis can use Hadoop, Hive and Spark normally")),(0,i.kt)("p",{parentName:"li"},"Modify the .bash_rc of the installing user, the command is as follows:"))),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"    vim /home/hadoop/.bash_rc\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"The following is an example of environment variables:\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"    #JDK\n    export JAVA_HOME=/nemo/jdk1.8.0_141\n    #HADOOP\n    export HADOOP_HOME=/appcom/Install/hadoop\n    export HADOOP_CONF_DIR=/appcom/config/hadoop-config\n    #Hive\n    export HIVE_HOME=/appcom/Install/hive\n    export HIVE_CONF_DIR=/appcom/config/hive-config\n    #Spark\n    export SPARK_HOME=/appcom/Install/spark\n    export SPARK_CONF_DIR=/appcom/config/spark-config/spark-submit\n    export PYSPARK_ALLOW_INSECURE_GATEWAY=1 # Pyspark must add parameters\n")),(0,i.kt)("ol",{start:4},(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("strong",{parentName:"li"},"If your Pyspark wants to have the drawing function, you also need to install the drawing module on all installation nodes"),". The command is as follows:")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"    python -m pip install matplotlib\n")),(0,i.kt)("h3",{id:"43-ssh-password-free-configuration-required-for-distributed-mode"},"4.3 SSH password-free configuration (required for distributed mode)"),(0,i.kt)("p",null,"\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","If your Linkis are deployed on the same server, this step can be skipped."),(0,i.kt)("p",null,"\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","If your Linkis is deployed on multiple servers, then you also need to configure ssh password-free login for these servers."),(0,i.kt)("p",null,"\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0",(0,i.kt)("a",{parentName:"p",href:"https://www.jianshu.com/p/0922095f69f3"},"How to configure SSH password-free login")),(0,i.kt)("h3",{id:"44-installation-package-preparation"},"4.4 Installation package preparation"),(0,i.kt)("p",null,"\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","from the released release of Linkis (",(0,i.kt)("a",{parentName:"p",href:"https://github.com/apache/incubator-linkis/releases"},"click here to enter the download page"),"), Download the latest installation package."),(0,i.kt)("p",null,"\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","First decompress the installation package to the installation directory, and modify the configuration of the decompressed files."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"    tar -xvf wedatasphere-linkis-x.x.0-dist.tar.gz\n")),(0,i.kt)("p",null,"   (1) Modify the basic configuration"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"    vi conf/config.sh\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-properties"},"    SSH_PORT=22 #Specify the SSH port, if the stand-alone version is installed, it may not be configured\n    deployUser=hadoop #Specify deployment user\n    LINKIS_HOME=/appcom/Install/Linkis # Specify the installation directory\n    WORKSPACE_USER_ROOT_PATH=file:///tmp/hadoop # Specify the user root directory, which is generally used to store the user's script files and log files, etc. It is the user's workspace.\n    HDFS_USER_ROOT_PATH=hdfs:///tmp/linkis # Specify the user's HDFS root directory, which is generally used to store the result set files of the job\n\n    # If you want to use it with Scriptis, the CDH version of Hive, you also need to configure the following parameters (the community version of Hive can ignore this configuration)\n    HIVE_META_URL=jdbc://... # HiveMeta Metadata Database URL\n    HIVE_META_USER= # HiveMeta Metadata Database User\n    HIVE_META_PASSWORD= # Password of HiveMeta Metabase\n    \n    # Configure hadoop/hive/spark configuration directory\n    HADOOP_CONF_DIR=/appcom/config/hadoop-config #hadoop's conf directory\n    HIVE_CONF_DIR=/appcom/config/hive-config #hive's conf directory\n    SPARK_CONF_DIR=/appcom/config/spark-config #spark's conf directory\n")),(0,i.kt)("p",null,"   (2) Modify the database configuration"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"    vi conf/db.sh\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-properties"},"\n    # Set the connection information of the database\n    # Including IP address, database name, user name, port\n    # Mainly used to store user-defined variables, configuration parameters, UDF and small functions, and provide the underlying storage of JobHistory\n    MYSQL_HOST=\n    MYSQL_PORT=\n    MYSQL_DB=\n    MYSQL_USER=\n    MYSQL_PASSWORD=\n")),(0,i.kt)("h2",{id:"5-installation-and-deployment"},"5 Installation and deployment"),(0,i.kt)("h3",{id:"51-execute-the-installation-script"},"5.1 Execute the installation script:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"    sh bin/install.sh\n")),(0,i.kt)("h3",{id:"52-installation-steps"},"5.2 Installation steps"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"The install.sh script will ask you about the installation mode.")),(0,i.kt)("p",null,"\xa0","\xa0","\xa0","\xa0","\xa0","The installation mode is condensed mode, simple mode or standard mode. Please choose the appropriate installation mode according to the environment you prepare."),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"The install.sh script will ask you if you need to initialize the database and import metadata.")),(0,i.kt)("p",null,"\xa0","\xa0","\xa0","\xa0","\xa0","Because the user is worried that the user repeatedly executes the install.sh script to clear the user data in the database, when the install.sh is executed, the user will be asked if they need to initialize the database and import metadata."),(0,i.kt)("p",null,"\xa0","\xa0","\xa0","\xa0","\xa0",(0,i.kt)("strong",{parentName:"p"},"Yes must be selected for the first installation"),"."),(0,i.kt)("h3",{id:"53-is-the-installation-successful"},"5.3 Is the installation successful:"),(0,i.kt)("p",null,"\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","Check whether the installation is successful by viewing the log information printed on the console."),(0,i.kt)("p",null,"\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","If there is an error message, you can check the specific reason for the error."),(0,i.kt)("h3",{id:"54-quick-start-linkis"},"5.4 Quick start Linkis"),(0,i.kt)("h4",{id:"1-start-the-service"},"(1), start the service:"),(0,i.kt)("p",null,"  Execute the following command in the installation directory to start all services:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"  ./bin/start-all.sh> start.log 2>start_error.log\n")),(0,i.kt)("h4",{id:"2-check-whether-the-startup-is-successful"},"(2), check whether the startup is successful"),(0,i.kt)("p",null,"  You can check the success of the service startup on the Eureka interface, and check the method:"),(0,i.kt)("p",null,"  Use http://${EUREKA_INSTALL_IP}:${EUREKA_PORT}, open it in a browser, and view the server\nWhether the registration is successful."),(0,i.kt)("p",null,"  If you did not specify EUREKA_INSTALL_IP and EUREKA_INSTALL_IP in config.sh, the HTTP address is: ",(0,i.kt)("a",{parentName:"p",href:"http://127.0.0.1:20303"},"http://127.0.0.1:20303")),(0,i.kt)("p",null,"  As shown in the figure below, if the following microservices appear on your Eureka homepage, it means that the services have been started successfully and you can provide services to the outside world normally:"),(0,i.kt)("p",null,"  ",(0,i.kt)("strong",{parentName:"p"},"Note:")," The ones marked in red are DSS services, and the rest are services of Linkis. If you only use linkis, you can ignore the parts marked in red"),(0,i.kt)("p",null," ",(0,i.kt)("img",{alt:"Eureka",src:n(87490).Z})),(0,i.kt)("h2",{id:"6-quickly-use-linkis"},"6. Quickly use Linkis"),(0,i.kt)("h3",{id:"61-overview"},"6.1 Overview"),(0,i.kt)("p",null,"\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","Linkis provides users with a Java client implementation, and users can use UJESClient to quickly access Linkis back-end services."),(0,i.kt)("h3",{id:"62-fast-running"},"6.2 Fast running"),(0,i.kt)("p",null,"\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","We provide two test classes of UJESClient under the ujes/client/src/test module:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"    com.webank.wedatasphere.linkis.ujes.client.UJESClientImplTestJ # Java-based test class\n    com.webank.wedatasphere.linkis.ujes.client.UJESClientImplTest # Test class based on Scala implementation\n    \n")),(0,i.kt)("p",null,"\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","If you cloned the source code of Linkis, you can run these two test classes directly."),(0,i.kt)("h3",{id:"63-quick-implementation"},"6.3 Quick implementation"),(0,i.kt)("p",null,"\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0",(0,i.kt)("strong",{parentName:"p"},"The following specifically introduces how to quickly implement a linkis code submission and execution. ")),(0,i.kt)("h4",{id:"631-maven-dependency"},"6.3.1 maven dependency"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml"},"<dependency>\n  <groupId>com.webank.wedatasphere.Linkis</groupId>\n  <artifactId>Linkis-ujes-client</artifactId>\n  <version>0.11.0</version>\n</dependency>\n")),(0,i.kt)("h4",{id:"632-reference-implementation"},"6.3.2 Reference Implementation"),(0,i.kt)("p",null,"-",(0,i.kt)("strong",{parentName:"p"},"JAVA")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-java"},'package com.webank.bdp.dataworkcloud.ujes.client;\n\nimport com.webank.wedatasphere.Linkis.common.utils.Utils;\nimport com.webank.wedatasphere.Linkis.httpclient.dws.authentication.StaticAuthenticationStrategy;\nimport com.webank.wedatasphere.Linkis.httpclient.dws.config.DWSClientConfig;\nimport com.webank.wedatasphere.Linkis.httpclient.dws.config.DWSClientConfigBuilder;\nimport com.webank.wedatasphere.Linkis.ujes.client.UJESClient;\nimport com.webank.wedatasphere.Linkis.ujes.client.UJESClientImpl;\nimport com.webank.wedatasphere.Linkis.ujes.client.request.JobExecuteAction;\nimport com.webank.wedatasphere.Linkis.ujes.client.request.ResultSetAction;\nimport com.webank.wedatasphere.Linkis.ujes.client.response.JobExecuteResult;\nimport com.webank.wedatasphere.Linkis.ujes.client.response.JobInfoResult;\nimport com.webank.wedatasphere.Linkis.ujes.client.response.JobProgressResult;\nimport com.webank.wedatasphere.Linkis.ujes.client.response.JobStatusResult;\nimport org.apache.commons.io.IOUtils;\n\nimport java.util.concurrent.TimeUnit;\n\n\npublic class UJESClientImplTestJ{\n    public static void main(String[] args){\n        // 1. Configure DWSClientBuilder, get a DWSClientConfig through DWSClientBuilder\n        DWSClientConfig clientConfig = ((DWSClientConfigBuilder) (DWSClientConfigBuilder.newBuilder()\n                .addUJESServerUrl("http://${ip}:${port}") //Specify ServerUrl, the address of the Linkis server-side gateway, such as http://{ip}:{port}\n                .connectionTimeout(30000) //connectionTimeOut client connection timeout\n                .discoveryEnabled(true).discoveryFrequency(1, TimeUnit.MINUTES) //Whether to enable registration discovery, if enabled, the newly launched Gateway will be automatically discovered\n                .loadbalancerEnabled(true) // Whether to enable load balancing, if registration discovery is not enabled, load balancing is meaningless\n                .maxConnectionSize(5) //Specify the maximum number of connections, that is, the maximum number of concurrent\n                .retryEnabled(false).readTimeout(30000) //execution failed, whether to allow retry\n                .setAuthenticationStrategy(new StaticAuthenticationStrategy()) //AuthenticationStrategy Linkis authentication method\n                .setAuthTokenKey("johnnwang").setAuthTokenValue("Abcd1234"))) //Authentication key, generally the user name; authentication value, generally the password corresponding to the user name\n                .setDWSVersion("v1").build(); //Linkis backend protocol version, the current version is v1\n        \n        // 2. Get a UJESClient through DWSClientConfig\n        UJESClient client = new UJESClientImpl(clientConfig);\n\n        // 3. Start code execution\n        JobExecuteResult jobExecuteResult = client.execute(JobExecuteAction.builder()\n                .setCreator("LinkisClient-Test") //creator, requesting the system name of the Linkis client, used for system-level isolation\n                .addExecuteCode("show tables") //ExecutionCode The code to be executed\n                .setEngineType(JobExecuteAction.EngineType$.MODULE$.HIVE()) // The execution engine type of Linkis that you want to request, such as Spark hive, etc.\n                .setUser("johnnwang") //User, requesting user; used for user-level multi-tenant isolation\n                .build());\n        System.out.println("execId: "+ jobExecuteResult.getExecID() + ", taskId:" + jobExecuteResult.taskID());\n        \n        // 4. Get the execution status of the script\n        JobStatusResult status = client.status(jobExecuteResult);\n        while(!status.isCompleted()) {\n            // 5. Get the execution progress of the script\n            JobProgressResult progress = client.progress(jobExecuteResult);\n            Utils.sleepQuietly(500);\n            status = client.status(jobExecuteResult);\n        }\n        \n        // 6. Get the job information of the script\n        JobInfoResult jobInfo = client.getJobInfo(jobExecuteResult);\n        // 7. Get the list of result sets (if the user submits multiple SQL at a time, multiple result sets will be generated)\n        String resultSet = jobInfo.getResultSetList(client)[0];\n        // 8. Get a specific result set through a result set information\n        Object fileContents = client.resultSet(ResultSetAction.builder().setPath(resultSet).setUser(jobExecuteResult.getUser()).build()).getFileContent();\n        System.out.println("fileContents: "+ fileContents);\n        IOUtils.closeQuietly(client);\n    }\n}\n')),(0,i.kt)("p",null,"-",(0,i.kt)("strong",{parentName:"p"},"SCALA")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-scala"},'\nimport java.util.concurrent.TimeUnit\n\nimport com.webank.wedatasphere.Linkis.common.utils.Utils\nimport com.webank.wedatasphere.Linkis.httpclient.dws.authentication.StaticAuthenticationStrategy\nimport com.webank.wedatasphere.Lin\nkis.httpclient.dws.config.DWSClientConfigBuilder\nimport com.webank.wedatasphere.Linkis.ujes.client.request.JobExecuteAction.EngineType\nimport com.webank.wedatasphere.Linkis.ujes.client.request.{JobExecuteAction, ResultSetAction}\nimport org.apache.commons.io.IOUtils\n\nobject UJESClientImplTest extends App {\n\n  // 1. Configure DWSClientBuilder, get a DWSClientConfig through DWSClientBuilder\n  val clientConfig = DWSClientConfigBuilder.newBuilder()\n    .addUJESServerUrl("http://${ip}:${port}") //Specify ServerUrl, the address of the Linkis server-side gateway, such as http://{ip}:{port}\n    .connectionTimeout(30000) //connectionTimeOut client connection timeout\n    .discoveryEnabled(true).discoveryFrequency(1, TimeUnit.MINUTES) //Whether to enable registration discovery, if enabled, the newly launched Gateway will be automatically discovered\n    .loadbalancerEnabled(true) // Whether to enable load balancing, if registration discovery is not enabled, load balancing is meaningless\n    .maxConnectionSize(5) //Specify the maximum number of connections, that is, the maximum number of concurrent\n    .retryEnabled(false).readTimeout(30000) //execution failed, whether to allow retry\n    .setAuthenticationStrategy(new StaticAuthenticationStrategy()) //AuthenticationStrategy Linkis authentication method\n    .setAuthTokenKey("${username}").setAuthTokenValue("${password}") //Authentication key, generally the user name; authentication value, generally the password corresponding to the user name\n    .setDWSVersion("v1").build() //Linkis backend protocol version, the current version is v1\n  \n  // 2. Get a UJESClient through DWSClientConfig\n  val client = UJESClient(clientConfig)\n\n  // 3. Start code execution\n  val jobExecuteResult = client.execute(JobExecuteAction.builder()\n    .setCreator("LinkisClient-Test") //creator, requesting the system name of the Linkis client, used for system-level isolation\n    .addExecuteCode("show tables") //ExecutionCode The code to be executed\n    .setEngineType(EngineType.SPARK) // The execution engine type of Linkis that you want to request, such as Spark hive, etc.\n    .setUser("${username}").build()) //User, request user; used for user-level multi-tenant isolation\n  println("execId: "+ jobExecuteResult.getExecID + ", taskId:" + jobExecuteResult.taskID)\n  \n  // 4. Get the execution status of the script\n  var status = client.status(jobExecuteResult)\n  while(!status.isCompleted) {\n  // 5. Get the execution progress of the script\n    val progress = client.progress(jobExecuteResult)\n    val progressInfo = if(progress.getProgressInfo != null) progress.getProgressInfo.toList else List.empty\n    println("progress: "+ progress.getProgress + ", progressInfo:" + progressInfo)\n    Utils.sleepQuietly(500)\n    status = client.status(jobExecuteResult)\n  }\n  \n  // 6. Get the job information of the script\n  val jobInfo = client.getJobInfo(jobExecuteResult)\n  // 7. Get the list of result sets (if the user submits multiple SQL at a time, multiple result sets will be generated)\n  val resultSet = jobInfo.getResultSetList(client).head\n  // 8. Get a specific result set through a result set information\n  val fileContents = client.resultSet(ResultSetAction.builder().setPath(resultSet).setUser(jobExecuteResult.getUser).build()).getFileContent\n  println("fileContents: "+ fileContents)\n  IOUtils.closeQuietly(client)\n}\n')))}c.isMDXComponent=!0},87490:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/Eureka_homepage-de09b1b756300a8b4878cfd9b547ea86.png"}}]);