"use strict";(self.webpackChunklinkis_web_apache=self.webpackChunklinkis_web_apache||[]).push([[37473],{3905:(e,n,t)=>{t.d(n,{Zo:()=>p,kt:()=>k});var a=t(67294);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function s(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function o(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?s(Object(t),!0).forEach((function(n){r(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):s(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function l(e,n){if(null==e)return{};var t,a,r=function(e,n){if(null==e)return{};var t,a,r={},s=Object.keys(e);for(a=0;a<s.length;a++)t=s[a],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(a=0;a<s.length;a++)t=s[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var i=a.createContext({}),c=function(e){var n=a.useContext(i),t=n;return e&&(t="function"==typeof e?e(n):o(o({},n),e)),t},p=function(e){var n=c(e.components);return a.createElement(i.Provider,{value:n},e.children)},u="mdxType",d={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},m=a.forwardRef((function(e,n){var t=e.components,r=e.mdxType,s=e.originalType,i=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),u=c(t),m=r,k=u["".concat(i,".").concat(m)]||u[m]||d[m]||s;return t?a.createElement(k,o(o({ref:n},p),{},{components:t})):a.createElement(k,o({ref:n},p))}));function k(e,n){var t=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var s=t.length,o=new Array(s);o[0]=m;var l={};for(var i in n)hasOwnProperty.call(n,i)&&(l[i]=n[i]);l.originalType=e,l[u]="string"==typeof e?e:r,o[1]=l;for(var c=2;c<s;c++)o[c]=t[c];return a.createElement.apply(null,o)}return a.createElement.apply(null,t)}m.displayName="MDXCreateElement"},96200:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>i,contentTitle:()=>o,default:()=>d,frontMatter:()=>s,metadata:()=>l,toc:()=>c});var a=t(87462),r=(t(67294),t(3905));const s={title:"Support spark ETL data synchronization",sidebar_position:.4},o=void 0,l={unversionedId:"feature/spark-etl",id:"feature/spark-etl",title:"Support spark ETL data synchronization",description:"1. Background",source:"@site/docs/feature/spark-etl.md",sourceDirName:"feature",slug:"/feature/spark-etl",permalink:"/docs/1.6.0/feature/spark-etl",draft:!1,editUrl:"https://github.com/apache/linkis-website/edit/dev/docs/feature/spark-etl.md",tags:[],version:"current",sidebarPosition:.4,frontMatter:{title:"Support spark ETL data synchronization",sidebar_position:.4},sidebar:"tutorialSidebar",previous:{title:"hive engine supports concurrency and multiplexing",permalink:"/docs/1.6.0/feature/hive-engine-support-concurrent"},next:{title:"Generate SQL according to the data source",permalink:"/docs/1.6.0/feature/datasource-generate-sql"}},i={},c=[{value:"1. Background",id:"1-background",level:2},{value:"2. Supported types",id:"2-supported-types",level:2},{value:"3. General configuration instructions",id:"3-general-configuration-instructions",level:2},{value:"4. Instructions for use",id:"4-instructions-for-use",level:2},{value:"4.1 Add the required jar package",id:"41-add-the-required-jar-package",level:3},{value:"4.2 linkis-cli submit task example",id:"42-linkis-cli-submit-task-example",level:3},{value:"4.3 Synchronization json script description of each data source",id:"43-synchronization-json-script-description-of-each-data-source",level:3},{value:"4.3.1 jdbc",id:"431-jdbc",level:4},{value:"4.3.2 file",id:"432-file",level:4},{value:"4.3.3 redis",id:"433-redis",level:4},{value:"4.3.4 kafka",id:"434-kafka",level:4},{value:"4.3.5 elasticsearch",id:"435-elasticsearch",level:4},{value:"4.3.6 mongo",id:"436-mongo",level:4},{value:"4.3.7 delta",id:"437-delta",level:4},{value:"4.3.8 hudi",id:"438-hudi",level:4}],p={toc:c},u="wrapper";function d(e){let{components:n,...t}=e;return(0,r.kt)(u,(0,a.Z)({},p,t,{components:n,mdxType:"MDXLayout"}),(0,r.kt)("h2",{id:"1-background"},"1. Background"),(0,r.kt)("p",null,"Using the Spark ETL function, users can synchronize Spark data by configuring json."),(0,r.kt)("h2",{id:"2-supported-types"},"2. Supported types"),(0,r.kt)("p",null,"currently supported types"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-text"},"jdbc, file, redis, kafka, elasticsearch, mongo, datalake (hudi, delta)\n")),(0,r.kt)("h2",{id:"3-general-configuration-instructions"},"3. General configuration instructions"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-text"},"name: data source name\ntype: Contains `source`, `transformation`, `sink`, corresponding to input, transformation, and output respectively\noptions: configuration parameters\nsaveMode: save mode, currently supports: `overwrite` and `append`\npath: file path, can be: 'file://' or 'hdfs://'(default)\n`resultTable` needs to correspond to `sourceTable`\n")),(0,r.kt)("h2",{id:"4-instructions-for-use"},"4. Instructions for use"),(0,r.kt)("h3",{id:"41-add-the-required-jar-package"},"4.1 Add the required jar package"),(0,r.kt)("p",null,"When using the data source, you need to upload the corresponding spark connector jar to the spark/jars directory, the directory location is $SPARK_HOME/jars"),(0,r.kt)("p",null,"The spark connector jar can be obtained by the following command"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-text"},"git clone https://github.com/apache/linkis.git\n\ncd link is\n\ngit checkout master\n\ncd linkis-engineconn-plugins/spark/scala-2.12\n\nmvn clean install -Dmaven.test.skip=true\n")),(0,r.kt)("p",null,"The compiled spark connector jar is located in the following directory"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-text"},"linkis/linkis-engineconn-plugins/spark/scala-2.12/target/out/spark/dist/3.2.1/lib\n")),(0,r.kt)("h3",{id:"42-linkis-cli-submit-task-example"},"4.2 linkis-cli submit task example"),(0,r.kt)("p",null,"Just pass in the specific json code in code, pay attention to the conversion of quotation marks."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},'sh /appcom/Install/linkis/bin/linkis-cli -engineType spark-3.2.1 -codeType data_calc -code "" -submitUser hadoop -proxyUser hadoop\n')),(0,r.kt)("p",null,"Linkis-cli submits redis data synchronization task example"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},'sh ./bin/linkis-cli -engineType spark-3.2.1 -codeType data_calc -code "{\\"plugins\\":[{\\"name\\":\\"file\\",\\"type\\":\\" source\\",\\"config\\":{\\"resultTable\\":\\"test\\",\\"path\\":\\"hdfs://linkishdfs/tmp/linkis/spark_etl_test/etltest.dolphin\\",\\ "serializer\\":\\"csv\\",\\"options\\":{\\"header\\":\\"true\\",\\"delimiter\\":\\";\\"},\\"columnNames\\":[ \\"name\\",\\"age\\"]}},{\\"name\\":\\"redis\\",\\"type\\":\\"sink\\",\\"config\\":{\\"sourceTable \\":\\"test\\",\\"host\\":\\"wds07\\",\\"port\\":\\"6679\\",\\"auth\\":\\"password\\",\\"targetTable\\" :\\"spark_etl_test\\",\\"saveMode\\":\\"append\\"}}]}" -submitUser hadoop -proxyUser hadoop\n')),(0,r.kt)("h3",{id:"43-synchronization-json-script-description-of-each-data-source"},"4.3 Synchronization json script description of each data source"),(0,r.kt)("h4",{id:"431-jdbc"},"4.3.1 jdbc"),(0,r.kt)("p",null,"Configuration instructions"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-text"},"url: jdbc connection information\nuser: user name\npassword: password\nquery: sql query statement\n")),(0,r.kt)("p",null,"json code"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n    "sources": [\n        {\n            "name": "jdbc",\n            "type": "source",\n            "config": {\n                "resultTable": "test1",\n                "url": "jdbc:mysql://127.0.0.1:3306/dip_linkis?characterEncoding=UTF-8",\n                "driver": "com.mysql.jdbc.Driver",\n                "user": "root",\n                "password": "123456",\n                "query": "select * from dip_linkis.linkis_ps_udf_baseinfo",\n                "options": {\n                }\n            }\n        }\n    ],\n    "transformations": [\n        {\n            "name": "sql",\n            "type": "transformation",\n            "config": {\n                "resultTable": "T1654611700631",\n                "sql": "select * from test1"\n            }\n        }\n    ],\n    "sinks": [\n        {\n            "name": "jdbc",\n            "type": "sink",\n            "config": {\n                "sourceTable": "T1654611700631",\n                "url": "jdbc:mysql://127.0.0.1:3306/dip_linkis?characterEncoding=UTF-8",\n                "driver": "com.mysql.jdbc.Driver",\n                "user": "root",\n                "password": "123456",\n                "targetTable": "linkis_ps_udf_baseinfo2",\n                "options": {\n                }\n            }\n        }\n    ]\n}\n')),(0,r.kt)("p",null,"A new jar needs to be added, and the corresponding jar should be selected according to the specific data source used"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-text"},"DmJdbcDriver18.jar\nkingbase8-8.6.0.jar\npostgresql-42.3.8.jar\n")),(0,r.kt)("h4",{id:"432-file"},"4.3.2 file"),(0,r.kt)("p",null,"Configuration instructions"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-text"},"serializer: file format, can be `csv`, `parquet`, etc.\ncolumnNames: column names\n")),(0,r.kt)("p",null,"json code"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n    "sources": [\n        {\n            "name": "file",\n            "type": "source",\n            "config": {\n                "resultTable": "test2",\n                "path": "hdfs:///tmp/test_new_no_partition",\n                "serializer": "csv",\n                "columnNames": ["id", "create_user", "udf_name", "udf_type", "tree_id", "create_time", "update_time", "sys", "cluster_name", "is_expire", "is_shared"]\n            }\n        }\n    ],\n    "sinks": [\n        {\n            "name": "file",\n            "config": {\n                "sourceTable": "test2",\n                "path": "hdfs:///tmp/test_new",\n                "partitionBy": ["create_user"],\n                "saveMode": "overwrite",\n                "serializer": "csv"\n            }\n        }\n    ]\n}\n')),(0,r.kt)("p",null,"Need to add new jar"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"spark-excel-2.12.17-3.2.2_2.12-3.2.2_0.18.1.jar\n")),(0,r.kt)("h4",{id:"433-redis"},"4.3.3 redis"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-text"},'sourceTable: source table,\nhost: ip address,\nport": port,\nauth": password,\ntargetTable: target table,\nsaveMode: support append\n')),(0,r.kt)("p",null,"json code"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n  "plugins":[\n    {\n      "name": "file",\n      "type": "source",\n      "config": {\n        "resultTable": "test",\n        "path": "hdfs://linkishdfs/tmp/linkis/spark_etl_test/etltest.dolphin",\n        "serializer": "csv",\n        "options": {\n          "header": "true",\n          "delimiter": ";"\n        },\n        "columnNames": ["name", "age"]\n      }\n    },\n    {\n      "name": "redis",\n      "type": "sink",\n      "config": {\n        "sourceTable": "test",\n        "host": "wds07",\n        "port": "6679",\n        "auth": "password",\n        "targetTable": "spark_etl_test",\n        "saveMode": "append"\n      }\n    }\n  ]\n}\n')),(0,r.kt)("p",null,"Need to add new jar"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-text"},"jedis-3.2.0.jar\ncommons-pool2-2.8.1.jar\nspark-redis_2.12-2.6.0.jar\n")),(0,r.kt)("h4",{id:"434-kafka"},"4.3.4 kafka"),(0,r.kt)("p",null,"Configuration instructions"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-text"},"servers: kafka connection information\nmode: currently supports `batch` and `stream`\ntopic: kafka topic name\n")),(0,r.kt)("p",null,"Data written to json code"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n    "sources": [\n        {\n            "name": "file",\n            "type": "source",\n            "config": {\n                "resultTable": "T1654611700631",\n                "path": "file://{filePath}/etltest.dolphin",\n                "serializer": "csv",\n                "options": {\n                "header": "true",\n                "delimiter": ";"\n                },\n                "columnNames": ["name", "age"]\n            }\n        }\n    ],\n    "sinks": [\n        {\n            "name": "kafka",\n            "config": {\n                "sourceTable": "T1654611700631",\n                "servers": "localhost:9092",\n                "mode": "batch",\n                "topic": "test121212"\n            }\n        }\n    ]\n}\n')),(0,r.kt)("p",null,"Data read json code"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n    "sources": [\n        {\n            "name": "kafka",\n            "type": "source",\n            "config": {\n                "resultTable": "T1654611700631",\n                "servers": "localhost:9092",\n                "topic": "test121212"\n            }\n        }\n    ],\n    "sinks": [\n        {\n            "name": "kafka",\n            "config": {\n                "sourceTable": "T1654611700631",\n                "servers": "localhost:9092",\n                "mode": "stream",\n                "topic": "test55555"\n            }\n        }\n    ]\n}\n')),(0,r.kt)("p",null,"Need to add new jar"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"kafka-clients-2.8.0.jar\nspark-sql-kafka-0-10_2.12-3.2.1.jar\nspark-token-provider-kafka-0-10_2.12-3.2.1.jar\n")),(0,r.kt)("h4",{id:"435-elasticsearch"},"4.3.5 elasticsearch"),(0,r.kt)("p",null,"Configuration instructions"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-text"},"node: elasticsearch ip\nport: elasticsearch port\nindex: elasticsearch index name\n")),(0,r.kt)("p",null,"Data written to json code"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n    "sources": [\n        {\n            "name": "file",\n            "type": "source",\n            "config": {\n                "resultTable": "T1654611700631",\n                "path": "file://{filePath}/etltest.dolphin",\n                "serializer": "csv",\n                "options": {\n                "header": "true",\n                "delimiter": ";"\n                },\n                "columnNames": ["name", "age"]\n            }\n        }\n    ],\n    "sinks": [\n        {\n            "name": "elasticsearch",\n            "config": {\n                "sourceTable": "T1654611700631",\n                "node": "localhost",\n                "port": "9200",\n                "index": "estest",\n                "saveMode": "overwrite"\n            }\n        }\n    ]\n}\n')),(0,r.kt)("p",null,"Data read json code"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n    "sources": [\n        {\n            "name": "elasticsearch",\n            "type": "source",\n            "config": {\n                "resultTable": "T1654611700631",\n                "node": "localhost",\n                "port": "9200",\n                "index": "estest"\n            }\n        }\n    ],\n    "sinks": [\n        {\n            "name": "file",\n            "config": {\n                "sourceTable": "T1654611700631",\n                "path": "file://{filePath}/csv",\n                "saveMode": "overwrite",\n                "serializer": "csv"\n            }\n        }\n    ]\n}\n')),(0,r.kt)("p",null,"Need to add new jar"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"elasticsearch-spark-30_2.12-7.17.7.jar\n")),(0,r.kt)("h4",{id:"436-mongo"},"4.3.6 mongo"),(0,r.kt)("p",null,"Configuration instructions"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-text"},"uri: mongo connection information\ndatabase: mongo database\ncollection: mongo collection\n")),(0,r.kt)("p",null,"Data written to json code"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n    "sources": [\n        {\n            "name": "file",\n            "type": "source",\n            "config": {\n                "resultTable": "T1654611700631",\n                "path": "file://{filePath}/etltest.dolphin",\n                "serializer": "csv",\n                "options": {\n                "header": "true",\n                "delimiter": ";"\n                },\n                "columnNames": ["name", "age"]\n            }\n        }\n    ],\n    "sinks": [\n        {\n            "name": "mongo",\n            "config": {\n                "sourceTable": "T1654611700631",\n                "uri": "mongodb://localhost:27017/test",\n                "database": "test",\n                "collection": "test",\n                "saveMode": "overwrite"\n            }\n        }\n    ]\n}\n')),(0,r.kt)("p",null,"Data read json code"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n    "sources": [\n        {\n            "name": "mongo",\n            "type": "source",\n            "config": {\n                "resultTable": "T1654611700631",\n                "uri": "mongodb://localhost:27017/test",\n                "database": "test",\n                "collection": "test"\n            }\n        }\n    ],\n    "sinks": [\n        {\n            "name": "file",\n            "config": {\n                "sourceTable": "T1654611700631",\n                "path": "file://{filePath}/json",\n                "saveMode": "overwrite",\n                "serializer": "json"\n            }\n        }\n    ]\n}\n')),(0,r.kt)("p",null,"Need to add new jar"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"bson-3.12.8.jar\nmongo-spark-connector_2.12-3.0.1.jar\nmongodb-driver-core-3.12.8.jar\nmongodb-driver-sync-3.12.8.jar\n")),(0,r.kt)("h4",{id:"437-delta"},"4.3.7 delta"),(0,r.kt)("p",null,"Configuration instructions"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-text"},"tableFormat: currently supports `hudi` and `delta`\n")),(0,r.kt)("p",null,"Data written to json code"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n  "sources": [\n    {\n      "name": "file",\n      "type": "source",\n      "config": {\n        "resultTable": "T1654611700631",\n        "path": "file://{filePath}/etltest.dolphin",\n        "serializer": "csv",\n        "options": {\n          "header": "true",\n          "delimiter": ";"\n        },\n        "columnNames": ["name", "age"]\n      }\n    }\n  ],\n  "sinks": [\n    {\n      "name": "datalake",\n      "config": {\n        "sourceTable": "T1654611700631",\n        "tableFormat": "delta",\n        "path": "file://{filePath}/delta",\n        "saveMode": "overwrite"\n      }\n    }\n  ]\n}\n')),(0,r.kt)("p",null,"Data read json code"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n  "sources": [\n    {\n      "name": "datalake",\n      "type": "source",\n      "config": {\n        "resultTable": "T1654611700631",\n        "tableFormat": "delta",\n        "path": "file://{filePath}/delta",\n      }\n    }\n  ],\n  "sinks": [\n    {\n      "name": "file",\n      "config": {\n        "sourceTable": "T1654611700631",\n        "path": "file://{filePath}/csv",\n        "saveMode": "overwrite",\n        "options": {\n          "header": "true"\n        },\n        "serializer": "csv"\n      }\n    }\n  ]\n}\n')),(0,r.kt)("p",null,"Need to add new jar"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"delta-core_2.12-2.0.2.jar\ndelta-storage-2.0.2.jar\n")),(0,r.kt)("h4",{id:"438-hudi"},"4.3.8 hudi"),(0,r.kt)("p",null,"Configuration instructions"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-text"},"tableFormat: currently supports `hudi` and `delta`\n")),(0,r.kt)("p",null,"Data written to json code"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n  "sources": [\n    {\n      "name": "file",\n      "type": "source",\n      "config": {\n        "resultTable": "T1654611700631",\n        "path": "file://{filePath}/etltest.dolphin",\n        "serializer": "csv",\n        "options": {\n          "header": "true",\n          "delimiter": ";"\n        },\n        "columnNames": ["name", "age"]\n      }\n    }\n  ],\n  "transformations": [\n    {\n      "name": "sql",\n      "type": "transformation",\n      "config": {\n        "resultTable": "T111",\n        "sql": "select * from T1654611700631"\n      }\n    }\n  ],\n  "sinks": [\n    {\n      "name": "datalake",\n      "config": {\n        "sourceTable": "T1654611700631",\n        "tableFormat": "hudi",\n        "options": {\n          "hoodie.table.name": "huditest",\n          "hoodie.datasource.write.recordkey.field": "age",\n          "hoodie.datasource.write.precombine.field":"age"\n        },\n        "path": "file://{filePath}/hudi",\n        "saveMode": "append"\n      }\n    }\n  ]\n}\n')),(0,r.kt)("p",null,"Data read json code"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n  "sources": [\n    {\n      "name": "datalake",\n      "type": "source",\n      "config": {\n        "resultTable": "T1654611700631",\n        "tableFormat": "hudi",\n        "path": "file://{filePath}/hudi",\n      }\n    }\n  ],\n  "transformations": [\n    {\n      "name": "sql",\n      "type": "transformation",\n      "config": {\n        "resultTable": "T111",\n        "sql": "select * from T1654611700631"\n      }\n    }\n  ],\n  "sinks": [\n    {\n      "name": "file",\n      "config": {\n        "sourceTable": "T1654611700631",\n        "path": "file://{filePath}/csv",\n        "saveMode": "overwrite",\n        "options": {\n          "header": "true"\n        },\n        "serializer": "csv"\n      }\n    }\n  ]\n}\n')),(0,r.kt)("p",null,"Need to add new jar"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"hudi-spark3.2-bundle_2.12-0.13.0.jar\n")))}d.isMDXComponent=!0}}]);