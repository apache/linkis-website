"use strict";(self.webpackChunklinkis_web_apache=self.webpackChunklinkis_web_apache||[]).push([[48751],{3905:(e,n,a)=>{a.d(n,{Zo:()=>d,kt:()=>k});var t=a(67294);function r(e,n,a){return n in e?Object.defineProperty(e,n,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[n]=a,e}function i(e,n){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),a.push.apply(a,t)}return a}function o(e){for(var n=1;n<arguments.length;n++){var a=null!=arguments[n]?arguments[n]:{};n%2?i(Object(a),!0).forEach((function(n){r(e,n,a[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(a,n))}))}return e}function s(e,n){if(null==e)return{};var a,t,r=function(e,n){if(null==e)return{};var a,t,r={},i=Object.keys(e);for(t=0;t<i.length;t++)a=i[t],n.indexOf(a)>=0||(r[a]=e[a]);return r}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(t=0;t<i.length;t++)a=i[t],n.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var l=t.createContext({}),p=function(e){var n=t.useContext(l),a=n;return e&&(a="function"==typeof e?e(n):o(o({},n),e)),a},d=function(e){var n=p(e.components);return t.createElement(l.Provider,{value:n},e.children)},c="mdxType",u={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},m=t.forwardRef((function(e,n){var a=e.components,r=e.mdxType,i=e.originalType,l=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),c=p(a),m=r,k=c["".concat(l,".").concat(m)]||c[m]||u[m]||i;return a?t.createElement(k,o(o({ref:n},d),{},{components:a})):t.createElement(k,o({ref:n},d))}));function k(e,n){var a=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var i=a.length,o=new Array(i);o[0]=m;var s={};for(var l in n)hasOwnProperty.call(n,l)&&(s[l]=n[l]);s.originalType=e,s[c]="string"==typeof e?e:r,o[1]=s;for(var p=2;p<i;p++)o[p]=a[p];return t.createElement.apply(null,o)}return t.createElement.apply(null,a)}m.displayName="MDXCreateElement"},75058:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>i,metadata:()=>s,toc:()=>p});var t=a(87462),r=(a(67294),a(3905));const i={title:"Installation Spark lineage",sidebar_position:1},o=void 0,s={unversionedId:"deployment/integrated/spark-lineage",id:"deployment/integrated/spark-lineage",title:"Installation Spark lineage",description:"This paper mainly introduces the 'Spark' engine blood collection scheme in 'Linkis'.",source:"@site/docs/deployment/integrated/spark-lineage.md",sourceDirName:"deployment/integrated",slug:"/deployment/integrated/spark-lineage",permalink:"/docs/1.5.0/deployment/integrated/spark-lineage",draft:!1,editUrl:"https://github.com/apache/linkis-website/edit/dev/docs/deployment/integrated/spark-lineage.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{title:"Installation Spark lineage",sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Version Adaptation",permalink:"/docs/1.5.0/deployment/version-adaptation"},next:{title:"Installation Scriptis Tool",permalink:"/docs/1.5.0/deployment/integrated/install-scriptis"}},l={},p=[{value:"1. Introduction",id:"1-introduction",level:2},{value:"2. Download the required jar package for <code>spline-spark-agent</code>",id:"2-download-the-required-jar-package-for-spline-spark-agent",level:2},{value:"3. The spark lineage collected into the log",id:"3-the-spark-lineage-collected-into-the-log",level:2},{value:"3.1 Modify <code>spark-defaults.conf</code>",id:"31-modify-spark-defaultsconf",level:3},{value:"3.2 Data preparation",id:"32-data-preparation",level:3},{value:"3.3 Submit task",id:"33-submit-task",level:3},{value:"3.4 View logs",id:"34-view-logs",level:3},{value:"4. The spark lineage collected into the kafka",id:"4-the-spark-lineage-collected-into-the-kafka",level:2},{value:"4.1 Modify <code>spark-defaults.conf</code>",id:"41-modify-spark-defaultsconf",level:3},{value:"4.2 Submit task",id:"42-submit-task",level:3},{value:"4.3 View topic",id:"43-view-topic",level:3},{value:"5. More ways",id:"5-more-ways",level:2}],d={toc:p},c="wrapper";function u(e){let{components:n,...i}=e;return(0,r.kt)(c,(0,t.Z)({},d,i,{components:n,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"This paper mainly introduces the 'Spark' engine blood collection scheme in 'Linkis'."),(0,r.kt)("h2",{id:"1-introduction"},"1. Introduction"),(0,r.kt)("p",null,"The Spline Agent for Apache Spark is a complementary module to the Spline project that captures runtime lineage information from the Apache Spark jobs."),(0,r.kt)("p",null,"github address"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"https://github.com/AbsaOSS/spline-spark-agent\n")),(0,r.kt)("h2",{id:"2-download-the-required-jar-package-for-spline-spark-agent"},"2. Download the required jar package for ",(0,r.kt)("inlineCode",{parentName:"h2"},"spline-spark-agent")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"cd $SPARK_HOME/jars\n\nwget https://repo1.maven.org/maven2/za/co/absa/spline/agent/spark/spark-3.2-spline-agent-bundle_2.12/2.0.0/spark-3.2-spline-agent-bundle_2.12-2.0.0.jar\n")),(0,r.kt)("p",null,"When the download is complete, ",(0,r.kt)("inlineCode",{parentName:"p"},"$SPARK_HOME/jars")," will appear ",(0,r.kt)("inlineCode",{parentName:"p"},"spark-3.2-spline-agent-bundle_2.12-2.0.0.jar")),(0,r.kt)("h2",{id:"3-the-spark-lineage-collected-into-the-log"},"3. The spark lineage collected into the log"),(0,r.kt)("h3",{id:"31-modify-spark-defaultsconf"},"3.1 Modify ",(0,r.kt)("inlineCode",{parentName:"h3"},"spark-defaults.conf")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"vim $SPARK_HOME/conf/spark-defaults.conf\n\nAdd the following configuration\nspark.sql.queryExecutionListeners=za.co.absa.spline.harvester.listener.SplineQueryExecutionListener\nspark.spline.lineageDispatcher=log\nspark.spline.lineageDispatcher.log.level=INFO\nspark.spline.lineageDispatcher.log.className=za.co.absa.spline.harvester.dispatcher.LoggingLineageDispatcher\n")),(0,r.kt)("h3",{id:"32-data-preparation"},"3.2 Data preparation"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},'Create input files and upload them to hdfs\n\nvim read.json\n\n{"name":"linkis","age":"5"}\n\nhadoop fs -put read.json /tmp\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"Create output directory\nhadoop fs -mkdir /tmp/jsonWrite\n")),(0,r.kt)("h3",{id:"33-submit-task"},"3.3 Submit task"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"sh ./bin/linkis-cli -engineType spark-3.2.1 -codeType sql -code \\\n\"CREATE TEMPORARY VIEW jsonReadTable\nUSING org.apache.spark.sql.json\nOPTIONS (\n  path '/tmp/read.json'\n);\nINSERT OVERWRITE DIRECTORY '/tmp/jsonWrite' SELECT * FROM jsonReadTable;\"  \\\n-submitUser hadoop -proxyUser hadoop\n")),(0,r.kt)("h3",{id:"34-view-logs"},"3.4 View logs"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"cat /appcom/tmp/hadoop/20230829/spark/117ca887-f9d6-4923-8ca1-cef7155ee0e7/logs/stdout \n")),(0,r.kt)("p",null,"The output is as follows:\n",(0,r.kt)("img",{alt:"spark-lineage-log",src:a(97169).Z,width:"1624",height:"214"})),(0,r.kt)("p",null,"Details are as follows:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n    "id":"a5b273b3-a87f-5a30-8ced-c8eeff2d1458",\n    "name":"Linkis-EngineConn-Spark_LINKISCLI",\n    "operations":{\n        "write":{\n            "outputSource":"/tmp/jsonWrite",\n            "append":false,\n            "id":"op-0",\n            "name":"InsertIntoHiveDirCommand",\n            "childIds":[\n                "op-1"\n            ],\n            "extra":{\n                "destinationType":"hive"\n            }\n        },\n        "reads":[\n            {\n                "inputSources":[\n                    "hdfs://linkishdfs/tmp/read.json"\n                ],\n                "id":"op-4",\n                "name":"LogicalRelation",\n                "output":[\n                    "attr-0",\n                    "attr-1"\n                ],\n                "params":{\n                    "path":"/tmp/read.json"\n                },\n                "extra":{\n                    "sourceType":"json"\n                }\n            }\n        ],\n        "other":[\n            {\n                "id":"op-3",\n                "name":"View",\n                "childIds":[\n                    "op-4"\n                ],\n                "output":[\n                    "attr-0",\n                    "attr-1"\n                ],\n                "params":{\n                    "desc":"CatalogTable(\\nTable: jsonReadTable\\nCreated Time: Tue Aug 29 11:52:10 CST 2023\\nLast Access: UNKNOWN\\nCreated By: Spark \\nType: VIEW\\nTable Properties: []\\nSchema: root\\n |-- age: string (nullable = true)\\n |-- name: string (nullable = true)\\n)",\n                    "isTempView":true\n                }\n            },\n            {\n                "id":"op-2",\n                "name":"SubqueryAlias",\n                "childIds":[\n                    "op-3"\n                ],\n                "output":[\n                    "attr-0",\n                    "attr-1"\n                ],\n                "params":{\n                    "identifier":"jsonreadtable"\n                }\n            },\n            {\n                "id":"op-1",\n                "name":"Project",\n                "childIds":[\n                    "op-2"\n                ],\n                "output":[\n                    "attr-0",\n                    "attr-1"\n                ],\n                "params":{\n                    "projectList":[\n                        {\n                            "__attrId":"attr-0"\n                        },\n                        {\n                            "__attrId":"attr-1"\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    "attributes":[\n        {\n            "id":"attr-0",\n            "dataType":"e63adadc-648a-56a0-9424-3289858cf0bb",\n            "name":"age"\n        },\n        {\n            "id":"attr-1",\n            "dataType":"e63adadc-648a-56a0-9424-3289858cf0bb",\n            "name":"name"\n        }\n    ],\n    "expressions":{\n\n    },\n    "systemInfo":{\n        "name":"spark",\n        "version":"3.2.1"\n    },\n    "agentInfo":{\n        "name":"spline",\n        "version":"2.0.0"\n    },\n    "extraInfo":{\n        "appName":"Linkis-EngineConn-Spark_LINKISCLI",\n        "dataTypes":[\n            {\n                "id":"e63adadc-648a-56a0-9424-3289858cf0bb",\n                "name":"string",\n                "nullable":true,\n                "_typeHint":"dt.Simple"\n            }\n        ]\n    }\n}\n')),(0,r.kt)("h2",{id:"4-the-spark-lineage-collected-into-the-kafka"},"4. The spark lineage collected into the kafka"),(0,r.kt)("h3",{id:"41-modify-spark-defaultsconf"},"4.1 Modify ",(0,r.kt)("inlineCode",{parentName:"h3"},"spark-defaults.conf")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"vim $SPARK_HOME/conf/spark-defaults.conf\n\nAdd the following configuration\nspark.sql.queryExecutionListeners=za.co.absa.spline.harvester.listener.SplineQueryExecutionListener\nspark.spline.lineageDispatcher=kafka\nspark.spline.lineageDispatcher.kafka.topic=linkis_spark_lineage_test\nspark.spline.lineageDispatcher.kafka.producer.bootstrap.servers=localhost:9092\n")),(0,r.kt)("h3",{id:"42-submit-task"},"4.2 Submit task"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"sh ./bin/linkis-cli -engineType spark-3.2.1 -codeType sql -code \\\n\"CREATE TEMPORARY VIEW jsonReadTable\nUSING org.apache.spark.sql.json\nOPTIONS (\n  path '/tmp/read.json'\n);\nINSERT OVERWRITE DIRECTORY '/tmp/jsonWrite' SELECT * FROM jsonReadTable;\"  \\\n-submitUser hadoop -proxyUser hadoop\n")),(0,r.kt)("h3",{id:"43-view-topic"},"4.3 View topic"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"kafka/bin/kafka-console-consumer.sh  --topic linkis_spark_lineage_test --from-beginning --bootstrap-server localhost:9092\n")),(0,r.kt)("p",null,"The output is as follows:\n",(0,r.kt)("img",{alt:"spark-lineage-kafka",src:a(235).Z,width:"1628",height:"367"})),(0,r.kt)("p",null,"Details are as follows:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n    "id":"3a0e2b8e-11dc-5bd1-9bbc-cfba2fa469e9",\n    "name":"Linkis-EngineConn-Spark_LINKISCLI",\n    "operations":{\n        "write":{\n            "outputSource":"/tmp/jsonWrite",\n            "append":false,\n            "id":"op-0",\n            "name":"InsertIntoHiveDirCommand",\n            "childIds":[\n                "op-1"\n            ],\n            "extra":{\n                "destinationType":"hive"\n            }\n        },\n        "reads":[\n            {\n                "inputSources":[\n                    "hdfs://linkishdfs/tmp/read.json"\n                ],\n                "id":"op-4",\n                "name":"LogicalRelation",\n                "output":[\n                    "attr-0",\n                    "attr-1"\n                ],\n                "params":{\n                    "path":"/tmp/read.json"\n                },\n                "extra":{\n                    "sourceType":"json"\n                }\n            }\n        ],\n        "other":[\n            {\n                "id":"op-3",\n                "name":"View",\n                "childIds":[\n                    "op-4"\n                ],\n                "output":[\n                    "attr-0",\n                    "attr-1"\n                ],\n                "params":{\n                    "desc":"CatalogTable(\\nTable: jsonReadTable\\nCreated Time: Tue Aug 29 14:48:06 CST 2023\\nLast Access: UNKNOWN\\nCreated By: Spark \\nType: VIEW\\nTable Properties: []\\nSchema: root\\n |-- age: string (nullable = true)\\n |-- name: string (nullable = true)\\n)",\n                    "isTempView":true\n                }\n            },\n            {\n                "id":"op-2",\n                "name":"SubqueryAlias",\n                "childIds":[\n                    "op-3"\n                ],\n                "output":[\n                    "attr-0",\n                    "attr-1"\n                ],\n                "params":{\n                    "identifier":"jsonreadtable"\n                }\n            },\n            {\n                "id":"op-1",\n                "name":"Project",\n                "childIds":[\n                    "op-2"\n                ],\n                "output":[\n                    "attr-0",\n                    "attr-1"\n                ],\n                "params":{\n                    "projectList":[\n                        {\n                            "__attrId":"attr-0"\n                        },\n                        {\n                            "__attrId":"attr-1"\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    "attributes":[\n        {\n            "id":"attr-0",\n            "dataType":"e63adadc-648a-56a0-9424-3289858cf0bb",\n            "name":"age"\n        },\n        {\n            "id":"attr-1",\n            "dataType":"e63adadc-648a-56a0-9424-3289858cf0bb",\n            "name":"name"\n        }\n    ],\n    "expressions":{\n\n    },\n    "systemInfo":{\n        "name":"spark",\n        "version":"3.2.1"\n    },\n    "agentInfo":{\n        "name":"spline",\n        "version":"2.0.0"\n    },\n    "extraInfo":{\n        "appName":"Linkis-EngineConn-Spark_LINKISCLI",\n        "dataTypes":[\n            {\n                "id":"e63adadc-648a-56a0-9424-3289858cf0bb",\n                "name":"string",\n                "nullable":true,\n                "_typeHint":"dt.Simple"\n            }\n        ]\n    }\n}\n')),(0,r.kt)("h2",{id:"5-more-ways"},"5. More ways"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-text"},"`spline-spark-agent` also supports more collection modes, such as Http and Console. For details, see the official documentation\nhttps://github.com/AbsaOSS/spline-spark-agent/#configuration\n")))}u.isMDXComponent=!0},235:(e,n,a)=>{a.d(n,{Z:()=>t});const t=a.p+"assets/images/spark-lineage-kafka-068606f3757638694ac31024ac1e22ac.png"},97169:(e,n,a)=>{a.d(n,{Z:()=>t});const t=a.p+"assets/images/spark-lineage-log-d9b6a49b9407a1376fc66fd44703a28b.png"}}]);