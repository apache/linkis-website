"use strict";(self.webpackChunklinkis_web_apache=self.webpackChunklinkis_web_apache||[]).push([[44448],{3905:(e,n,t)=>{t.d(n,{Zo:()=>d,kt:()=>m});var a=t(67294);function i(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function s(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?o(Object(t),!0).forEach((function(n){i(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function r(e,n){if(null==e)return{};var t,a,i=function(e,n){if(null==e)return{};var t,a,i={},o=Object.keys(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||(i[t]=e[t]);return i}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var l=a.createContext({}),p=function(e){var n=a.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):s(s({},n),e)),t},d=function(e){var n=p(e.components);return a.createElement(l.Provider,{value:n},e.children)},c={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},h=a.forwardRef((function(e,n){var t=e.components,i=e.mdxType,o=e.originalType,l=e.parentName,d=r(e,["components","mdxType","originalType","parentName"]),h=p(t),m=i,u=h["".concat(l,".").concat(m)]||h[m]||c[m]||o;return t?a.createElement(u,s(s({ref:n},d),{},{components:t})):a.createElement(u,s({ref:n},d))}));function m(e,n){var t=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var o=t.length,s=new Array(o);s[0]=h;var r={};for(var l in n)hasOwnProperty.call(n,l)&&(r[l]=n[l]);r.originalType=e,r.mdxType="string"==typeof e?e:i,s[1]=r;for(var p=2;p<o;p++)s[p]=t[p];return a.createElement.apply(null,s)}return a.createElement.apply(null,t)}h.displayName="MDXCreateElement"},17312:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>c,frontMatter:()=>o,metadata:()=>r,toc:()=>p});var a=t(87462),i=(t(67294),t(3905));const o={title:"Linkis1.1.1 adapts Hadoop 3.1.1 and deploys other services",authors:["ruY9527"],tags:["blog","linki1.1.1","hadoop3.1.1","spark3.0.1","hive3.1.2","flink1.13.2"]},s=void 0,r={permalink:"/blog/2022/08/08/linkis111-compile-integration",editUrl:"https://github.com/apache/incubator-linkis-website/edit/dev/blog/2022-08-08-linkis111-compile-integration/index.md",source:"@site/blog/2022-08-08-linkis111-compile-integration/index.md",title:"Linkis1.1.1 adapts Hadoop 3.1.1 and deploys other services",description:"Environment and Version",date:"2022-08-08T00:00:00.000Z",formattedDate:"August 8, 2022",tags:[{label:"blog",permalink:"/blog/tags/blog"},{label:"linki1.1.1",permalink:"/blog/tags/linki-1-1-1"},{label:"hadoop3.1.1",permalink:"/blog/tags/hadoop-3-1-1"},{label:"spark3.0.1",permalink:"/blog/tags/spark-3-0-1"},{label:"hive3.1.2",permalink:"/blog/tags/hive-3-1-2"},{label:"flink1.13.2",permalink:"/blog/tags/flink-1-13-2"}],readingTime:13.675,truncated:!1,authors:[{name:"ruY9527",title:"contributors",url:"https://github.com/ruY9527",imageURL:"https://avatars.githubusercontent.com/u/43773582?v=4",key:"ruY9527"}],prevItem:{title:"Deploy Apache Linkis1.1.1 and DSS1.1.0 based on CDH6.3.2",permalink:"/blog/2022/09/27/linkis111-deploy"},nextItem:{title:"Deploy Linkis with Kubernetes",permalink:"/blog/2022/07/16/deploy-linkis-with-kubernetes"}},l={authorsImageUrls:[void 0]},p=[{value:"Environment and Version",id:"environment-and-version",children:[]},{value:"Scenarios and versions of each component",id:"scenarios-and-versions-of-each-component",children:[]},{value:"Deployment sequence",id:"deployment-sequence",children:[]},{value:"Dependency adjustment and packaging",id:"dependency-adjustment-and-packaging",children:[{value:"linkis",id:"linkis",children:[]},{value:"DataSphereStudio",id:"dataspherestudio",children:[]},{value:"Schedulis",id:"schedulis",children:[]},{value:"Qualitis",id:"qualitis",children:[]},{value:"Exchangis",id:"exchangis",children:[]},{value:"Visualis",id:"visualis",children:[]},{value:"Streamis",id:"streamis",children:[]}]},{value:"Installation deployment",id:"installation-deployment",children:[{value:"Path unification",id:"path-unification",children:[]},{value:"Notes on linkis deployment",id:"notes-on-linkis-deployment",children:[]},{value:"Considerations for DSS deployment",id:"considerations-for-dss-deployment",children:[]},{value:"Schedulis deployment considerations",id:"schedulis-deployment-considerations",children:[]},{value:"Considerations for qualitis deployment",id:"considerations-for-qualitis-deployment",children:[]},{value:"Exchange deployment considerations",id:"exchange-deployment-considerations",children:[]},{value:"Visualis",id:"visualis-1",children:[]},{value:"Streamis",id:"streamis-1",children:[]},{value:"dss-appconn",id:"dss-appconn",children:[]}]},{value:"Nginx deployment example",id:"nginx-deployment-example",children:[]}],d={toc:p};function c(e){let{components:n,...o}=e;return(0,i.kt)("wrapper",(0,a.Z)({},d,o,{components:n,mdxType:"MDXLayout"}),(0,i.kt)("h2",{id:"environment-and-version"},"Environment and Version"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"jdk-8 , maven-3.6.3"),(0,i.kt)("li",{parentName:"ul"},"node-14.15.0(Compiling the front end requires)"),(0,i.kt)("li",{parentName:"ul"},"Gradle-4.6(Compile qualitis quality service)"),(0,i.kt)("li",{parentName:"ul"},"hadoop-3.1.1,Spark-3.0.1,Hive-3.1.2,Flink-1.13.2,Sqoop-1.4.7 (Apache version)"),(0,i.kt)("li",{parentName:"ul"},"linkis-1.1.1"),(0,i.kt)("li",{parentName:"ul"},"DataSphereStudio-1.1.0"),(0,i.kt)("li",{parentName:"ul"},"Schudulis-0.7.0"),(0,i.kt)("li",{parentName:"ul"},"Qualitis-0.9.2"),(0,i.kt)("li",{parentName:"ul"},"Visualis-1.0.0"),(0,i.kt)("li",{parentName:"ul"},"Streamis-0.2.0"),(0,i.kt)("li",{parentName:"ul"},"Exchangis-1.0.0"),(0,i.kt)("li",{parentName:"ul"},"Chrome recommends versions below 100")),(0,i.kt)("h2",{id:"scenarios-and-versions-of-each-component"},"Scenarios and versions of each component"),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:null},"System name"),(0,i.kt)("th",{parentName:"tr",align:null},"Version"),(0,i.kt)("th",{parentName:"tr",align:null},"scene"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"linkis"),(0,i.kt)("td",{parentName:"tr",align:null},"1.1.1"),(0,i.kt)("td",{parentName:"tr",align:null},"Engine orchestration, running and executing hive, spark, flinksql, shell, python, etc., unified data source management, etc")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"DataSphereStudio"),(0,i.kt)("td",{parentName:"tr",align:null},"1.1.0"),(0,i.kt)("td",{parentName:"tr",align:null},"Implement DAG scheduling of tasks, integrate the specifications of other systems and provide unified access, and provide sparksql based service API")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"Schudulis"),(0,i.kt)("td",{parentName:"tr",align:null},"0.7.0"),(0,i.kt)("td",{parentName:"tr",align:null},"Task scheduling, as well as scheduling details and rerouting, and provide trap data based on the selected time")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"Qualitis"),(0,i.kt)("td",{parentName:"tr",align:null},"0.9.2"),(0,i.kt)("td",{parentName:"tr",align:null},"Provide built-in SQL version and other functions, check common data quality and customizable SQL, verify some data that does not conform to the rules, and write it to the corresponding library")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"Exchangis"),(0,i.kt)("td",{parentName:"tr",align:null},"1.0.0"),(0,i.kt)("td",{parentName:"tr",align:null},"Hive to MySQL, data exchange between MySQL and hive")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"Streamis"),(0,i.kt)("td",{parentName:"tr",align:null},"0.2.0"),(0,i.kt)("td",{parentName:"tr",align:null},"Streaming development and Application Center")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"Visualis"),(0,i.kt)("td",{parentName:"tr",align:null},"1.0.0"),(0,i.kt)("td",{parentName:"tr",align:null},"Visual report display, can share external links")))),(0,i.kt)("h2",{id:"deployment-sequence"},"Deployment sequence"),(0,i.kt)("p",null,"  You can select and adjust the sequence after serial number 3 However, one thing to pay attention to when deploying exchangis is to copy the sqoop engine plug-in of exchangis to the engine plug-in package under lib of linkis\nSchedulis, qualitis, exchangis, streamis, visualis and other systems are integrated with DSS through their respective appconn. Note that after integrating the component appconn, restart the service module corresponding to DSS or restart DSS"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"linkis"),(0,i.kt)("li",{parentName:"ol"},"DataSphereStudio"),(0,i.kt)("li",{parentName:"ol"},"Schedulis"),(0,i.kt)("li",{parentName:"ol"},"Qualitis"),(0,i.kt)("li",{parentName:"ol"},"Exchangis"),(0,i.kt)("li",{parentName:"ol"},"Streamis"),(0,i.kt)("li",{parentName:"ol"},"Visualis")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"image.png",src:t(76945).Z})),(0,i.kt)("p",null,"If you integrate skywalking, you can see the service status and connection status in the extended topology diagram, as shown in the following figure:\n",(0,i.kt)("img",{alt:"image.png",src:t(9524).Z}),"\nAt the same time, you can also clearly see the call link in the trace, as shown in the following figure, which is also convenient for you to locate the error log file of the specific service\n",(0,i.kt)("img",{alt:"image.png",src:t(66959).Z})),(0,i.kt)("h2",{id:"dependency-adjustment-and-packaging"},"Dependency adjustment and packaging"),(0,i.kt)("h3",{id:"linkis"},"linkis"),(0,i.kt)("p",null,"Since spark uses version 3. X, Scala also needs to be upgraded to version 12\n",(0,i.kt)("a",{parentName:"p",href:"https://github.com/apache/incubator-linkis/tree/release-1.1.1"},"Original project code address"),"\n",(0,i.kt)("a",{parentName:"p",href:"https://github.com/ruY9527/incubator-linkis/tree/release-1.1.1-hadoop3.x"},"Adaptation modification code reference address")),(0,i.kt)("h4",{id:"the-pom-file-of-linkis"},"The pom file of linkis"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml"},"<hadoop.version>3.1.1</hadoop.version>\n<scala.version>2.12.10</scala.version>\n<scala.binary.version>2.12</scala.binary.version>\n\n\x3c!-- hadoop-hdfs replace with hadoop-hdfs-client --\x3e\n<dependency>\n    <groupId>org.apache.hadoop</groupId>\n    <artifactId>hadoop-hdfs-client</artifactId>\n    <version>${hadoop.version}</version>\n")),(0,i.kt)("h4",{id:"the-pom-file-of-linkis-hadoop-common"},"The pom file of linkis-hadoop-common"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml"},"       \x3c!-- Notice here <version>${hadoop.version}</version> , adjust according to whether you have encountered any errors --\x3e \n       <dependency>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-hdfs-client</artifactId>\n            <version>${hadoop.version}</version>\n        </dependency>\n")),(0,i.kt)("h4",{id:"the-pom-file-of-linkis-engineplugin-hive"},"The pom file of linkis-engineplugin-hive"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml"},"<hive.version>3.1.2</hive.version>\n")),(0,i.kt)("h4",{id:"the-pom-file-of-linkis-engineplugin-spark"},"The pom file of linkis-engineplugin-spark"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml"},"<spark.version>3.0.1</spark.version>\n")),(0,i.kt)("p",null,"The getfield method in sparkscalaexecutor needs to adjust the following code"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-java"},'protected def getField(obj: Object, name: String): Object = {\n    // val field = obj.getClass.getField(name)\n    val field = obj.getClass.getDeclaredField("in0")\n        field.setAccessible(true)\n        field.get(obj)\n  }\n')),(0,i.kt)("h4",{id:"the-pom-file-of-linkis-engineplugin-flink"},"The pom file of linkis-engineplugin-flink"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml"},"<flink.version>1.13.2</flink.version>\n")),(0,i.kt)("p",null,'Due to the adjustment of some classes in Flink 1.12.2 and 1.13.2, we refer to the temporary "violence" method given by the community students: copy the classes in part 1.12.2 to 1.13.2, adjust the scala version to 12, and compile them by ourselves\nIt involves the specific modules of flink: flink-sql-client_${scala.binary.version}'),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"-- Note that the following classes are copied from 1.12.2 to 1.13.2\norg.apache.flink.table.client.config.entries.DeploymentEntry\norg.apache.flink.table.client.config.entries.ExecutionEntry\norg.apache.flink.table.client.gateway.local.CollectBatchTableSink\norg.apache.flink.table.client.gateway.local.CollectStreamTableSink\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"image.png",src:t(34373).Z}),(0,i.kt)("img",{alt:"image.png",src:t(87058).Z})),(0,i.kt)("h4",{id:"linkis-engineplugin-python"},"linkis-engineplugin-python"),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://github.com/apache/incubator-linkis/commit/7a26e85c53fc7cd55ddefbd78b1748b00f85ddd6"},"Reference pr"),"\nIf resource / Python's python In the PY file, there is import pandas as PD. If you do not want to install pandas, you need to remove it"),(0,i.kt)("h4",{id:"linkis-label-common"},"linkis-label-common"),(0,i.kt)("p",null,"org.apache.linkis.manager.label.conf.LabelCommonConfig\nModify the default version to facilitate the use of subsequent self compilation scheduling components"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'    public static final CommonVars<String> SPARK_ENGINE_VERSION =\n            CommonVars.apply("wds.linkis.spark.engine.version", "3.0.1");\n\n    public static final CommonVars<String> HIVE_ENGINE_VERSION =\n            CommonVars.apply("wds.linkis.hive.engine.version", "3.1.2");\n')),(0,i.kt)("h4",{id:"linkis-computation-governance-common"},"linkis-computation-governance-common"),(0,i.kt)("p",null,"org.apache.linkis.governance.common.conf.GovernanceCommonConf\nModify the default version to facilitate the use of subsequent self compilation scheduling components"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'  val SPARK_ENGINE_VERSION = CommonVars("wds.linkis.spark.engine.version", "3.0.1")\n\n  val HIVE_ENGINE_VERSION = CommonVars("wds.linkis.hive.engine.version", "3.1.2")\n')),(0,i.kt)("h4",{id:"compile"},"Compile"),(0,i.kt)("p",null,"Ensure that the above modifications and environments are available and implemented in sequence"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-shell"},"    cd incubator-linkis-x.x.x\n    mvn -N  install\n    mvn clean install -DskipTests\n")),(0,i.kt)("h4",{id:"compilation-error-troubleshooting"},"Compilation error troubleshooting"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"If there is an error when you compile, try to enter a module to compile separately to see if there is an error and adjust it according to the specific error"),(0,i.kt)("li",{parentName:"ul"},"For example, the following example (the py4j version does not adapt when the group Friends adapt to the lower version of CDH): if you encounter this problem, you can adjust the version with the corresponding method to determine whether to adapt")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"image.png",src:t(96500).Z})),(0,i.kt)("h3",{id:"dataspherestudio"},"DataSphereStudio"),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://github.com/WeBankFinTech/DataSphereStudio/tree/1.1.0"},"Original project code address"),"\n",(0,i.kt)("a",{parentName:"p",href:"https://github.com/ruY9527/DataSphereStudio/tree/1.1.0-hadoop3.x"},"Adaptation modification code reference address")),(0,i.kt)("h4",{id:"the-pom-file-of-dataspherestudio"},"The pom file of DataSphereStudio"),(0,i.kt)("p",null,"Since DSS relies on linkis, all compilers should compile linkis before compiling DSS"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml"},"\x3c!-- scala consistent environment --\x3e\n<scala.version>2.12.10</scala.version>\n")),(0,i.kt)("h4",{id:"dss-dolphinschuduler-token"},"dss-dolphinschuduler-token"),(0,i.kt)("p",null,"DolphinSchedulerTokenRestfulApi: Remove type conversion"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'responseRef.getValue("expireTime")\n')),(0,i.kt)("h4",{id:"web-tuning"},"web tuning"),(0,i.kt)("p",null," ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/WeBankFinTech/DataSphereStudio-Doc/blob/main/zh_CN/%E5%BC%80%E5%8F%91%E6%96%87%E6%A1%A3/%E5%89%8D%E7%AB%AF%E7%BC%96%E8%AF%91%E6%96%87%E6%A1%A3.md"},"Front end compilation address"),"\n",(0,i.kt)("a",{parentName:"p",href:"https://github.com/WeBankFinTech/DataSphereStudio/commit/1dc9d99648e9f78b2dfb4776df4b9f46ef530c8a"},"Reference pr"),"\nOverwrite the contents of the following directories from the master branch, or build the web based on the master branch\n",(0,i.kt)("img",{alt:"image.png",src:t(59180).Z})),(0,i.kt)("h4",{id:"compile-1"},"Compile"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-shell"},"    cd DataSphereStudio\n    mvn -N  install\n    mvn clean install -DskipTests\n")),(0,i.kt)("h3",{id:"schedulis"},"Schedulis"),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://github.com/WeBankFinTech/Schedulis/tree/release-0.7.0"},"Original project code address"),"\n",(0,i.kt)("a",{parentName:"p",href:"https://github.com/ruY9527/Schedulis/tree/release-0.7.0-hadoop.x"},"Adaptation modification code reference address")),(0,i.kt)("h4",{id:"the-pom-file-of-schedulis"},"The pom file of Schedulis"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml"},"       <hadoop.version>3.1.1</hadoop.version>\n       <hive.version>3.1.2</hive.version>\n       <spark.version>3.0.1</spark.version>\n")),(0,i.kt)("h4",{id:"azkaban-jobtype"},"azkaban-jobtype"),(0,i.kt)("p",null,"Download the jobtype file of the corresponding version (note the corresponding version): ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/WeBankFinTech/Schedulis/blob/master/docs/schedulis_deploy_cn.md"},"Download address:"),"\nAfter downloading, put the entire jobtypes under jobtypes\n",(0,i.kt)("img",{alt:"image.png",src:t(30058).Z})),(0,i.kt)("h3",{id:"qualitis"},"Qualitis"),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://github.com/WeBankFinTech/Qualitis/tree/release-0.9.2"},"Original project code address")),(0,i.kt)("h4",{id:"forgerock-package-download"},"Forgerock package download"),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://github.com/WeBankFinTech/Qualitis/releases"},"release\u5730\u5740")," of ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/WeBankFinTech/Qualitis/releases/tag/release-0.9.1"},"release-0.9.1"),",after decompression, put it under. m2\\repository\\org"),(0,i.kt)("h4",{id:"compile-2"},"Compile"),(0,i.kt)("p",null,"Gradle version 4.6"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-shell"},"cd Qualitis\ngradle clean distZip\n")),(0,i.kt)("p",null,"After compiling, there will be a qualitis-0.9.2.zip file under qualitis\n",(0,i.kt)("img",{alt:"image.png",src:t(71897).Z})),(0,i.kt)("h4",{id:"dss-qualitis-appconn-compile"},"dss-qualitis-appconn compile"),(0,i.kt)("p",null,"Copy the appconn to the appconns under datasphere studio (create the DSS quality appconn folder), as shown in the following figure:\nCompile the DSS qualitis appconn. The qualitis under out is the package of integrating qualitis with DSS\n",(0,i.kt)("img",{alt:"image.png",src:t(62690).Z})),(0,i.kt)("h3",{id:"exchangis"},"Exchangis"),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://github.com/WeBankFinTech/Exchangis/tree/release-1.0.0"},"Original project code address"),"\n",(0,i.kt)("a",{parentName:"p",href:"https://github.com/ruY9527/Exchangis/tree/release-1.0.0-hadoop3.x"},"Adaptation modification code reference address")),(0,i.kt)("h4",{id:"the-pom-file-of-exchangis"},"The pom file of Exchangis"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml"},"\x3c!-- scala Consistent version --\x3e\n<scala.version>2.12.10</scala.version>\n")),(0,i.kt)("h4",{id:"back-end-compilation"},"Back end compilation"),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://github.com/WeBankFinTech/Exchangis/blob/dev-1.0.0/docs/zh_CN/ch1/exchangis_deploy_cn.md"},"Official compiled documents"),"\nIn the target package of the assembly package, wedatasphere-exchangis-1.0.0.tar.gz is its own service package\nLinkis engineplug sqoop needs to be put into linkis (lib/linkis enginecon plugins)\nExchangis-appconn.zip needs to be put into DSS (DSS appconns)"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml"},"mvn clean install \n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"image.png",src:t(30052).Z})),(0,i.kt)("h4",{id:"front-end-compilation"},"Front end compilation"),(0,i.kt)("p",null,"If you deploy the front-end using nginx yourself, you need to pay attention to the dist folder under dist\n",(0,i.kt)("img",{alt:"image.png",src:t(84155).Z})),(0,i.kt)("h3",{id:"visualis"},"Visualis"),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://github.com/WeBankFinTech/Visualis/tree/v1.0.0"},"Original project code address"),"\n",(0,i.kt)("a",{parentName:"p",href:"https://github.com/ruY9527/Visualis/tree/v1.0.0-hadoop3.x"},"Adaptation modification code reference address")),(0,i.kt)("h4",{id:"the-pom-file-of-visualis"},"The pom file of Visualis"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml"},"<scala.version>2.12.10</scala.version>\n")),(0,i.kt)("h4",{id:"compile-3"},"Compile"),(0,i.kt)("p",null," ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/WeBankFinTech/Visualis/blob/master/visualis_docs/zh_CN/Visualis_deploy_doc_cn.md"},"Official compiled documents"),"\nIn the target under assembly, visuis server zip is the package of its own service\nThe target of visualis appconn is visualis.zip, which is the package required by DSS (DSS appconns)\nBuild is the package printed by the front end"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml"},"cd Visualis\nmvn -N install\nmvn clean package -DskipTests=true\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"image.png",src:t(33765).Z})),(0,i.kt)("h3",{id:"streamis"},"Streamis"),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://github.com/WeBankFinTech/Streamis/tree/0.2.0"},"Original project code address"),"\n",(0,i.kt)("a",{parentName:"p",href:"https://github.com/ruY9527/Streamis/tree/0.2.0-hadoop3.x"},"Adaptation modification code reference address")),(0,i.kt)("h4",{id:"the-pom-file-of-streamis"},"The pom file of Streamis"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml"},"<scala.version>2.12.10</scala.version>\n")),(0,i.kt)("p",null,"The pom file of streamis-project-server"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml"},"       \x3c!-- If you are 1.0.1 here, adjust it to ${dss.version} --\x3e\n       <dependency>\n            <groupId>com.webank.wedatasphere.dss</groupId>\n            <artifactId>dss-sso-integration-standard</artifactId>\n            <version>${dss.version}</version>\n            <scope>compile</scope>\n        </dependency>\n")),(0,i.kt)("h4",{id:"compile-4"},"Compile"),(0,i.kt)("p",null," ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/WeBankFinTech/Streamis/blob/main/docs/zh_CN/0.2.0/Streamis%E5%AE%89%E8%A3%85%E6%96%87%E6%A1%A3.md"},"Official compiled documents"),"\nUnder assembly, the target package wedatasphere-streams-0.2.0-dist.tar.gz is the package of its own back-end service\nThe stream.zip package of target under stream appconn is required by DSS (DSS appconns)\ndist under dist is the front-end package"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml"},"cd ${STREAMIS_CODE_HOME}\nmvn -N install\nmvn clean install\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"image.png",src:t(10737).Z})),(0,i.kt)("h2",{id:"installation-deployment"},"Installation deployment"),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://linkis.apache.org/zh-CN/docs/1.1.1/deployment/quick-deploy"},"Official deployment address"),"\n",(0,i.kt)("a",{parentName:"p",href:"https://linkis.apache.org/zh-CN/blog/2022/02/21/linkis-deploy"},"Common error address")),(0,i.kt)("h3",{id:"path-unification"},"Path unification"),(0,i.kt)("p",null,"It is recommended to deploy the relevant components in the same path (for example, I unzip them all in /home/hadoop/application)\n",(0,i.kt)("img",{alt:"image.png",src:t(13962).Z})),(0,i.kt)("h3",{id:"notes-on-linkis-deployment"},"Notes on linkis deployment"),(0,i.kt)("h4",{id:"deploy-config-folder"},"Deploy config folder"),(0,i.kt)("p",null,"db.sh, the address of the links connection configured by mysql, and the metadata connection address of hive\nlinkis-env.sh"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-shell"},"-- The path to save the script script. Next time, there will be a folder with the user's name, and the script of the corresponding user will be stored in this folder\nWORKSPACE_USER_ROOT_PATH=file:///home/hadoop/logs/linkis\n-- Log files for storing materials and engine execution\nHDFS_USER_ROOT_PATH=hdfs:///tmp/linkis\n-- Log of each execution of the engine and information related to starting engineconnexec.sh\nENGINECONN_ROOT_PATH=/home/hadoop/logs/linkis/apps\n-- Access address of yarn master node (active resource manager)\nYARN_RESTFUL_URL\n-- Conf address of Hadoop / hive / spark\nHADOOP_CONF_DIR\nHIVE_CONF_DIR\nSPARK_CONF_DIR\n-- Specify the corresponding version\nSPARK_VERSION=3.0.1\nHIVE_VERSION=3.1.2\n-- Specify the path after the installation of linkis. For example, I agree to specify the path under the corresponding component here\nLINKIS_HOME=/home/hadoop/application/linkis/linkis-home\n")),(0,i.kt)("h4",{id:"flink"},"flink"),(0,i.kt)("p",null,"If you use Flink, you can try importing it from ",(0,i.kt)("a",{target:"_blank",href:t(94846).Z},"flink-engine.sql"),"  into the database of linkis"),(0,i.kt)("p",null,"Need to modify @Flink_LABEL version is the corresponding version, and the queue of yarn is default by default"),(0,i.kt)("p",null,'At the same time, in this version, if you encounter the error of "1g" converting digital types, try to remove the 1g unit and the regular check rules. Refer to the following:'),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"flink3.png",src:t(88667).Z})),(0,i.kt)("h4",{id:"lzo"},"lzo"),(0,i.kt)("p",null,"If your hive uses LZO, copy the corresponding LZO jar package to the hive path. For example, the following path:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"lib/linkis-engineconn-plugins/hive/dist/v3.1.2/lib\n")),(0,i.kt)("h4",{id:"frequently-asked-questions-and-precautions"},"Frequently asked questions and precautions"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"The MySQL driver package must be copied to /lib/linkis-commons/public-module/ and /lib/linkis-spring-cloud-services/linkis-mg-gateway/"),(0,i.kt)("li",{parentName:"ul"},"Initialization password in conf/linkis-mg-gateway.properties -> wds.linkis.admin.password"),(0,i.kt)("li",{parentName:"ul"},"ps-cs in the startup script,there may be failures, if any,use sh linkis-daemon.sh ps-cs , start it separately"),(0,i.kt)("li",{parentName:"ul"},"At present, if there is time to back up the log, sometimes if the previous error log cannot be found, it may be backed up to the folder of the corresponding date"),(0,i.kt)("li",{parentName:"ul"},"At present lib/linkis-engineconn-plugins have only spark/shell/python/hive,If you want appconn, flink and sqoop, go to DSS, linkis and exchangis to get them"),(0,i.kt)("li",{parentName:"ul"},"Configuration file version check")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-shell"},"linkis.properties,flink see if it is used\nwds.linkis.spark.engine.version=3.0.1\nwds.linkis.hive.engine.version=3.1.2\nwds.linkis.flink.engine.version=1.13.2\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"image.png",src:t(91569).Z}),"\n",(0,i.kt)("img",{alt:"image.png",src:t(68324).Z})),(0,i.kt)("h4",{id:"error-record"},"Error record"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Incompatible versions. If you encounter the following error, it is whether the scala version is not completely consistent. Check and compile it")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"1905943989d7782456c356b6ce0d72b.png",src:t(51165).Z})),(0,i.kt)("ol",{start:2},(0,i.kt)("li",{parentName:"ol"},"Yarn configures the active node address. If the standby address is configured, the following error will appear:")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"1ca32f79d940016d72bf1393e4bccc8.jpg",src:t(10397).Z})),(0,i.kt)("h3",{id:"considerations-for-dss-deployment"},"Considerations for DSS deployment"),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://github.com/WeBankFinTech/DataSphereStudio-Doc/tree/main/zh_CN"},"Official installation document")),(0,i.kt)("h4",{id:"config-folder"},"config folder"),(0,i.kt)("p",null,"db.sh: configure the database of DSS\nconfig.sh"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-shell"},"-- The installation path of DSS, for example, is defined in the folder under DSS\nDSS_INSTALL_HOME=/home/hadoop/application/dss/dss\n")),(0,i.kt)("h4",{id:"conf-folder"},"conf folder"),(0,i.kt)("p",null,"dss.properties"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-properties"},"# Mainly check whether spark / hive and other versions are available. If not, add\nwds.linkis.spark.engine.version=3.0.1\nwds.linkis.hive.engine.version=3.1.2\nwds.linkis.flink.engine.version=1.13.2\n")),(0,i.kt)("p",null,"dss-flow-execution-server.properties"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-properties"},"# Mainly check whether spark / hive and other versions are available. If not, add\nwds.linkis.spark.engine.version=3.0.1\nwds.linkis.hive.engine.version=3.1.2\nwds.linkis.flink.engine.version=1.13.2\n")),(0,i.kt)("p",null,"If you want to use dolphin scheduler for scheduling, please add the corresponding spark / hive version to this pr\n",(0,i.kt)("a",{parentName:"p",href:"https://github.com/WeBankFinTech/DataSphereStudio/pull/914/files"},"Reference pr")),(0,i.kt)("h4",{id:"dss-appconns"},"dss-appconns"),(0,i.kt)("p",null,"Exchangis, qualitis, streamis and visualis should be obtained from the projects of exchangis, qualitis, streamis and visualis respectively"),(0,i.kt)("h4",{id:"frequently-asked-questions-and-precautions-1"},"Frequently asked questions and precautions"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Since we integrate scheduleis, qualitis, exchangis and other components into DSS, all the interfaces of these components will be called synchronously when creating a project, so we ensure that dss_appconn_instance  configuration paths in the instance are correct and accessible"),(0,i.kt)("li",{parentName:"ul"},"The Chrome browser recommends that the kernel use version 100 or below. Otherwise, there will be a problem that you can separate scdulis, qaulitis and other components, but you cannot log in successfully through DSS"),(0,i.kt)("li",{parentName:"ul"},"Hostname and IP. If IP access is used, make sure it is IP when executing appconn-install.sh installation Otherwise, when accessing other components, you will be prompted that you do not have login or permission")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"ec4989a817646f785c59f6802d0fab2.jpg",src:t(35726).Z})),(0,i.kt)("h3",{id:"schedulis-deployment-considerations"},"Schedulis deployment considerations"),(0,i.kt)("p",null," ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/WeBankFinTech/Schedulis/blob/master/docs/schedulis_deploy_cn.md"},"Official deployment document")),(0,i.kt)("h4",{id:"conf-folder-1"},"conf folder"),(0,i.kt)("p",null,"azkaban.properties"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-properties"},"# azkaban.jobtype.plugin.dir and executor.global.properties. It's better to change the absolute path here\n# Azkaban JobTypes Plugins\nazkaban.jobtype.plugin.dir=/home/hadoop/application/schedulis/apps/schedulis_0.7.0_exec/plugins/jobtypes\n\n# Loader for projects\nexecutor.global.properties=/home/hadoop/application/schedulis/apps/schedulis_0.7.0_exec/conf/global.properties\n\n# Engine version\nwds.linkis.spark.engine.version=3.0.1\nwds.linkis.hive.engine.version=3.1.2\nwds.linkis.flink.engine.version=1.13.2\n")),(0,i.kt)("h4",{id:"web-modular"},"web modular"),(0,i.kt)("p",null,"plugins/viewer/system/conf:  Here, you need to configure the database connection address to be consistent with scheduleis\nazkaban.properties:  Configuration of user parameters and system management"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-properties"},"viewer.plugins=system\nviewer.plugin.dir=/home/hadoop/application/schedulis/apps/schedulis_0.7.0_web/plugins/viewer\n")),(0,i.kt)("h4",{id:"frequently-asked-questions-and-precautions-2"},"Frequently asked questions and precautions"),(0,i.kt)("p",null,"If there are resources or there are no static files such as CSS in the web interface, change the relevant path to an absolute path\nIf the configuration file cannot be loaded, you can also change the path to an absolute path\nFor example:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"### web module\nweb.resource.dir=/home/hadoop/application/schedulis/apps/schedulis_0.7.0_web/web/\nviewer.plugin.dir=/home/hadoop/application/schedulis/apps/schedulis_0.7.0_web/plugins/viewer\n\n### exec module\nazkaban.jobtype.plugin.dir=/home/hadoop/application/schedulis/apps/schedulis_0.7.0_exec/plugins/jobtypes\nexecutor.global.properties=/home/hadoop/application/schedulis/apps/schedulis_0.7.0_exec/conf/global.properties\n")),(0,i.kt)("h3",{id:"considerations-for-qualitis-deployment"},"Considerations for qualitis deployment"),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://github.com/WeBankFinTech/Qualitis/blob/master/docs/zh_CN/ch1/%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%E6%89%8B%E5%86%8C%E2%80%94%E2%80%94%E5%8D%95%E6%9C%BA%E7%89%88.md"},"Official deployment document")),(0,i.kt)("h4",{id:"conf-folder-2"},"conf folder"),(0,i.kt)("p",null,"application-dev.yml"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-properties"},"  # The correct spark version is configured here\n  spark:\n    application:\n      name: IDE\n      reparation: 50\n    engine:\n      name: spark\n      version: 3.0.1\n")),(0,i.kt)("h3",{id:"exchange-deployment-considerations"},"Exchange deployment considerations"),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://github.com/WeBankFinTech/Exchangis/blob/dev-1.0.0/docs/zh_CN/ch1/exchangis_deploy_cn.md"},"Official deployment document")),(0,i.kt)("h4",{id:"frequently-asked-questions-and-precautions-3"},"Frequently asked questions and precautions"),(0,i.kt)("p",null,"If you click the data source and there is an error that has not been published, you can try to add linkis",(0,i.kt)("em",{parentName:"p"},"ps_dm_datasource")," -> published_version_id Modify the published_version_id value to 1 (if it is null)"),(0,i.kt)("h3",{id:"visualis-1"},"Visualis"),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://github.com/WeBankFinTech/Visualis/blob/master/visualis_docs/zh_CN/Visualis_deploy_doc_cn.md"},"Official deployment document")),(0,i.kt)("h4",{id:"frequently-asked-questions-and-precautions-4"},"Frequently asked questions and precautions"),(0,i.kt)("p",null,"If the preview view is inconsistent, please check whether the bin / phantomjs file is uploaded completely\nIf you can see the following results, the upload is complete"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-properties"},"./phantomjs -v\n2.1.1\n")),(0,i.kt)("h3",{id:"streamis-1"},"Streamis"),(0,i.kt)("p",null," ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/WeBankFinTech/Streamis/blob/main/docs/zh_CN/0.2.0/Streamis%E5%AE%89%E8%A3%85%E6%96%87%E6%A1%A3.md"},"Official deployment document")),(0,i.kt)("h3",{id:"dss-appconn"},"dss-appconn"),(0,i.kt)("p",null,"Qualitis, exchangis, streams and visualis are compiled from various modules, copied to DSS appconns under DSS, and then executed appconn-install.sh under bin to install their components\nIf you find the following SQL script errors during integration, please check whether there are comments around the wrong SQL. If so, delete the comments and try appconn install again\n",(0,i.kt)("img",{alt:"903ceec2f69fc1c7a2be5f309f69726.png",src:t(88173).Z}),"\nFor example, for qualitis, the following IP and host ports are determined according to their specific use"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"qualitis\n172.21.129.78\n8090\n")),(0,i.kt)("h2",{id:"nginx-deployment-example"},"Nginx deployment example"),(0,i.kt)("p",null," linkis.conf:   dss/linkis/visualis front end\nexchangis.conf:  exchangis front end\nstreamis.conf:      streamis front end\nScheduling and Qaulitis are in their own projects\nLinkis / Visualis needs to change the dist or build packaged from the front end to the name of the corresponding component here\n",(0,i.kt)("img",{alt:"image.png",src:t(68910).Z}),"\n",(0,i.kt)("img",{alt:"image.png",src:t(1340).Z}),"\n",(0,i.kt)("img",{alt:"image.png",src:t(66051).Z})),(0,i.kt)("h4",{id:"linkisconf"},"linkis.conf"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"server {\nlisten       8089;# Access port:\nserver_name  localhost;\n#charset koi8-r;\n#access_log  /var/log/nginx/host.access.log  main;\n\nlocation /dss/visualis {\n# Modify to your own front-end path\nroot   /home/hadoop/application/webs; # Static file directory\nautoindex on;\n}\n\nlocation /dss/linkis {\n# Modify to your own front-end path\nroot   /home/hadoop/application/webs; # linkis Static file directory of management console\nautoindex on;\n}\n\nlocation / {\n# Modify to your own front-end path\nroot   /home/hadoop/application/webs/dist; # Static file directory\n#root /home/hadoop/dss/web/dss/linkis;\nindex  index.html index.html;\n}\n\nlocation /ws {\nproxy_pass http://172.21.129.210:9001;#Address of back-end linkis\nproxy_http_version 1.1;\nproxy_set_header Upgrade $http_upgrade;\nproxy_set_header Connection upgrade;\n}\n\nlocation /api {\nproxy_pass http://172.21.129.210:9001; #Address of back-end linkis\nproxy_set_header Host $host;\nproxy_set_header X-Real-IP $remote_addr;\nproxy_set_header x_real_ipP $remote_addr;\nproxy_set_header remote_addr $remote_addr;\nproxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\nproxy_http_version 1.1;\nproxy_connect_timeout 4s;\nproxy_read_timeout 600s;\nproxy_send_timeout 12s;\nproxy_set_header Upgrade $http_upgrade;\nproxy_set_header Connection upgrade;\n}\n\n#error_page  404              /404.html;\n# redirect server error pages to the static page /50x.html\n#\nerror_page   500 502 503 504  /50x.html;\nlocation = /50x.html {\nroot   /usr/share/nginx/html;\n}\n}\n\n")),(0,i.kt)("h4",{id:"exchangisconf"},"exchangis.conf"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"server {\n            listen       9800; # Access port: if the port is occupied, it needs to be modified\n            server_name  localhost;\n            #charset koi8-r;\n            #access_log  /var/log/nginx/host.access.log  main;\n            location / {\n            # Modify to own path\n            root   /home/hadoop/application/webs/exchangis/dist/dist; #Modify to your own path\n            autoindex on;\n            }\n\n            location /api {\n            proxy_pass http://172.21.129.210:9001;  # The address of the backend link needs to be modified\n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n            proxy_set_header x_real_ipP $remote_addr;\n            proxy_set_header remote_addr $remote_addr;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_http_version 1.1;\n            proxy_connect_timeout 4s;\n            proxy_read_timeout 600s;\n            proxy_send_timeout 12s;\n            proxy_set_header Upgrade $http_upgrade;\n            proxy_set_header Connection upgrade;\n            }\n\n            #error_page  404              /404.html;\n            # redirect server error pages to the static page /50x.html\n            #\n            error_page   500 502 503 504  /50x.html;\n            location = /50x.html {\n            root   /usr/share/nginx/html;\n            }\n        }\n\n")),(0,i.kt)("h4",{id:"streamisconf"},"streamis.conf"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"server {\n    listen       9088;# Access port: if the port is occupied, it needs to be modified\n    server_name  localhost;\n    location / {\n    # Modify to your own path\n        root   /home/hadoop/application/webs/streamis/dist/dist;  #Modify to your own path\n        index  index.html index.html;\n    }\n    location /api {\n    proxy_pass http://172.21.129.210:9001;        # The address of the backend link needs to be modified\n    proxy_set_header Host $host;\n    proxy_set_header X-Real-IP $remote_addr;\n    proxy_set_header x_real_ipP $remote_addr;\n    proxy_set_header remote_addr $remote_addr;\n    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n    proxy_http_version 1.1;\n    proxy_connect_timeout 4s;\n    proxy_read_timeout 600s;\n    proxy_send_timeout 12s;\n    proxy_set_header Upgrade $http_upgrade;\n    proxy_set_header Connection upgrade;\n    }\n\n    #error_page  404              /404.html;\n    # redirect server error pages to the static page /50x.html\n    #\n    error_page   500 502 503 504  /50x.html;\n    location = /50x.html {\n    root   /usr/share/nginx/html;\n    }\n}\n")))}c.isMDXComponent=!0},94846:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/files/flink-engine-2a1a0199ef097db16d2d0238005245e3.sql"},13962:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/deploy1-74f8a7facb0f4a9de3d43c233bcbb29c.png"},66051:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/deploy10-4c6b8e7214798a09d2d7245cdbb25b4a.png"},91569:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/deploy2-cc5f46ae416646c300738e32a63754fe.png"},68324:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/deploy3-a07614a61a4284be9bb0ad32c1ac6045.png"},51165:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/deploy4-fe0bda10b58063c12e25064aabcc9b8d.png"},10397:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/deploy5-9a10e4fd895a0dd493b263f827fe9f92.jpg"},35726:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/deploy6-93dece9bd52296bfd4a8269e387f6be0.jpg"},88173:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/deploy7-3d8d21245d502290cf376d396905122c.png"},68910:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/deploy8-3db0b22721e52b2f2b8b389637b8ead7.png"},1340:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/deploy9-363fc68aa5231009c4a649fa457e8e3a.png"},59180:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/dss1-20ef6540150ed9310355835fb507fbee.png"},30052:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/exchangis1-4921fd02462370bd7c40cb7f9ceb7fb2.png"},84155:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/exchangis2-e5f54b200065ab3ceec007472851d54a.png"},34373:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/flink1-087dcf93f3692dc19b9eff3ba204823d.png"},87058:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/flink2-fa8164c77b9feccfd6381b816995fcdd.png"},88667:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/flink3-7e851b84808cf7d6116653a6978ca63d.png"},96500:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/linkis1-8a23252e4187925ce9aa3baf384fc782.png"},71897:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/qualitis1-8e3dba5b4b155e0c96ad3b1680ca7914.png"},62690:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/qualitis2-15148e52070389ee6491f08119d6445b.png"},30058:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/schedulis1-3dd098ac55f1fff03d5466bf1acf503b.png"},10737:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/streamis1-067a0add4da497c30b77a6c0aba3128a.png"},76945:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/sx1-808e41fd5ce8b7cdf2a72953efcfb52a.png"},9524:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/sx2-72292e18cadcfa92b920613ea29abb73.png"},66959:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/sx3-967b6ac9e375892ec45cfc8fe8e1a46c.png"},33765:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/visualis1-526c531225821a7aa99946d3a194ec18.png"}}]);