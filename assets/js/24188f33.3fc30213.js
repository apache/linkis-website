"use strict";(self.webpackChunklinkis_web_apache=self.webpackChunklinkis_web_apache||[]).push([[52791],{3905:(e,t,n)=>{n.d(t,{Zo:()=>h,kt:()=>g});var i=n(67294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);t&&(i=i.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,i)}return n}function s(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function a(e,t){if(null==e)return{};var n,i,r=function(e,t){if(null==e)return{};var n,i,r={},o=Object.keys(e);for(i=0;i<o.length;i++)n=o[i],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(i=0;i<o.length;i++)n=o[i],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var l=i.createContext({}),c=function(e){var t=i.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):s(s({},t),e)),n},h=function(e){var t=c(e.components);return i.createElement(l.Provider,{value:t},e.children)},d="mdxType",p={inlineCode:"code",wrapper:function(e){var t=e.children;return i.createElement(i.Fragment,{},t)}},u=i.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,l=e.parentName,h=a(e,["components","mdxType","originalType","parentName"]),d=c(n),u=r,g=d["".concat(l,".").concat(u)]||d[u]||p[u]||o;return n?i.createElement(g,s(s({ref:t},h),{},{components:n})):i.createElement(g,s({ref:t},h))}));function g(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,s=new Array(o);s[0]=u;var a={};for(var l in t)hasOwnProperty.call(t,l)&&(a[l]=t[l]);a.originalType=e,a[d]="string"==typeof e?e:r,s[1]=a;for(var c=2;c<o;c++)s[c]=n[c];return i.createElement.apply(null,s)}return i.createElement.apply(null,n)}u.displayName="MDXCreateElement"},52854:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>s,default:()=>p,frontMatter:()=>o,metadata:()=>a,toc:()=>c});var i=n(87462),r=(n(67294),n(3905));const o={},s="FAQ",a={unversionedId:"main",id:"main",title:"FAQ",description:"Linkis1.0 common problems and solutions//docs.qq.com/doc/DWlN4emlJeEJxWlR0",source:"@site/faq/main.md",sourceDirName:".",slug:"/main",permalink:"/faq/main",draft:!1,editUrl:"https://github.com/apache/linkis-website/edit/dev/faq/main.md",tags:[],version:"current",frontMatter:{},sidebar:"defaultSidebar"},l={},c=[{value:"Q1: The ps-cs service log of linkis reports this error: figServletWebServerApplicationContext (559)",id:"q1-the-ps-cs-service-log-of-linkis-reports-this-error-figservletwebserverapplicationcontext-559",level:2},{value:"Q2: Linkis-eureka debugging instructions",id:"q2-linkis-eureka-debugging-instructions",level:2},{value:"Q3: eureka automatically stops when it starts for the first time, and needs to be restarted manually",id:"q3-eureka-automatically-stops-when-it-starts-for-the-first-time-and-needs-to-be-restarted-manually",level:2},{value:"Q4: Linkis Entrance LogWriter missing dependencies",id:"q4-linkis-entrance-logwriter-missing-dependencies",level:2},{value:"Q5: When Linkis1.0 executes tasks, the ECP service throws the following error: Caused by: java.util.NoSuchElementException: None.get?",id:"q5-when-linkis10-executes-tasks-the-ecp-service-throws-the-following-error-caused-by-javautilnosuchelementexception-noneget",level:2},{value:"Q6: Linkis1.X general troubleshooting method for insufficient resources",id:"q6-linkis1x-general-troubleshooting-method-for-insufficient-resources",level:2},{value:"Q7: linkis startup error: NoSuchMethodErrorgetSessionManager()Lorg/eclipse/jetty/server/SessionManager",id:"q7-linkis-startup-error-nosuchmethoderrorgetsessionmanagerlorgeclipsejettyserversessionmanager",level:2},{value:"Q1: Linkis1.0 execution task report: select list is not in group by clause",id:"q1-linkis10-execution-task-report-select-list-is-not-in-group-by-clause",level:2},{value:"Q2: When executing scripts after deployment, executing commands, and collecting results, I encountered such an error, IOException: File header type must be dolphin:",id:"q2-when-executing-scripts-after-deployment-executing-commands-and-collecting-results-i-encountered-such-an-error-ioexception-file-header-type-must-be-dolphin",level:2},{value:"Q3: The json4s package conflict caused by inconsistent Spark versions, the error is as follows: Error message: caused by: java.lang.NoSuchMethodError: org.json4s.jackson.jsonMethod$",id:"q3-the-json4s-package-conflict-caused-by-inconsistent-spark-versions-the-error-is-as-follows-error-message-caused-by-javalangnosuchmethoderror-orgjson4sjacksonjsonmethod",level:2},{value:"Q4: When Linkis1.X submits spark sql tasks in version CDH5.16.1, how to troubleshoot 404 problems",id:"q4-when-linkis1x-submits-spark-sql-tasks-in-version-cdh5161-how-to-troubleshoot-404-problems",level:2},{value:"Q5: running error report missing package matplotlib",id:"q5-running-error-report-missing-package-matplotlib",level:2},{value:"Q6: When the microservice linkis-ps-cs is started, DebuggClassWriter overrides final method visit is reported",id:"q6-when-the-microservice-linkis-ps-cs-is-started-debuggclasswriter-overrides-final-method-visit-is-reported",level:2},{value:"Q7: When the shell engine schedules execution, the engine execution directory reports the following error /bin/java: No such file or directory:",id:"q7-when-the-shell-engine-schedules-execution-the-engine-execution-directory-reports-the-following-error-binjava-no-such-file-or-directory",level:2},{value:"Q8: When the hive engine is scheduled, the error log of engineConnManager is as follows: method did not exist: SessionHandler:",id:"q8-when-the-hive-engine-is-scheduled-the-error-log-of-engineconnmanager-is-as-follows-method-did-not-exist-sessionhandler",level:2},{value:"Q9: When the hive engine is executing, the following error is reported Lcom/google/common/collect/UnmodifiableIterator:",id:"q9-when-the-hive-engine-is-executing-the-following-error-is-reported-lcomgooglecommoncollectunmodifiableiterator",level:2},{value:"Q10: During engine scheduling, the following error is reported: Python processes is not alive:",id:"q10-during-engine-scheduling-the-following-error-is-reported-python-processes-is-not-alive",level:2},{value:"Q11: When the spark engine is running, the following error is reported: NoClassDefFoundError: org/apache/hadoop/hive/ql/io/orc/OrcFile:",id:"q11-when-the-spark-engine-is-running-the-following-error-is-reported-noclassdeffounderror-orgapachehadoophiveqlioorcorcfile",level:2},{value:"Q1: The database on the left side of the script cannot be refreshed",id:"q1-the-database-on-the-left-side-of-the-script-cannot-be-refreshed",level:2},{value:"Q2: The right side of Scriptis cannot refresh the database, and it has been refreshing (it should be noted that the metadata of linkis does not support docking sentry and Ranger for the time being, only supports native permission control of hive), error message: the front-end database tab has been refreshing state",id:"q2-the-right-side-of-scriptis-cannot-refresh-the-database-and-it-has-been-refreshing-it-should-be-noted-that-the-metadata-of-linkis-does-not-support-docking-sentry-and-ranger-for-the-time-being-only-supports-native-permission-control-of-hive-error-message-the-front-end-database-tab-has-been-refreshing-state",level:2},{value:"Q3: [Scriptis]Workspace When I log in to Scriptis, the root directory does not exist, and there are two root directories: workspace and HDFS: Error message: After logging in, the front end pops up the following message (the user\u2019s local directory does not exist, please Contact admin to add)",id:"q3-scriptisworkspace-when-i-log-in-to-scriptis-the-root-directory-does-not-exist-and-there-are-two-root-directories-workspace-and-hdfs-error-message-after-logging-in-the-front-end-pops-up-the-following-message-the-users-local-directory-does-not-exist-please-contact-admin-to-add",level:2},{value:"Q4: [Management Console]Settings How to adjust the yarn queue used by the task? Error message: When executing the sql task, it is reported that 1. Obtaining the Yarn queue information is abnormal or user XX cannot submit to the queue",id:"q4-management-consolesettings-how-to-adjust-the-yarn-queue-used-by-the-task-error-message-when-executing-the-sql-task-it-is-reported-that-1-obtaining-the-yarn-queue-information-is-abnormal-or-user-xx-cannot-submit-to-the-queue",level:2},{value:"Q5: When Hive queries, it reports: Can not find zk-related classes such as: org.apache.curator.<em>, error message: When executing hive tasks, the log report cannot find the class starting with org.apache.curator.</em> , classNotFound",id:"q5-when-hive-queries-it-reports-can-not-find-zk-related-classes-such-as-orgapachecurator-error-message-when-executing-hive-tasks-the-log-report-cannot-find-the-class-starting-with-orgapachecurator--classnotfound",level:2},{value:"Q6: How does Linkis support kerberos",id:"q6-how-does-linkis-support-kerberos",level:2},{value:"Q7: Regarding Linkis, besides supporting deployment user login, can other user logins be configured?",id:"q7-regarding-linkis-besides-supporting-deployment-user-login-can-other-user-logins-be-configured",level:2},{value:"Q8: How to open Linkis management console, administrator page ECM and microservice management?",id:"q8-how-to-open-linkis-management-console-administrator-page-ecm-and-microservice-management",level:2},{value:"Q9: When starting the microservice linkis-ps-publicservice, kJdbcUtils.getDriverClassName NPE",id:"q9-when-starting-the-microservice-linkis-ps-publicservice-kjdbcutilsgetdriverclassname-npe",level:2},{value:"Q10: When scheduling the hive engine, the following error is reported: EngineConnPluginNotFoundException: errorCode: 70063",id:"q10-when-scheduling-the-hive-engine-the-following-error-is-reported-engineconnpluginnotfoundexception-errorcode-70063",level:2},{value:"Q11: When the hive engine schedules execution, the following error is reported: opertion failed NullPointerException:",id:"q11-when-the-hive-engine-schedules-execution-the-following-error-is-reported-opertion-failed-nullpointerexception",level:2},{value:"Q12: When the spark engine starts, an error is reported get the queue information excepiton. (obtaining Yarn queue information exception) and http link exception",id:"q12-when-the-spark-engine-starts-an-error-is-reported-get-the-queue-information-excepiton-obtaining-yarn-queue-information-exception-and-http-link-exception",level:2},{value:"Q13: pythonspark scheduling execution, error: initialize python executor failed ClassNotFoundException org.slf4j.impl.StaticLoggerBinder",id:"q13-pythonspark-scheduling-execution-error-initialize-python-executor-failed-classnotfoundexception-orgslf4jimplstaticloggerbinder",level:2},{value:"Q14: Common package conflicts:",id:"q14-common-package-conflicts",level:2},{value:"Q15: The Mysql script running Scripts reports an errorsql engine reports an error",id:"q15-the-mysql-script-running-scripts-reports-an-errorsql-engine-reports-an-error",level:2},{value:"Q16: The waiting time for scriptis to execute the script is long",id:"q16-the-waiting-time-for-scriptis-to-execute-the-script-is-long",level:2},{value:"Q17: scriptis executes jdbc script and reports an error",id:"q17-scriptis-executes-jdbc-script-and-reports-an-error",level:2},{value:"Q18: Turn off resource checking",id:"q18-turn-off-resource-checking",level:2},{value:"Q19: Executing the script reports an error",id:"q19-executing-the-script-reports-an-error",level:2},{value:"Q20: ScriptIs execute script TimeoutException",id:"q20-scriptis-execute-script-timeoutexception",level:2},{value:"Q21: Engine timeout setting",id:"q21-engine-timeout-setting",level:2},{value:"Q22: When creating a new workflow, it prompts &quot;504 Gateway Time-out&quot;",id:"q22-when-creating-a-new-workflow-it-prompts-504-gateway-time-out",level:2},{value:"Q23: Scripts execute python scripts (the content of the script is very simple print) and execute successfully normally. It can also be executed successfully through the task scheduling system. It can also be executed successfully through the job flow editing job script page, but an error is reported through job flow execution.",id:"q23-scripts-execute-python-scripts-the-content-of-the-script-is-very-simple-print-and-execute-successfully-normally-it-can-also-be-executed-successfully-through-the-task-scheduling-system-it-can-also-be-executed-successfully-through-the-job-flow-editing-job-script-page-but-an-error-is-reported-through-job-flow-execution",level:2},{value:"Q24: After installing Exchangis0.5.0, click on the dss menu to enter a new page and prompt &quot;Sorry, Page Not Found&quot;. F12 view has 404 exception",id:"q24-after-installing-exchangis050-click-on-the-dss-menu-to-enter-a-new-page-and-prompt-sorry-page-not-found-f12-view-has-404-exception",level:2},{value:"Q25: There is an infinite loop in configuring atlas in HIVE, resulting in stack overflow",id:"q25-there-is-an-infinite-loop-in-configuring-atlas-in-hive-resulting-in-stack-overflow",level:2},{value:"Q26: Linkis1.0.X compiles based on spark3 hadoop3 hive3 or hdp3.1.4, please refer to:",id:"q26-linkis10x-compiles-based-on-spark3-hadoop3-hive3-or-hdp314-please-refer-to",level:2},{value:"Q27 Linkis cannot get user name when executing jdbc task",id:"q27-linkis-cannot-get-user-name-when-executing-jdbc-task",level:2},{value:"Q28: After changing the hive version in the configuration before installation, the configuration of the management console still shows the version as 2.3.3",id:"q28-after-changing-the-hive-version-in-the-configuration-before-installation-the-configuration-of-the-management-console-still-shows-the-version-as-233",level:2},{value:"Q29: When linkis-cli submits a task, it prompts GROUP BY clause; sql_mode=only_full_group_by error",id:"q29-when-linkis-cli-submits-a-task-it-prompts-group-by-clause-sql_modeonly_full_group_by-error",level:2},{value:"Q30: An error is reported when the flink engine starts and the TokenCache is found",id:"q30-an-error-is-reported-when-the-flink-engine-starts-and-the-tokencache-is-found",level:2},{value:"Q31: When starting the flink engine/spark engine, the engine-entrance reports an error org.json4s.JsonAST$JNothing$ cannot be cast to org.json4s.JsonAST$JString",id:"q31-when-starting-the-flink-enginespark-engine-the-engine-entrance-reports-an-error-orgjson4sjsonastjnothing-cannot-be-cast-to-orgjson4sjsonastjstring",level:2},{value:"Q32: ClassNotFoundException is reported when the function script is executed",id:"q32-classnotfoundexception-is-reported-when-the-function-script-is-executed",level:2},{value:"Q33: Failed to start bean &#39;webServerStartStop when Linkis executes Spark task in CDH environment",id:"q33-failed-to-start-bean-webserverstartstop-when-linkis-executes-spark-task-in-cdh-environment",level:2},{value:"Q34: An error is reported when running the flink task: Failed to create engineConnPlugin: com.webank.wedatasphere.linkis.engineplugin.hive.HiveEngineConnPluginjava.lang.ClassNotFoundException: com.webank.wedatasphere.linkis.engineplugin.hive.HiveEngineConnPlugin",id:"q34-an-error-is-reported-when-running-the-flink-task-failed-to-create-engineconnplugin-comwebankwedataspherelinkisenginepluginhivehiveengineconnpluginjavalangclassnotfoundexception-comwebankwedataspherelinkisenginepluginhivehiveengineconnplugin",level:2}],h={toc:c},d="wrapper";function p(e){let{components:t,...o}=e;return(0,r.kt)(d,(0,i.Z)({},h,o,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"faq"},"FAQ"),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"Linkis1.0 common problems and solutions: ",(0,r.kt)("a",{parentName:"p",href:"https://docs.qq.com/doc/DWlN4emlJeEJxWlR0"},"https://docs.qq.com/doc/DWlN4emlJeEJxWlR0"))),(0,r.kt)("h1",{id:"1-use-problems"},"1. Use problems"),(0,r.kt)("h2",{id:"q1-the-ps-cs-service-log-of-linkis-reports-this-error-figservletwebserverapplicationcontext-559"},"Q1: The ps-cs service log of linkis reports this error: figServletWebServerApplicationContext (559)"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"[refresh] - Exception encountered during context initi\nalization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'eurekaInstanceLabelClient': Invocation of initkaba method failed; nested exception is java.lang.RuntimeException: com.netflix.client.ClientException: Load balancer does not have available server for client: linkis-ps-publicservice\n")),(0,r.kt)("p",null,"A: This is because the publicservice service did not start successfully. It is recommended to manually restart the publicservice sh/sbin/linkis-dameo.sh restart ps-publicservice"),(0,r.kt)("h2",{id:"q2-linkis-eureka-debugging-instructions"},"Q2: Linkis-eureka debugging instructions"),(0,r.kt)("p",null,"A: If you need to debug the eureka program, you need to do some configuration first, as shown below\napplication-eureka.yml needs to remove part of the comment configuration, and the normal startup configuration is as follows:\n",(0,r.kt)("img",{alt:"1639466558031",src:n(51338).Z,width:"2047",height:"914"})),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"1639466558031",src:n(99262).Z,width:"1028",height:"1079"})),(0,r.kt)("h2",{id:"q3-eureka-automatically-stops-when-it-starts-for-the-first-time-and-needs-to-be-restarted-manually"},"Q3: eureka automatically stops when it starts for the first time, and needs to be restarted manually"),(0,r.kt)("p",null,"A: This is because eureka does not use nohup when starting the Java process. After the session exits, the task is automatically cleaned up by the operating system. You need to modify the eureka startup script and add nohup:"),(0,r.kt)("p",null,(0,r.kt)("img",{src:n(77886).Z,width:"1042",height:"83"})),(0,r.kt)("p",null,"You can refer to PR: ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/apache/linkis/pull/837/files"},"https://github.com/apache/linkis/pull/837/files")),(0,r.kt)("h2",{id:"q4-linkis-entrance-logwriter-missing-dependencies"},"Q4: Linkis Entrance LogWriter missing dependencies"),(0,r.kt)("p",null,"A: Hadoop 3 needs to modify the linkis-hadoop-common pom file, see: ",(0,r.kt)("a",{parentName:"p",href:"https://linkis.apache.org/zh-CN/docs/next/development/linkis-compile-and-package/"},"https://linkis.apache.org/zh-CN/docs/next/development/linkis-compile-and-package/")),(0,r.kt)("h2",{id:"q5-when-linkis10-executes-tasks-the-ecp-service-throws-the-following-error-caused-by-javautilnosuchelementexception-noneget"},"Q5: When Linkis1.0 executes tasks, the ECP service throws the following error: Caused by: java.util.NoSuchElementException: None.get?"),(0,r.kt)("p",null,"Error detailed log:"),(0,r.kt)("p",null,"Solution:\nAt this time, because the corresponding engine version material does not have a corresponding record in the database table, it may be caused by an error when the ecp service is started. You can restart the ecp service to see if there is an error when uploading the BML. The corresponding table is: linkis_cg_engine_conn_plugin_bml_resources"),(0,r.kt)("h2",{id:"q6-linkis1x-general-troubleshooting-method-for-insufficient-resources"},"Q6: Linkis1.X general troubleshooting method for insufficient resources"),(0,r.kt)("p",null,"Insufficient resources fall into two situations:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Insufficient resources of the server itself"),(0,r.kt)("li",{parentName:"ol"},"The user's own resources are insufficient (linkis will control user resources).\nThese two resources are recorded in linkis_cg_manager_label_resource and linkis_cg_manager_linkis_resource in linkis1.X, the former is the association table of label and resource, and the latter is the resource table\nNormally, linkis1.0 is safe for high concurrency control of resources, and it is not recommended to forcibly reset user resource records by modifying table records. However, due to the difference in the execution environment of linkis during the installation and debugging process, engine startup failures may occur, or repeated restarts of microservices during the engine startup process lead to unsafe release of resources, or the monitor does not have time to automatically clean up (some hours) level delay), there may be a problem of insufficient resources, and in severe cases, most of the user's resources will be locked. Therefore, you can refer to the following steps to troubleshoot insufficient resources:\na. Confirm on the management console whether the remaining resources of the ECM are greater than the requested resources of the engine. If the remaining resources of the ECM are very small, it will cause the request for a new engine to fail. You need to manually turn off some idle engines in the ECM. There is also a mechanism for automatic release when idle, but this time is set relatively long by default.\nb. If the ECM resources are sufficient, it must be that the remaining resources of the user are not enough to request a new engine. First, determine the label generated when the user executes the task. For example, if the user hadoop executes the spark2.4.3 script on Scriptis, it will be corresponding in the linkis_cg_manager_label table next record\nWe get the id value of this label, find the corresponding resourceId in the association table linkis_cg_manager_label_resource, and find the corresponding resource record of the label in linkis_cg_manager_linkis_resource through resourceId, you can check the remaining resources in this record")),(0,r.kt)("p",null,"If this resource is checked and determined to be abnormal, that is, it does not match the resources generated by the actual engine startup. The following operations can be performed to recover:\nAfter confirming that all engines under the label have been shut down, you can directly delete this resource and the associated record corresponding to the association table linkis_cg_manager_label_resource, and this resource will be automatically reset when you request again.\nNote: All engines of this label have been shut down. In the previous example, it means that the spark2.4.3 engines started by the hadoop user on Scriptis have all been shut down. You can see all the engines started by the user in the resource management of the management console. instance. Otherwise, the resource record exception of the label may also occur."),(0,r.kt)("h2",{id:"q7-linkis-startup-error-nosuchmethoderrorgetsessionmanagerlorgeclipsejettyserversessionmanager"},"Q7: linkis startup error: NoSuchMethodErrorgetSessionManager()Lorg/eclipse/jetty/server/SessionManager"),(0,r.kt)("p",null,"Specific stack:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"startup of context o.s.b.w.e.j.JettyEmbeddedWebAppContext@6c6919ff{application,/,[file:///tmp/jetty-docbase.9102.6375358926927953589/],UNAVAILABLE} java.lang.NoSuchMethodError: org.eclipse.jetty.server.session.SessionHandler.getSessionManager()Lorg/eclipse/jetty/server/SessionManager;\nat org.eclipse.jetty.servlet.ServletContextHandler\\$Context.getSessionCookieConfig(ServletContextHandler.java:1415) ~[jetty-servlet-9.3.20.v20170531.jar:9.3.20.v20170531]\n")),(0,r.kt)("p",null,"Solution: jetty-servlet and jetty-security versions need to be upgraded from 9.3.20 to 9.4.20;"),(0,r.kt)("h1",{id:"2-environmental-issues"},"2. Environmental issues"),(0,r.kt)("h2",{id:"q1-linkis10-execution-task-report-select-list-is-not-in-group-by-clause"},"Q1: Linkis1.0 execution task report: select list is not in group by clause"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"1639466558031",src:n(14897).Z,width:"2560",height:"1139"})),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"1639466558031",src:n(43957).Z,width:"1087",height:"444"})),(0,r.kt)("p",null,"This problem is caused by the default value of the global setting mode in mysql 5.8 version, you need to execute the following line in the mysql cli:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"SET GLOBAL sql_mode=(SELECT REPLACE(@@sql_mode,'ONLY_FULL_GROUP_BY',''));\n")),(0,r.kt)("h2",{id:"q2-when-executing-scripts-after-deployment-executing-commands-and-collecting-results-i-encountered-such-an-error-ioexception-file-header-type-must-be-dolphin"},"Q2: When executing scripts after deployment, executing commands, and collecting results, I encountered such an error, IOException: File header type must be dolphin:"),(0,r.kt)("p",null,"A: This should be caused by repeated installations, resulting in the result set being written in the same file. The previous Linkis 0.X version used the result set to write append, and 1.0 has been modified to add a new one. You can clean up the result set Directory: The configuration parameter is wds.linkis.resultSet.store.path, you can clean up this directory"),(0,r.kt)("h2",{id:"q3-the-json4s-package-conflict-caused-by-inconsistent-spark-versions-the-error-is-as-follows-error-message-caused-by-javalangnosuchmethoderror-orgjson4sjacksonjsonmethod"},"Q3: The json4s package conflict caused by inconsistent Spark versions, the error is as follows: Error message: caused by: java.lang.NoSuchMethodError: org.json4s.jackson.jsonMethod$"),(0,r.kt)("p",null,"solution:\nThis is because of Spark jars' json4s and lib/linkis-engineconn-plugins/spark/dist/version/lib\nThe json4s version in the package is inconsistent. When the official release is released, the supported version of Spark will be indicated later. If it is inconsistent, this problem will exist.\nThe solution is to replace the json4s package in Spark jars with lib/linkis-engineconn-plugins/spark/dist/version/lib\nThe json4s version inside the package. In addition, there may be conflicts in the netty package, which can be handled according to the method of Json4s. Then restart the ecp service: sh sbin/linkis-damon.sh restart cg-engineplugin"),(0,r.kt)("h2",{id:"q4-when-linkis1x-submits-spark-sql-tasks-in-version-cdh5161-how-to-troubleshoot-404-problems"},"Q4: When Linkis1.X submits spark sql tasks in version CDH5.16.1, how to troubleshoot 404 problems"),(0,r.kt)("p",null,"The main error message is as follows:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'21304, Task is Failed,errorMsg: errCode: 12003 ,desc: ip:port_x Failed to async get EngineNode FeignException.NotFound: status 404 reading RPCReceiveRemote#receiveAndReply(Message) ,ip: xxxxx ,port: 9104 ,serviceKind: linkis-cg-entrance\norg.apache.jasper.servlet.JspServlet 89 warn - PWC6117: File "/home/hadoop/dss1.0/tmp/hadoop/workDir/7c3b796f-aadd-46a5-b515-0779e523561a/tmp/jetty-docbase.1802511762054502345.46019/api/rest_j/v1/rpc/receiveAndReply" not found\n')),(0,r.kt)("p",null,"The above error message is mainly caused by the jar conflict in the cdh environment variable. You need to find the jar package where the org.apache.jasper.servlet.JspServlet class is located. The local cdh environment variable path is: /opt/cloudera/parcels/CDH -5.16.1-1.cdh5.16.1.p0.3/jars, delete the corresponding jasper-compile-${version}.jar and jsp-${version}.jar jar packages under this directory, The spark sql task can be run again without restarting the service, and the problem is solved."),(0,r.kt)("h2",{id:"q5-running-error-report-missing-package-matplotlib"},"Q5: running error report missing package matplotlib"),(0,r.kt)("p",null,"In the standard python environment, anaconda2 and anaconda3 need to be installed, and the default anaconda is anaconda2. This contains most common python libraries."),(0,r.kt)("h2",{id:"q6-when-the-microservice-linkis-ps-cs-is-started-debuggclasswriter-overrides-final-method-visit-is-reported"},"Q6: When the microservice linkis-ps-cs is started, DebuggClassWriter overrides final method visit is reported"),(0,r.kt)("p",null,"Specific exception stack:"),(0,r.kt)("p",null,"Solution: jar package conflict, delete asm-5.0.4.jar;"),(0,r.kt)("h2",{id:"q7-when-the-shell-engine-schedules-execution-the-engine-execution-directory-reports-the-following-error-binjava-no-such-file-or-directory"},"Q7: When the shell engine schedules execution, the engine execution directory reports the following error /bin/java: No such file or directory:"),(0,r.kt)("p",null,(0,r.kt)("img",{src:n(89911).Z,width:"746",height:"173"})),(0,r.kt)("p",null,"Solution: There is a problem with the environment variables of the local java, and a symbolic link needs to be made to the java command."),(0,r.kt)("h2",{id:"q8-when-the-hive-engine-is-scheduled-the-error-log-of-engineconnmanager-is-as-follows-method-did-not-exist-sessionhandler"},"Q8: When the hive engine is scheduled, the error log of engineConnManager is as follows: method did not exist: SessionHandler:"),(0,r.kt)("p",null,(0,r.kt)("img",{src:n(49049).Z,width:"2864",height:"1626"})),(0,r.kt)("p",null,"Solution: Under the hive engine lib, the jetty jar package conflicts, replace jetty-security and jetty-server with 9.4.20;"),(0,r.kt)("h2",{id:"q9-when-the-hive-engine-is-executing-the-following-error-is-reported-lcomgooglecommoncollectunmodifiableiterator"},"Q9: When the hive engine is executing, the following error is reported Lcom/google/common/collect/UnmodifiableIterator:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"2021-03-16 13:32:23.304 ERROR [pool-2-thread-1]com.webank.wedatasphere.linkis.engineplugin.hive.executor.HiveEngineConnExecutor 140 run - query failed, reason : java.lang.AccessError: tried to access method com.google.common.collect.Iterators.emptyIterator() Lcom/google/common/collect/UnmodifiableIterator; from class org.apache.hadoop.hive.ql.exec.FetchOperator \nat org.apache.hadoop.hive.ql.exec.FetchOperator.<init>(FetchOperator.java:108) ~[hive-exec-2.1.1-cdh6.1.0.jar:2.1.1-cdh6.1.0]\nat org.apache.hadoop.hive.ql.exec.FetchTask.initialize(FetchTask.java:86) ~[hive-exec-2.1.1-cdh6.1.0.jar:2.1.1-cdh6.1.0]\nat org.apache.hadoop.hive.ql..compile(Driver.java:629) ~[hive-exec-2.1.1-cdh6.1.0.jar:2.1.1-cdh6.1.0]\nat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1414) ~[hive-exec-2.1.1-cdh6.1.0.jar:2.1.1-cdh6.1.0]\nat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1543) ~[hive-exec-2.1.1-cdh6.1.0.jar:2.1.1-cdh6.1.0]\nat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1332) ~[hive-exec-2.1.1-cdh6.1.0.jar:2.1.1-cdh6.1.0]\nat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1321) ~[hive-exec-2.1.1-cdh6.1.0.jar:2.1.1-cdh6.1.0]\natcom.webank.wedatasphere.linkis.engineplugin.hive.executor.HiveEngineConnExecutor$$anon$1.run(HiveEngineConnExecutor.scala:152) [linkis-engineplugin-hive-dev-1.0.0.jar:?]\natcom.webank.wedatasphere.linkis.engineplugin.hive.executor.HiveEngineConnExecutor$$anon$1.run(HiveEngineConnExecutor.scala:126) [linkis-engineplugin-hive-dev-1.0.0.jar:?]\n")),(0,r.kt)("p",null,"Solution: guava package conflict, delete guava-25.1-jre.jar under hive/dist/v1.2.1/lib;"),(0,r.kt)("h2",{id:"q10-during-engine-scheduling-the-following-error-is-reported-python-processes-is-not-alive"},"Q10: During engine scheduling, the following error is reported: Python processes is not alive:"),(0,r.kt)("p",null,(0,r.kt)("img",{src:n(45804).Z,width:"1761",height:"204"})),(0,r.kt)("p",null,"Solution: Install the anaconda3 package manager on the server. After debugging python, two problems were found: (1) lack of pandas and matplotlib modules, which need to be installed manually; (2) when the new version of the python engine is executed, it depends on a higher version of python, and python3 is installed first. Next, make a symbolic link (as shown below), and restart the engineplugin service."),(0,r.kt)("h2",{id:"q11-when-the-spark-engine-is-running-the-following-error-is-reported-noclassdeffounderror-orgapachehadoophiveqlioorcorcfile"},"Q11: When the spark engine is running, the following error is reported: NoClassDefFoundError: org/apache/hadoop/hive/ql/io/orc/OrcFile:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"2021-03-19 15:12:49.227 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler 57 logInfo -ShuffleMapStage 5 (show at <console>:69) failed in 21.269 s due to Job aborted due to stage failure: Task 1 in stage 5.0 failed 4 times, most recent failure: Lost task 1.3 in stage 5.0 (TID 139, cdh03, executor 6):java.lang.NoClassDefFoundError: org/apache/hadoop/hive/ql/io/orc/OrcFile \n")),(0,r.kt)("p",null,"Solution: The classpath of the cdh6.3.2 cluster spark engine only has /opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/spark/jars, you need to add hive-exec-2.1.1- cdh6.1.0.jar, and restart spark."),(0,r.kt)("h1",{id:"three-configuration-problems"},"Three, configuration problems"),(0,r.kt)("h2",{id:"q1-the-database-on-the-left-side-of-the-script-cannot-be-refreshed"},"Q1: The database on the left side of the script cannot be refreshed"),(0,r.kt)("p",null,"solution:\na. The reason may be that the linkis-metatdata service did not read the HIVE_CONF_DIR error, you can configure the parameters of linkis-metadata: corresponding to the JDBC connection string of the metadata database"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"hive.meta.url=\nhive.meta.user=\nhive.meta.password=\n")),(0,r.kt)("h2",{id:"q2-the-right-side-of-scriptis-cannot-refresh-the-database-and-it-has-been-refreshing-it-should-be-noted-that-the-metadata-of-linkis-does-not-support-docking-sentry-and-ranger-for-the-time-being-only-supports-native-permission-control-of-hive-error-message-the-front-end-database-tab-has-been-refreshing-state"},"Q2: The right side of Scriptis cannot refresh the database, and it has been refreshing (it should be noted that the metadata of linkis does not support docking sentry and Ranger for the time being, only supports native permission control of hive), error message: the front-end database tab has been refreshing state"),(0,r.kt)("p",null,"solution:\nThis is because we have restricted permissions for the database on the right, and this relies on hive to enable authorized access:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"hive.security.authorization.enabled=true; \n")),(0,r.kt)("p",null,"For details, please refer to: ",(0,r.kt)("a",{parentName:"p",href:"https://blog.csdn.net/yancychas/article/details/84202400"},"https://blog.csdn.net/yancychas/article/details/84202400"),"\nIf you have configured this parameter and have not yet enabled it, you need to grant the user the corresponding database table permission and execute the grant statement. You can refer to the same link authorization section."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"# Authorization reference Take hadoop as an example:\n\n# Enter the hive client to check the hadoop user database authorization status:\n\nshow grant user hadoop on database default;\n\n# Authorize the user database:\n\ngrant all on database default to user hadoop;\n\n")),(0,r.kt)("p",null,"If you don't want to enable permission control, that is, every user can see the library table, you can modify: com/webank/wedatasphere/linkis/metadata/hive/dao/impl/HiveMetaDao.xml sql remove the permission control part"),(0,r.kt)("h2",{id:"q3-scriptisworkspace-when-i-log-in-to-scriptis-the-root-directory-does-not-exist-and-there-are-two-root-directories-workspace-and-hdfs-error-message-after-logging-in-the-front-end-pops-up-the-following-message-the-users-local-directory-does-not-exist-please-contact-admin-to-add"},"Q3: ","[","Scriptis]","[Workspace]"," When I log in to Scriptis, the root directory does not exist, and there are two root directories: workspace and HDFS: Error message: After logging in, the front end pops up the following message (the user\u2019s local directory does not exist, please Contact admin to add)"),(0,r.kt)("p",null,"solution:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"a. Confirm the linkis.properties parameter in the conf directory of linkis-ps-publicservice: wds.linkis.workspace.filesystem.localuserrootpath=file:///tmp/linkis/ Does it start with file://?"),(0,r.kt)("li",{parentName:"ul"},"b. Confirm whether wds.linkis.workspace.filesystem.hdfsuserrootpath.prefix=hdfs:///tmp/linkis/ starts with hdfs://"),(0,r.kt)("li",{parentName:"ul"},"c. Confirm whether there is a user directory under the /tmp/linkis directory. The user here refers to the front-end login user, such as hadoop user login, then create: /tmp/linkis/hadoop directory, if the directory exists, confirm the directory permission login user It can be operated. If it still doesn\u2019t work, you can refer to the error report of publicservice. The error will explain the permission or the path problem.")),(0,r.kt)("h2",{id:"q4-management-consolesettings-how-to-adjust-the-yarn-queue-used-by-the-task-error-message-when-executing-the-sql-task-it-is-reported-that-1-obtaining-the-yarn-queue-information-is-abnormal-or-user-xx-cannot-submit-to-the-queue"},"Q4: ","[","Management Console]","[Settings]"," How to adjust the yarn queue used by the task? Error message: When executing the sql task, it is reported that 1. Obtaining the Yarn queue information is abnormal or user XX cannot submit to the queue"),(0,r.kt)("p",null,"solution:\nIn the front end - management console - settings - general settings - Yarn queue configuration login user has permission queue"),(0,r.kt)("h2",{id:"q5-when-hive-queries-it-reports-can-not-find-zk-related-classes-such-as-orgapachecurator-error-message-when-executing-hive-tasks-the-log-report-cannot-find-the-class-starting-with-orgapachecurator--classnotfound"},"Q5: When Hive queries, it reports: Can not find zk-related classes such as: org.apache.curator.",(0,r.kt)("em",{parentName:"h2"},", error message: When executing hive tasks, the log report cannot find the class starting with org.apache.curator.")," , classNotFound"),(0,r.kt)("p",null,"solution:\nThis is because the hive transaction is enabled, you can modify the hive-site.xml on the linkis machine to turn off the transaction configuration, refer to the hive transaction: ",(0,r.kt)("a",{parentName:"p",href:"https://www.jianshu.com/p/aa0f0fdd234c"},"https://www.jianshu.com/p/aa0f0fdd234c"),"\nOr put the relevant package into the engine plugin directory lib/linkis-engineconn-plugins/hive/dist/version/lib"),(0,r.kt)("h2",{id:"q6-how-does-linkis-support-kerberos"},"Q6: How does Linkis support kerberos"),(0,r.kt)("p",null,"solution:\nObtaining Hadoop's FileSystem in linkis is implemented through the HDFSUtils class, so we put kerberos in this class, and users can see the logic of this class. The login modes currently supported are as follows:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'if(KERBEROS_ENABLE.getValue) {\n      val path = new File(\n      TAB_FILE.getValue \uff0c userName + ".keytab").getPath\n      val user = getKerberosUser(userName)\n      UserGroupInformation.setConfiguration(getConfiguration(userName))\n      UserGroupInformation.loginUserFromKeytabAndReturnUGI(user\uff0c path)\n    } else {\n      UserGroupInformation.createRemoteUser(userName)\n    }\n')),(0,r.kt)("p",null,"Users only need to configure the following parameters in the configuration file linkis.properties:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"wds.linkis.keytab.enable=true\nwds.linkis.keytab.file=/appcom/keytab/ #keytab placement directory, which stores the username.keytab files of multiple users\nwds.linkis.keytab.host.enabled=false #Whether to bring principle client authentication\nwds.linkis.keytab.host=127.0.0.1 #principle authentication needs to bring the client IP\n")),(0,r.kt)("h2",{id:"q7-regarding-linkis-besides-supporting-deployment-user-login-can-other-user-logins-be-configured"},"Q7: Regarding Linkis, besides supporting deployment user login, can other user logins be configured?"),(0,r.kt)("p",null,"solution:\nsure. Deployment users are for convenience only. linkis-mg-gateway supports access by configuring LDAP service and SSO service. It does not have a user verification system. For example, to enable LDAP service access, you only need to configure linkis-mg-gateway.properties. The configuration of your LDAP server is as follows:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"wds.linkis.ldap.proxy.url=ldap://127.0.0.1:1389/#Your LDAP service URL\nwds.linkis.ldap.proxy.baseDN=dc=webank\uff0cdc=com#Configuration of your LDAP service\n")),(0,r.kt)("p",null,"If the user needs to perform tasks, a user with the corresponding user name needs to be created on the Linux server. If it is a standard version, the user needs to be able to perform Spark and hive tasks, and needs to establish a corresponding user in the local workspace and HDFS directory /tmp/linkis directory."),(0,r.kt)("h2",{id:"q8-how-to-open-linkis-management-console-administrator-page-ecm-and-microservice-management"},"Q8: How to open Linkis management console, administrator page ECM and microservice management?"),(0,r.kt)("p",null,(0,r.kt)("img",{src:n(61558).Z,width:"238",height:"336"})),(0,r.kt)("p",null,"Solution: You need to set the administrator in the conf/linkis.properties file:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"wds.linkis.governance.station.admin=hadoop,peacewong\n")),(0,r.kt)("p",null,"After the setting is complete, restart the publicservice service"),(0,r.kt)("h2",{id:"q9-when-starting-the-microservice-linkis-ps-publicservice-kjdbcutilsgetdriverclassname-npe"},"Q9: When starting the microservice linkis-ps-publicservice, kJdbcUtils.getDriverClassName NPE"),(0,r.kt)("p",null,"Specific exception stack: ExternalResourceProvider"),(0,r.kt)("p",null,(0,r.kt)("img",{src:n(86326).Z,width:"1908",height:"739"})),(0,r.kt)("p",null,"Solution: Caused by linkis-ps-publicservice configuration problem, modify the three parameters at the beginning of linkis.properties hive.meta:"),(0,r.kt)("h2",{id:"q10-when-scheduling-the-hive-engine-the-following-error-is-reported-engineconnpluginnotfoundexception-errorcode-70063"},"Q10: When scheduling the hive engine, the following error is reported: EngineConnPluginNotFoundException: errorCode: 70063"),(0,r.kt)("p",null,(0,r.kt)("img",{src:n(6401).Z,width:"1001",height:"492"})),(0,r.kt)("p",null,"Solution: The version of the corresponding engine was not modified during installation, so the engine type inserted into the db by default is the default version, and the compiled version is not the default version."),(0,r.kt)("p",null,"Specific modification steps:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"cd /appcom/Install/dss-linkis/linkis/lib/linkis-engineconn-plugins/\uff0c\nModify the v2.1.1 directory name in the dist directory to v1.2.1\nModify the subdirectory name 2.1.1 under the plugin directory to the default version 1.2.1\nIf it is Spark, you need to modify dist/2.4.3 and plugin/2.4.3 accordingly\nFinally restart the engineplugin service.\n")),(0,r.kt)("h2",{id:"q11-when-the-hive-engine-schedules-execution-the-following-error-is-reported-opertion-failed-nullpointerexception"},"Q11: When the hive engine schedules execution, the following error is reported: opertion failed NullPointerException:"),(0,r.kt)("p",null,(0,r.kt)("img",{src:n(5410).Z,width:"1855",height:"577"})),(0,r.kt)("p",null,"Solution: The server lacks environment variables, and /etc/profile adds ",(0,r.kt)("inlineCode",{parentName:"p"},"export HIVE_CONF_DIR=/etc/hive/conf;")),(0,r.kt)("h2",{id:"q12-when-the-spark-engine-starts-an-error-is-reported-get-the-queue-information-excepiton-obtaining-yarn-queue-information-exception-and-http-link-exception"},"Q12: When the spark engine starts, an error is reported get the queue information excepiton. (obtaining Yarn queue information exception) and http link exception"),(0,r.kt)("p",null,"Solution: To migrate the address configuration of yarn to the DB configuration, the following configuration needs to be added:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'INSERT INTO `linkis_cg_rm_external_resource_provider` (`resource_type`, `name`, `labels`, `config`) VALUES (\'Yarn\', \'default\', NULL, \'{\\r\\n"rmWebAddress": "http://xxip:xxport",\\r\\n"hadoopVersion": "2.7.2",\\r\\n"authorEnable":true,\\r\\n"user":"hadoop",\\r\\n"pwd":"xxxx"\\r\\n}\');\n\nconfig field example\n{\n"rmWebAddress": "http://10.10.10.10:8080",\n"hadoopVersion": "2.7.2",\n"authorEnable":true,\n"user":"hadoop",\n"pwd":"passwordxxx"\n}\n')),(0,r.kt)("p",null,(0,r.kt)("img",{src:n(19583).Z,width:"1414",height:"435"})),(0,r.kt)("h2",{id:"q13-pythonspark-scheduling-execution-error-initialize-python-executor-failed-classnotfoundexception-orgslf4jimplstaticloggerbinder"},"Q13: pythonspark scheduling execution, error: initialize python executor failed ClassNotFoundException org.slf4j.impl.StaticLoggerBinder"),(0,r.kt)("p",null,"details as follows:"),(0,r.kt)("p",null,(0,r.kt)("img",{src:n(18182).Z,width:"1912",height:"350"})),(0,r.kt)("p",null,"Solution: The reason is that the spark server lacks slf4j-log4j12-1.7.25.jar, copy the above jar to /opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/spark/jars ."),(0,r.kt)("h2",{id:"q14-common-package-conflicts"},"Q14: Common package conflicts:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"java.lang.NoSuchMethodError: javax.ws.rs.core.Application.getProperties()Ljava/util/Map;\nThe conflict package is: jsr311-api-1.1.1.jar may also conflict with jessery")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"java.lang.BootstrapMethodError: java.lang.NoSuchMethodError: javax.servlet.ServletContext.setInitParameter(Ljava/lang/String;Ljava/lang/String;)Z"),(0,r.kt)("p",{parentName:"li"},"The conflict package is: servlet-api.jar")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"org/eclipse/jetty/util/processorUtils\nThe conflicting package is: jetty-util-9.4.11.v20180605.jar This is the correct version")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"java.lang.NoClassDefFoundError: Could not initialize class dispatch.Http$"),(0,r.kt)("p",{parentName:"li"},"The conflicting package needs to be copied in: netty-3.6.2.Final.jar")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Conflicts caused by other jars brought in by hive-exec Calcite-avatica-1.6.0.jar may also bring conflicts in the jackson package, resulting in errors related to com.fasterxml.jackson.databind\nCannot inherit from final class is caused by calcite-avatica-1.6.0.jar")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"LZO compression problem hadoop-lzo jar\n7.org.eclipse.jetty.server.session.SessionHandler.getSessionManager()Lorg/eclipse/jetty/server/SessionManager;\nNeed to replace conflicting packages: jetty-servlet and jetty-security with 9.4.20"))),(0,r.kt)("h2",{id:"q15-the-mysql-script-running-scripts-reports-an-errorsql-engine-reports-an-error"},"Q15: The Mysql script running Scripts reports an error\\sql engine reports an error"),(0,r.kt)("p",null,"MYSQL script: run sql error:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"com.webank.wedatasphere.linkis.orchestrator.ecm.exception.ECMPluginErrorException: errCode: 12003 ,desc: localhost:9101_0 Failed  to async get EngineNode RMErrorException: errCode: 11006 ,desc: Failed to request external resourceClassCastException: org.json4s.JsonAST$JNothing$ cannot be cast to org.json4s.JsonAST$JString ,ip: localhost ,port: 9101 ,serviceKind: linkis-cg-linkismanager ,ip: localhost ,port: 9104 ,serviceKind: linkis-cg-entrance\n    at com.webank.wedatasphere.linkis.orchestrator.ecm.ComputationEngineConnManager.getEngineNodeAskManager(ComputationEngineConnManager.scala:157) ~[linkis-orchestrator-ecm-plugin-1.0.2.jar:?]\n")),(0,r.kt)("p",null,"Solution: modify the correct yarn address in the linkis_cg_rm_external_resource_provider table"),(0,r.kt)("h2",{id:"q16-the-waiting-time-for-scriptis-to-execute-the-script-is-long"},"Q16: The waiting time for scriptis to execute the script is long"),(0,r.kt)("p",null,(0,r.kt)("img",{src:n(65662).Z,width:"1483",height:"177"})),(0,r.kt)("p",null,"scriptis execution script waits for a long time, and reports Failed to async get EngineNode TimeoutException:\nSolution: You can check the linkismanager log, usually because the engine startup timed out"),(0,r.kt)("h2",{id:"q17-scriptis-executes-jdbc-script-and-reports-an-error"},"Q17: scriptis executes jdbc script and reports an error"),(0,r.kt)("p",null,"scriptis executes the jdbc script and reports an error"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"Failed  to async get EngineNode ErrorException: errCode: 0 ,desc: operation failed(\u64cd\u4f5c\u5931\u8d25)s\uff01the reason(\u539f\u56e0)\uff1aEngineConnPluginNotFoundException: errCode: 70063 ,desc: No plugin foundjdbc-4please check your configuration \n")),(0,r.kt)("p",null,"Solution\nYou need to install the corresponding engine plug-in, you can refer to: ",(0,r.kt)("a",{parentName:"p",href:"/docs/latest/deployment/install-engineconn"},"Engine Installation Guide")),(0,r.kt)("h2",{id:"q18-turn-off-resource-checking"},"Q18: Turn off resource checking"),(0,r.kt)("p",null,"Error reporting phenomenon: Insufficient resources\nLinkismanager service modify this configuration: wds.linkis.manager.rm.request.enable=false\nYou can clean up resource records, or set smaller resources\nor turn off detection\nLinkismanager service modify this configuration: wds.linkis.manager.rm.request.enable=false"),(0,r.kt)("h2",{id:"q19-executing-the-script-reports-an-error"},"Q19: Executing the script reports an error"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"GatewayErrorException: errCode: 11012 ,desc: Cannot find an instance in the routing chain of serviceId [linkis-cg-entrance], please retry ,ip: localhost ,port: 9001 ,serviceKind: linkis-mg-gateway\n")),(0,r.kt)("p",null,(0,r.kt)("img",{src:n(51154).Z,width:"1309",height:"92"})),(0,r.kt)("p",null,"A: Please check whether the linkis-cg-entrance service starts normally."),(0,r.kt)("h2",{id:"q20-scriptis-execute-script-timeoutexception"},"Q20: ScriptIs execute script TimeoutException"),(0,r.kt)("p",null,(0,r.kt)("img",{src:n(10937).Z,width:"2144",height:"1344"})),(0,r.kt)("p",null,"In linkis-cg-linkismanager.log, Need a ServiceInstance(linkis-cg-entrance, localhost:9104), but cannot find in DiscoveryClient refresh list is repeatedly printed.\nThis is because the instance is forcibly shut down, but the persistence in the database thinks that the instance still exists. The solution is to clear the two tables linkis_cg_manager_service_instance and linkis_cg_manager_service_instance_metrics after stopping the service."),(0,r.kt)("h2",{id:"q21-engine-timeout-setting"},"Q21: Engine timeout setting"),(0,r.kt)("p",null,(0,r.kt)("img",{src:n(67907).Z,width:"1852",height:"1648"})),(0,r.kt)("p",null,"\u2460 The parameter configuration of the management console can correspond to the engine parameters, and the timeout time can be modified. After saving, kill the existing engine.\n\u2461If the timeout configuration is not displayed, you need to manually modify the linkis-engineconn-plugins directory, the corresponding engine plugin directory such as spark/dist/v2.4.3/conf/linkis-engineconn.properties, the default configuration is wds.linkis.engineconn.max.free.time =1h, means 1h overtime, and can have units m and h. 0 means no timeout, no automatic kill. After the change, you need to restart ecp, and kill the existing engine, and run a new task to start the engine to take effect."),(0,r.kt)("h2",{id:"q22-when-creating-a-new-workflow-it-prompts-504-gateway-time-out"},'Q22: When creating a new workflow, it prompts "504 Gateway Time-out"'),(0,r.kt)("p",null,(0,r.kt)("img",{src:n(44281).Z,width:"3366",height:"855"})),(0,r.kt)("p",null,"Error message: The instance 05f211cb021e:9108 of application linkis-ps-cs is not exists. ,ip: 5d30e4bb2f42 ,port: 9001 ,serviceKind: linkis-mg-gateway, as shown below:"),(0,r.kt)("h2",{id:"q23-scripts-execute-python-scripts-the-content-of-the-script-is-very-simple-print-and-execute-successfully-normally-it-can-also-be-executed-successfully-through-the-task-scheduling-system-it-can-also-be-executed-successfully-through-the-job-flow-editing-job-script-page-but-an-error-is-reported-through-job-flow-execution"},"Q23: Scripts execute python scripts (the content of the script is very simple print) and execute successfully normally. It can also be executed successfully through the task scheduling system. It can also be executed successfully through the job flow editing job script page, but an error is reported through job flow execution."),(0,r.kt)("p",null,"Error message:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'You can go to this path(/opt/kepler/work/engine/hadoop/workDir/9c28976e-63ba-4d9d-b85e-b37d84144596/logs) to find the reason or ask the administrator for help ,ip: host1 ,port: 9101 ,serviceKind: linkis-cg-linkismanager ,ip: host1 ,port: 9104 ,serviceKind: linkis-cg-entrance\nException in thread "main" java.lang.NullPointerException\n    at com.webank.wedatasphere.linkis.rpc.sender.SpringCloudFeignConfigurationCache$.getClient(SpringCloudFeignConfigurationCache.scala:73)\n    at com.webank.wedatasphere.linkis.rpc.sender.SpringMVCRPCSender.doBuilder(SpringMVCRPCSender.scala:49)\nJava HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=250m; support was removed in 8.0\nJava HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0\nException in thread "main" java.lang.NullPointerException\n    at com.webank.wedatasphere.linkis.rpc.sender.SpringCloudFeignConfigurationCache$.getClient(SpringCloudFeignConfigurationCache.scala:73)\n    at com.webank.wedatasphere.linkis.rpc.sender.SpringMVCRPCSender.doBuilder(SpringMVCRPCSender.scala:49)\n    at com.webank.wedatasphere.linkis.rpc.BaseRPCSender.newRPC(BaseRPCSender.scala:67)\n    at com.webank.wedatasphere.linkis.rpc.BaseRPCSender.com$webank$wedatasphere$linkis$rpc$BaseRPCSender$$getRPC(BaseRPCSender.scala:54)\n    at com.webank.wedatasphere.linkis.rpc.BaseRPCSender.send(BaseRPCSender.scala:105)\n    at com.webank.wedatasphere.linkis.engineconn.callback.service.AbstractEngineConnStartUpCallback.callback(EngineConnCallback.scala:39)\n    at com.webank.wedatasphere.linkis.engineconn.callback.hook.CallbackEngineConnHook.afterEngineServerStartFailed(CallbackEngineConnHook.scala:63)\n    at com.webank.wedatasphere.linkis.engineconn.launch.EngineConnServer$$anonfun$main$15.apply(EngineConnServer.scala:64)\n    at com.webank.wedatasphere.linkis.engineconn.launch.EngineConnServer$$anonfun$main$15.apply(EngineConnServer.scala:64)\n    at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n    at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n    at com.webank.wedatasphere.linkis.engineconn.launch.EngineConnServer$.main(EngineConnServer.scala:64)\n    at com.webank.wedatasphere.linkis.engineconn.launch.EngineConnServer.main(EngineConnServer.scala)\n')),(0,r.kt)("p",null,"Solution: The conf in the /opt/kepler/work/engine/hadoop/workDir/9c28976e-63ba-4d9d-b85e-b37d84144596 directory is empty. lib and conf are detected by the system (linkis/lib/linkis-engineconn-plugins/python) when the microservice starts, and the python engine material package zip changes, and are automatically uploaded to the engine/engineConnPublickDir/ directory. The temporary solution to the problem is to copy the lib and conf content under linkis/lib/linkis-engineconn-plugins/python to the corresponding directory of engine/engineConnPublicDir/ (that is, the external link referenced in workDir/9c28976e-63ba-4d9d-b85e-b37d84144596 Under contents. The official solution needs to solve the problem that the change of the material package cannot be successfully uploaded to engineConnPublicDir."),(0,r.kt)("h2",{id:"q24-after-installing-exchangis050-click-on-the-dss-menu-to-enter-a-new-page-and-prompt-sorry-page-not-found-f12-view-has-404-exception"},'Q24: After installing Exchangis0.5.0, click on the dss menu to enter a new page and prompt "Sorry, Page Not Found". F12 view has 404 exception'),(0,r.kt)("p",null,"Error message: F12 view vue.runtime.esm.js:6785 GET ",(0,r.kt)("a",{parentName:"p",href:"http://10.0.xx.xx:29008/udes/auth?redirect=http%3A%2F%2F10.0.xx.xx%3A29008&dssurl="},"http://10.0.xx.xx:29008/udes/auth?redirect=http%3A%2F%2F10.0.xx.xx%3A29008&dssurl=")," http%3A%2F%2F10.0.xx.xx%3A8088&cookies=bdp-user-ticket-id%3DM7UZXQP9Ld1xeftV5DUGYeHdOc9oAFgW2HLiVea4FcQ%3D%3B%20workspaceId%3D225 404 (Not Found)"),(0,r.kt)("h2",{id:"q25-there-is-an-infinite-loop-in-configuring-atlas-in-hive-resulting-in-stack-overflow"},"Q25: There is an infinite loop in configuring atlas in HIVE, resulting in stack overflow"),(0,r.kt)("p",null,"You need to add all the content jar packages and subdirectories under ${ATLAS_HOME}/atlas/hook/hive/ to the lib directory of hive engine, otherwise AtlasPluginClassLoader cannot find the correct implementation class and finds the one under hive-bridge-shim class, resulting in an infinite loop\nHowever, the current execution method of Linkis (1.0.2) does not support subdirectories under lib, and the code needs to be modified. Refer to:\n",(0,r.kt)("a",{parentName:"p",href:"https://github.com/apache/linkis/pull/1058"},"https://github.com/apache/linkis/pull/1058")),(0,r.kt)("h2",{id:"q26-linkis10x-compiles-based-on-spark3-hadoop3-hive3-or-hdp314-please-refer-to"},"Q26: Linkis1.0.X compiles based on spark3 hadoop3 hive3 or hdp3.1.4, please refer to:"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://github.com/lordk911/Linkis/commits/master"},"https://github.com/lordk911/Linkis/commits/master"),"\nAfter compiling, please recompile DSS according to the compiled package, keep the same version of scala, and use the family bucket for web modules"),(0,r.kt)("h2",{id:"q27-linkis-cannot-get-user-name-when-executing-jdbc-task"},"Q27 Linkis cannot get user name when executing jdbc task"),(0,r.kt)("p",null,"2021-10-31 05:16:54.016 ERROR Task is Failed,errorMsg: NullPointerException: jdbc.username cannot be null.\nSource code: com.webank.wedatasphere.linkis.manager.engineplugin.jdbc.executer.JDBCEngineConnExecutor Received val properties = engineExecutorContext.getProperties.asInstanceOf[util.Map","[String, String]","] No jdbc.username parameter"),(0,r.kt)("p",null,"Solution 1:\nDocumentation: Temporary fixes for JDBC issues.note\nLink: ",(0,r.kt)("a",{parentName:"p",href:"http://note.youdao.com/noteshare?id=08163f429dd2e226a13877eba8bad1e3&sub=4ADEE86F433B4A59BBB20621A1C4B2AE"},"http://note.youdao.com/noteshare?id=08163f429dd2e226a13877eba8bad1e3&sub=4ADEE86F433B4A59BBB20621A1C4B2AE"),"\nSolution 2: Compare and modify this file\n",(0,r.kt)("a",{parentName:"p",href:"https://github.com/apache/linkis/blob/319213793881b0329022cf4137ee8d4c502395c7/linkis-engineconn-plugins/engineconn-plugins/jdbc/src/main/scala/com/webank/wedatasphere/linkis/manager/engineplugin/jdbc/"},"https://github.com/apache/linkis/blob/319213793881b0329022cf4137ee8d4c502395c7/linkis-engineconn-plugins/engineconn-plugins/jdbc/src/main/scala/com/webank/wedatasphere/linkis/manager/engineplugin/jdbc/")," executor/JDBCEngineConnExecutor.scala"),(0,r.kt)("h2",{id:"q28-after-changing-the-hive-version-in-the-configuration-before-installation-the-configuration-of-the-management-console-still-shows-the-version-as-233"},"Q28: After changing the hive version in the configuration before installation, the configuration of the management console still shows the version as 2.3.3"),(0,r.kt)("p",null,(0,r.kt)("img",{src:n(75357).Z,width:"1684",height:"922"})),(0,r.kt)("p",null,"Solution 1: It is determined that there is a bug in the install.sh script, please change the hive-1.2.1 here to hive-2.3.3 and reinstall it.\nSolution 2: If you don\u2019t want to reinstall, you need to change all the values \u200b\u200bof hive-2.3.3 in the label_value in the linkis_cg_manager_label table to the desired hive version\nNote: You are welcome to submit a PR to the github Linkis project to fix this problem, and then let us know, we will review and merge it into the code as soon as possible (currently unfixed, Deadline November 30, 2021)"),(0,r.kt)("h2",{id:"q29-when-linkis-cli-submits-a-task-it-prompts-group-by-clause-sql_modeonly_full_group_by-error"},"Q29: When linkis-cli submits a task, it prompts GROUP BY clause; sql_mode=only_full_group_by error"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"_8_codeExec_8 com.webank.wedatasphere.linkis.orchestrator.ecm.exception.ECMPluginErrorException: errCode: 12003 ,desc: uathadoop01:9101_8 Failed  to async get EngineNode MySQLSyntaxErrorException: Expression #6 of SELECT list is not in GROUP BY clause and contains nonaggregated column 'dss_linkis.si.name' which is not functionally dependent on columns in GROUP BY clause; this is incompatible with sql_mode=only_full_group_by ,ip: uathadoop01 ,port: 9104 ,serviceKind: linkis-cg-entrance\n")),(0,r.kt)("p",null,'Reason: This error occurs in mysql version 5.7 and above. Problem: Because the configuration strictly implements the "SQL92 standard", the solution: enter the /etc/mysql directory to modify the my.cnf file and add code under ',"[mysqld]",":\nsql_mode = STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION"),(0,r.kt)("h2",{id:"q30-an-error-is-reported-when-the-flink-engine-starts-and-the-tokencache-is-found"},"Q30: An error is reported when the flink engine starts and the TokenCache is found"),(0,r.kt)("p",null,"ERROR ","[main]"," com.webank.wedatasphere.linkis.engineconn.computation.executor.hook.ComputationEngineConnHook 57 error - EngineConnSever start failed! now exit. java.lang.NoClassDefFoundError: org/apache/hadoop/mapreduce/security/TokenCache\nReason: The jar package hadoop-mapreduce-client-core.jar is missing under flink-enginecon lib, just copy a copy from hadoop lib."),(0,r.kt)("h2",{id:"q31-when-starting-the-flink-enginespark-engine-the-engine-entrance-reports-an-error-orgjson4sjsonastjnothing-cannot-be-cast-to-orgjson4sjsonastjstring"},"Q31: When starting the flink engine/spark engine, the engine-entrance reports an error org.json4s.JsonAST$JNothing$ cannot be cast to org.json4s.JsonAST$JString"),(0,r.kt)("p",null,"The reason is that linkis-manager reports an error in the yarn queue to obtain an exception\nSolution: Modify the linkis_cg_rm_external_resource_provider table and modify the yarn queue information corresponding to config"),(0,r.kt)("h2",{id:"q32-classnotfoundexception-is-reported-when-the-function-script-is-executed"},"Q32: ClassNotFoundException is reported when the function script is executed"),(0,r.kt)("p",null,(0,r.kt)("img",{src:n(51954).Z,width:"1002",height:"192"})),(0,r.kt)("p",null,"Reason: linkis creates a function by adding the path of the function to the classPath, and then executing create temporary function. . . Statement, when submitting tasks to the yarn cluster in this way, the jar package of the function will not be uploaded to hdfs, resulting in class loading failure!"),(0,r.kt)("p",null,"Solution: Modify the method of generating function statements, or use HiveAddJarsEngineHook to solve it. Here, modify the constructCode method of JarUdfEngineHook. After packaging replace all"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'  override protected def constructCode(udfInfo: UDFInfo): String = {\n    "%sql\\n" + "add jar " + udfInfo.getPath + "\\n%sql\\n" + udfInfo.getRegisterFormat\n  }\n')),(0,r.kt)("h2",{id:"q33-failed-to-start-bean-webserverstartstop-when-linkis-executes-spark-task-in-cdh-environment"},"Q33: Failed to start bean 'webServerStartStop when Linkis executes Spark task in CDH environment"),(0,r.kt)("p",null,"Detailed log:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"Caused by: java.lang.IllegalStateException\nat org.eclipse.jetty.servlet.ServletHolder.setClassFrom(ServletHolder.java:300) ~[jetty-servlet-9.4.48.v20220622.jar:9.4.48.v20220622]\nat org.eclipse.jetty.servlet.ServletHolder.doStart(ServletHolder.java:347) ~[jetty-servlet-9.4.48.v20220622.jar:9.4.48.v20220622]\nat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73) ~[jetty-util-9.4.48.v20220622.jar:9.4.48.v20220622]\nat org.eclipse.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:749) ~[jetty-servlet-9.4.48.v20220622.jar:9.4.48.v20220622]\nat java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:357) ~[?:1.8.0_292]\nat java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:483) ~[?:1.8.0_292]\nat java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472) ~[?:1.8.0_292]\nat java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:313) ~[?:1.8.0_292]\nat java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:743) ~[?:1.8.0_292]\nat java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:647) ~[?:1.8.0_292]\nat org.eclipse.jetty.servlet.ServletHandler.initialize(ServletHandler.java:774) ~[jetty-servlet-9.4.48.v20220622.jar:9.4.48.v20220622]\nat org.springframework.boot.web.embedded.jetty.JettyEmbeddedWebAppContext$JettyEmbeddedServletHandler.deferredInitialize(JettyEmbeddedWebAppContext.java:46) ~[spring-boot-2.3.12.RELEASE.jar:2.3.12.RELEASE]\nat\n")),(0,r.kt)("p",null,"Reason: This is due to the conflict between the classPath and Linkis that CDH-Spark depends on at the bottom.\nSolution: On the machine where linkis is deployed, you can check the classPath in spark-env.sh, comment it out, and run it again. For details, please refer to ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/apache/linkis/issues/3282"},"3282")),(0,r.kt)("h2",{id:"q34-an-error-is-reported-when-running-the-flink-task-failed-to-create-engineconnplugin-comwebankwedataspherelinkisenginepluginhivehiveengineconnpluginjavalangclassnotfoundexception-comwebankwedataspherelinkisenginepluginhivehiveengineconnplugin"},"Q34: An error is reported when running the flink task: Failed to create engineConnPlugin: com.webank.wedatasphere.linkis.engineplugin.hive.HiveEngineConnPluginjava.lang.ClassNotFoundException: com.webank.wedatasphere.linkis.engineplugin.hive.HiveEngineConnPlugin"),(0,r.kt)("p",null,(0,r.kt)("img",{src:n(32573).Z,width:"1286",height:"174"})),(0,r.kt)("p",null,"Reason: The configuration file in the conf in the flink engine directory is empty, and the default configuration is read (by default, the hived engine configuration is read), delete the conf about flink in the configuration table and restart ecp"))}p.isMDXComponent=!0},61558:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/q15_1-7df6cf4928b02cb4cf2335fabbdaa193.png"},86326:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/q23_1-9745a8c9c95053318ee900d53cb50771.png"},89911:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/q24_1-ba0715c637d27f81c926e66cc6858102.png"},6401:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/q25_1-feffb49fb454a8fc0b0fd32dd2fe8c1a.png"},5410:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/q26_1-3ce3dfd6f40af09fce2c891b647df7d1.png"},49049:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/q27_1-9604d3aee4192bf09b279225c011357b.png"},45804:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/q29_1-1787e33057b87430ad3e23f25c945720.png"},51338:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/q2_1-697a210422acb1835076820356a56df5.png"},99262:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/q2_2-0b84185f007116f30800e4c4f5639004.png"},19583:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/q31_1-82ad7f3ad31c7ce507e41fa27f939e2a.png"},18182:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/q32_2-6771db328f6fedb7edde749b18e26be6.png"},65662:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/q35_1-c00cc3df2f86dd1c3eee9a152d93bb2a.png"},51154:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/q39_1-764160ac65a63a31c9cc9e2ea6f36f5c.png"},77886:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/q3_1-d1ab012f4d757efb95241370f4e2e3e1.png"},10937:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/q41_1-fc7fee9a7949d93925853282d6b34519.png"},67907:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/q43_1-2eab821fc930ab31beb6990a6c5db7a1.png"},44281:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/q44_1-bf45a4798e444d605dacf795803cbd9c.png"},75357:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/q50_1-da3747bd98c7849f807cf006c80fc4d0.png"},32573:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/q53_1-e521f8846a09ef0f3e8bf245174ca474.png"},51954:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/q55_1-dcb8abee478065f48183c591ca54f431.png"},14897:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/q5_1-0ee7a3e215d17b5032fba9039eaf41f9.jpg"},43957:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/q5_2-7c44e577054f2a65c49d197babf7a702.png"}}]);