"use strict";(self.webpackChunklinkis_web_apache=self.webpackChunklinkis_web_apache||[]).push([[11477],{30010:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"/2023/03/08/linkis-integration-with-oceanbase","metadata":{"permalink":"/blog/2023/03/08/linkis-integration-with-oceanbase","editUrl":"https://github.com/apache/linkis-website/edit/dev/blog/2023-03-08-linkis-integration-with-oceanbase/index.md","source":"@site/blog/2023-03-08-linkis-integration-with-oceanbase/index.md","title":"Linkis 1.3.2 Integration with OceanBase","description":"This article mainly introduces the integration of OceanBase database in Linkis 1.3.2 version. OceanBase database is compatible with most functions and syntax of MySQL 5.7/8.0. Therefore, the OceanBase database can be used as MySQL.","date":"2023-03-08T00:00:00.000Z","formattedDate":"March 8, 2023","tags":[],"readingTime":1.54,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Linkis 1.3.2 Integration with OceanBase","sidebar_position":3},"nextItem":{"title":"Engine Material Management","permalink":"/blog/2022/12/02/material-manage"}},"content":"This article mainly introduces the integration of OceanBase database in `Linkis` 1.3.2 version. OceanBase database is compatible with most functions and syntax of MySQL 5.7/8.0. Therefore, the OceanBase database can be used as MySQL.\\n\\n## 1. Preparations\\n### 1.1 Environment installation\\n\\nInstall and deploy the OceanBase database, see [Quick experience](https://en.oceanbase.com/docs/community-observer-en-10000000000829647)\\n\\n### 1.2 Environment Verification\\nYou can use the MySQL command to verify the installation of the OceanBase database.\\n```sql \\nmysql -h${ip} -P${port} -u${username} -p${password} -D${db_name}\\n```\\nThe connection is successful as shown in the figure below:\\n![](./img/ob-conn.png)\\n\\n## 2. Linkis submits OceanBase database tasks\\n### 2.1 Submit tasks through the shell\\nshell\\n\\n```shell\\n sh ./bin/linkis-cli -engineType jdbc-4 -codeType jdbc -code \\"show tables\\" -submitUser hadoop -proxyUser hadoop -runtimeMap wds.linkis.jdbc.connect.url=jdbc:mysql://${ip} :${port}/${db_name} -runtimeMap wds.linkis.jdbc.driver=com.mysql.jdbc.Driver -runtimeMap wds.linkis.jdbc.username=${username} -runtimeMap wds.linkis.jdbc.password =${password} \\n``` \\n\\n### 2.2 Submit tasks through Linkis SDK\\n`Linkis` provides `SDK` of `Java` and `Scala` to submit tasks to `Linkis` server. For details, please refer to [JAVA SDK Manual](/docs/latest/user-guide/sdk-manual). For `OceanBase` tasks, you only need to modify `EngineConnType` and `CodeType` parameters in `Demo`:\\n```java \\nMap<String, Object> labels = new HashMap<String, Object>(); \\nlabels.put (LabelKeyConstant.ENGINE_TYPE_KEY, \\"jdbc-4\\"); // required engineType Label\\nlabels.put(LabelKeyConstant.USER_CREATOR_TYPE_KEY, \\"hadoop-IDE\\");// required execute user and creator \\nlabels.put(LabelKeyConstant.CODE_TYPE_KEY, \\"jdbc\\"); // required codeType \\n``` \\n\\n### 2.3 Multi-data source support\\nAddress: Login Management Platform --\x3e Data Source Management\\n\\nStep 1: Create a new data source\\n\\n![](./img/ds-manage-en.png)\\n\\n![](./img/new-en.png)\\n\\nStep 2: Connection test\\n\\nClick Test Connect button to test\\n![](./img/ob-test-en.png)\\n\\nStep 3: Publish data source\\n\\n![](./img/publish-1-en.png)\\n\\n![](./img/publish-2-en.png)\\n\\nStep 4: Submit the OceanBase task by specifying the data source name\\n\\nRequest URL: `http://${gateway_url}:${port}/api/rest_j/v1/entrance/submit` Request\\n\\nmethod: POST\\n\\nRequest parameter:\\n```json \\n{\\n    \\"executionContent\\": {\\n        \\"code\\": \\"show databases\\",\\n        \\"runType\\": \\"jdbc\\"\\n    },\\n    \\"params\\": {\\n        \\"variable\\": {},\\n        \\"configuration\\": {\\n            \\"startup\\": {},\\n            \\"runtime\\": {\\n                \\"wds.linkis.engine.runtime.datasource\\": \\"ob-test\\"\\n            }\\n        }\\n    },\\n    \\"labels\\": {\\n        \\"engineType\\": \\"jdbc-4\\"\\n    }\\n}\\n```\\nResponse\uff1a\\n```json\\n{\\n  \\"method\\": \\"/api/entrance/submit\\",\\n  \\"status\\": 0,\\n  \\"message\\": \\"OK\\",\\n  \\"data\\": {\\n    \\"taskID\\": 93,\\n    \\"execID\\": \\"exec_id018017linkis-cg-entrance000830fb1364:9104IDE_hadoop_jdbc_0\\"\\n  }\\n}\\n```\\n\\n![](./img/show-status-en.png)"},{"id":"/2022/12/02/material-manage","metadata":{"permalink":"/blog/2022/12/02/material-manage","editUrl":"https://github.com/apache/linkis-website/edit/dev/blog/2022-12-02-material-manage/index.md","source":"@site/blog/2022-12-02-material-manage/index.md","title":"Engine Material Management","description":"background","date":"2022-12-02T00:00:00.000Z","formattedDate":"December 2, 2022","tags":[{"label":"bml","permalink":"/blog/tags/bml"},{"label":"linki1.3.1","permalink":"/blog/tags/linki-1-3-1"}],"readingTime":3.1,"hasTruncateMarker":false,"authors":[{"name":"aiceflower","title":"Development Engineer of WeBank","url":"https://github.com/aiceflower/","imageURL":"https://avatars.githubusercontent.com/u/22620332?s=400&v=4","key":"aiceflower"}],"frontMatter":{"title":"Engine Material Management","authors":["aiceflower"],"tags":["bml","linki1.3.1"]},"prevItem":{"title":"Linkis 1.3.2 Integration with OceanBase","permalink":"/blog/2023/03/08/linkis-integration-with-oceanbase"},"nextItem":{"title":"Apache Linkis 1.3.0 PES(Public Enhancement Services) Some Service Merge","permalink":"/blog/2022/10/09/linkis-service-merge"}},"content":"## background\\n\\nEngine material management is the linkis engine material management system, which is mainly used to manage Linkis engine material files and store various engine files of users, including engine type, engine version and other information. The overall process is that the compressed file is uploaded to the material library (BML) through the front-end browser, and the material compressed file is decompressed and verified. If the engine does not exist locally when it needs to be executed, it needs to be searched in the material library, downloaded, installed and registered for execution.\\n\\nHas the following function points:\\n\\n1) Support uploading packaged engine files. The size of uploaded files is affected by nginx configuration, and the file type is zip file type. It is not supported to package zip compressed files by yourself in the windows environment.\\n\\n2) Support for updating existing engine materials. After updating, add a storage version of bml engine materials in BML, and the current version can be rolled back and deleted.\\n\\n3) An engine involves two engine materials, namely lib and conf, which can be managed separately.\\n\\n## Architecture Diagram\\n\\n![](./img/bml.jpg)\\n\\n## Architecture Description\\n\\n1. Engine material management requires administrator privileges in the Linkis web management console, and the administrator field in the test environment needs to be set during development and debugging.\\n\\n2. Engine material management involves adding, updating, and deleting engine material files. Material files are divided into lib and conf to store them separately. The concept of two versions is involved in the file, one is the version of the engine itself, and the other is the material version. In the update operation, if the material is modified, a new material version will be added and stored in BML, which supports the material version delete and rollback.\\n\\n3. Use the BML Service to store the engine material files, call the BML service to store the files through RPC, and obtain the stored resource id and version and save them.\\n\\n### Core process\\n\\n1. Upload the engine plug-in file of zip type, first store it in the Home directory of the engine plug-in and decompress the file, and then start the refresh program.\\n2. Compress the conf and lib directories in the decompressed engine file, upload it to the BML (material management system), obtain the corresponding BML resource id and resource version, and read the corresponding engine name and version information.\\n3. In the engine material resource table, add a new engine material record, and each upload will generate lib and conf data respectively. In addition to recording the name and type information of the engine, the most important thing is to record the information of the engine in the material management system, including the resource id and version information of the engine, which are linked to the resource table in BML.\\n\\n## Database Design\\n\\nEngine Material Resource Information Table (linkis_cg_engine_conn_plugin_bml_resources)\\n\\n| Field name | Function | Remarks |\\n| --- | --- | --- |\\n| id | engine material package identification id | Primary key |\\n| engine_conn_type | The location where resources are stored | such as Spark |\\n| version | engine version | such as Spark\'s v2.4.3 |\\n| file_name | engine file name | such as lib.zip |\\n| file_size | engine file size | |\\n| last_modified | The last modification time of the file | |\\n| bml_resource_id | The id of the record resource in BML (material management system) | The id used to identify the engine file in BML |\\n| bml_resource_version | record resource version in BML | such as v000001 |\\n| create_time | resource creation time | |\\n| last_update_time | The last update time of the resource | |"},{"id":"/2022/10/09/linkis-service-merge","metadata":{"permalink":"/blog/2022/10/09/linkis-service-merge","editUrl":"https://github.com/apache/linkis-website/edit/dev/blog/2022-10-09-linkis-service-merge.md","source":"@site/blog/2022-10-09-linkis-service-merge.md","title":"Apache Linkis 1.3.0 PES(Public Enhancement Services) Some Service Merge","description":"Foreword","date":"2022-10-09T00:00:00.000Z","formattedDate":"October 9, 2022","tags":[{"label":"blog","permalink":"/blog/tags/blog"},{"label":"linki1.3.0","permalink":"/blog/tags/linki-1-3-0"},{"label":"service merge","permalink":"/blog/tags/service-merge"}],"readingTime":5.175,"hasTruncateMarker":false,"authors":[{"name":"aiceflower","title":"Development Engineer of WeBank","url":"https://github.com/aiceflower/","imageURL":"https://avatars.githubusercontent.com/u/22620332?s=400&v=4","key":"aiceflower"}],"frontMatter":{"title":"Apache Linkis 1.3.0 PES(Public Enhancement Services) Some Service Merge","authors":["aiceflower"],"tags":["blog","linki1.3.0","service merge"]},"prevItem":{"title":"Engine Material Management","permalink":"/blog/2022/12/02/material-manage"},"nextItem":{"title":"Deploy Apache Linkis1.1.1 and DSS1.1.0 based on CDH6.3.2","permalink":"/blog/2022/09/27/linkis111-deploy"}},"content":"### Foreword \\n\\nWith the development of business and the update and iteration of community products, we found that Linkis1 There are too many .X services, and services can be merged appropriately to reduce the number of services and facilitate deployment and debugging. At present, Linkis services are mainly divided into three categories, including computing governance services (CG: entrance/ecp/ecm/linkismanager), public enhancement services (PS: publicservice/datasource/cs) and microservice governance services (MG: Gateway/Eureka) . There are too many sub-services extended by these three types of services, and services can be merged, so that all PS services can be merged, CG services can be merged, and ecm services can be separated out. \\n\\n### Service merge changes \\n\\nThe main changes of this service merge are as follows: \\n\\n- Support Restful service forwarding: The modification point is mainly the forwarding logic of Gateway, similar to the current publicservice service merge parameter: wds.linkis.gateway.conf.publicservice.list \\n- Support Change the remote call of the RPC service to a local call, similar to LocalMessageSender, and now it is possible to complete the return of the local call by changing the Sender \\n- Configuration file changes \\n- Service start and stop script changes \\n\\n### To be achieved \\n\\n- Basic goal: merge PS services into one service \\n- Basic goal: merge CG service into CG-Service and ECM\\n- Advanced goal: merge CG services into one server \\n- Final goal: remove eureka, gateway into single service \\n\\n\\n### Specific changes \\n\\n#### Gateway changes (org.apache.linkis.gateway.ujes.route.HaContextGatewayRouter) \\n\\n```scala \\n//Override before changing \\ndef route(gatewayContext: GatewayContext): ServiceInstance = { \\n\\n    if (gatewayContext.getGatewayRoute.getRequestURI.contains(HaContextGatewayRouter.CONTEXT_SERVICE_STR) || \\n        gatewayContext.getGatewayRoute.getRequestURI.contains(HaContextGatewayRouter.OLD_CONTEXT_SERVICE_PREFIX)){ \\n      val params: util.HashMap[String, String] = gatewayContext.getGatewayRoute.getParams \\n      if (!gatewayContext.getRequest.getQueryParams.isEmpty) { \\n        for ((k, vArr) <- gatewayContext.getRequest.getQueryParams) {\\n          if (vArr.nonEmpty) {\\n            params.putIfAbsent(k, vArr.head)\\n          }\\n        }\\n      }\\n      if (gatewayContext.getRequest.getHeaders.containsKey(ContextHTTPConstant.CONTEXT_ID_STR)) {\\n        params.putIfAbsent(ContextHTTPConstant.CONTEXT_ID_STR, gatewayContext.getRequest.getHeaders.get(ContextHTTPConstant.CONTEXT_ID_STR)(0))\\n      }\\n      if (null == params || params.isEmpty) {\\n        dealContextCreate(gatewayContext)\\n      } else {\\n        var contextId : String = null\\n        for ((key, value) <- params) {\\n          if (key.equalsIgnoreCase(ContextHTTPConstant.CONTEXT_ID_STR)) {\\n            contextId = value\\n            }\\n        }\\n        if (StringUtils.isNotBlank(contextId)) {\\n          dealContextAccess(contextId.toString, gatewayContext)\\n        } else {\\n          dealContextCreate(gatewayContext)\\n        }\\n      }\\n    }else{\\n      null\\n    }\\n  }\\n  //after modification\\n  override def route(gatewayContext: GatewayContext): ServiceInstance = {\\n\\n    if (\\n        gatewayContext.getGatewayRoute.getRequestURI.contains(\\n          RPCConfiguration.CONTEXT_SERVICE_REQUEST_PREFIX\\n        )\\n    ) {\\n      val params: util.HashMap[String, String] = gatewayContext.getGatewayRoute.getParams\\n      if (!gatewayContext.getRequest.getQueryParams.isEmpty) {\\n        for ((k, vArr) <- gatewayContext.getRequest.getQueryParams.asScala) {\\n          if (vArr.nonEmpty) {\\n            params.putIfAbsent(k, vArr.head)\\n          }\\n        }\\n      }\\n      if (gatewayContext.getRequest.getHeaders.containsKey(ContextHTTPConstant.CONTEXT_ID_STR)) {\\n        params.putIfAbsent(\\n          ContextHTTPConstant.CONTEXT_ID_STR,\\n          gatewayContext.getRequest.getHeaders.get(ContextHTTPConstant.CONTEXT_ID_STR)(0)\\n        )\\n      }\\n      if (null == params || params.isEmpty) {\\n        dealContextCreate(gatewayContext)\\n      } else {\\n        var contextId: String = null\\n        for ((key, value) <- params.asScala) {\\n          if (key.equalsIgnoreCase(ContextHTTPConstant.CONTEXT_ID_STR)) {\\n            contextId = value\\n          }\\n        }\\n        if (StringUtils.isNotBlank(contextId)) {\\n          dealContextAccess(contextId, gatewayContext)\\n        } else {\\n          dealContextCreate(gatewayContext)\\n        }\\n      }\\n    } else {\\n      null\\n    }\\n  }\\n\\n\\n  // before modification\\n  def dealContextCreate(gatewayContext:GatewayContext):ServiceInstance = {\\n    val serviceId =  findService(HaContextGatewayRouter.CONTEXT_SERVICE_STR, list => {\\n      val services = list.filter(_.contains(HaContextGatewayRouter.CONTEXT_SERVICE_STR))\\n      services.headOption\\n    })\\n    val serviceInstances = ServiceInstanceUtils.getRPCServerLoader.getServiceInstances(serviceId.orNull)\\n    if (serviceInstances.size > 0) {\\n      val index = new Random().nextInt(serviceInstances.size)\\n      serviceInstances(index)\\n    } else {\\n      logger.error(s\\"No valid instance for service : \\" + serviceId.orNull)\\n      null\\n    }\\n  }\\n  //after modification\\n  def dealContextCreate(gatewayContext: GatewayContext): ServiceInstance = {\\n    val serviceId = findService(\\n      RPCConfiguration.CONTEXT_SERVICE_NAME,\\n      list => {\\n        val services = list.filter(_.contains(RPCConfiguration.CONTEXT_SERVICE_NAME))\\n        services.headOption\\n      }\\n    )\\n    val serviceInstances =\\n      ServiceInstanceUtils.getRPCServerLoader.getServiceInstances(serviceId.orNull)\\n    if (serviceInstances.size > 0) {\\n      val index = new Random().nextInt(serviceInstances.size)\\n      serviceInstances(index)\\n    } else {\\n      logger.error(s\\"No valid instance for service : \\" + serviceId.orNull)\\n      null\\n    }\\n  }\\n\\n  // before modification\\n  def dealContextAccess(contextIdStr:String, gatewayContext: GatewayContext):ServiceInstance = {\\n    val contextId : String = {\\n      var tmpId : String = null\\n      if (serializationHelper.accepts(contextIdStr)) {\\n        val contextID : ContextID = serializationHelper.deserialize(contextIdStr).asInstanceOf[ContextID]\\n        if (null != contextID) {\\n          tmpId = contextID.getContextId\\n        } else {\\n          logger.error(s\\"Deserializate contextID null. contextIDStr : \\" + contextIdStr)\\n        }\\n      } else {\\n        logger.error(s\\"ContxtIDStr cannot be deserialized. contextIDStr : \\" + contextIdStr)\\n      }\\n      if (null == tmpId) {\\n        contextIdStr\\n      } else {\\n        tmpId\\n      }\\n    }\\n    val instances = contextIDParser.parse(contextId)\\n    var serviceId:Option[String] = None\\n    serviceId = findService(HaContextGatewayRouter.CONTEXT_SERVICE_STR, list => {\\n      val services = list.filter(_.contains(HaContextGatewayRouter.CONTEXT_SERVICE_STR))\\n        services.headOption\\n      })\\n    val serviceInstances = ServiceInstanceUtils.getRPCServerLoader.getServiceInstances(serviceId.orNull)\\n    if (instances.size() > 0) {\\n      serviceId.map(ServiceInstance(_, instances.get(0))).orNull\\n    } else if (serviceInstances.size > 0) {\\n      serviceInstances(0)\\n    } else {\\n      logger.error(s\\"No valid instance for service : \\" + serviceId.orNull)\\n      null\\n    }\\n  }\\n\\n}\\n//after modification\\ndef dealContextAccess(contextIdStr: String, gatewayContext: GatewayContext): ServiceInstance = {\\n    val contextId: String = {\\n      var tmpId: String = null\\n      if (serializationHelper.accepts(contextIdStr)) {\\n        val contextID: ContextID =\\n          serializationHelper.deserialize(contextIdStr).asInstanceOf[ContextID]\\n        if (null != contextID) {\\n          tmpId = contextID.getContextId\\n        } else {\\n          logger.error(s\\"Deserializate contextID null. contextIDStr : \\" + contextIdStr)\\n        }\\n      } else {\\n        logger.error(s\\"ContxtIDStr cannot be deserialized. contextIDStr : \\" + contextIdStr)\\n      }\\n      if (null == tmpId) {\\n        contextIdStr\\n      } else {\\n        tmpId\\n      }\\n    }\\n    val instances = contextIDParser.parse(contextId)\\n    var serviceId: Option[String] = None\\n    serviceId = findService(\\n      RPCConfiguration.CONTEXT_SERVICE_NAME,\\n      list => {\\n        val services = list.filter(_.contains(RPCConfiguration.CONTEXT_SERVICE_NAME))\\n        services.headOption\\n      }\\n    )\\n    val serviceInstances =\\n      ServiceInstanceUtils.getRPCServerLoader.getServiceInstances(serviceId.orNull)\\n    if (instances.size() > 0) {\\n      serviceId.map(ServiceInstance(_, instances.get(0))).orNull\\n    } else if (serviceInstances.size > 0) {\\n      serviceInstances(0)\\n    } else {\\n      logger.error(s\\"No valid instance for service : \\" + serviceId.orNull)\\n      null\\n    }\\n  }\\n\\n// before modification\\nobject HaContextGatewayRouter{\\n  val CONTEXT_ID_STR:String = \\"contextId\\"\\n  val CONTEXT_SERVICE_STR:String = \\"ps-cs\\"\\n  @Deprecated\\n  val OLD_CONTEXT_SERVICE_PREFIX = \\"contextservice\\"\\n  val CONTEXT_REGEX: Regex = (normalPath(API_URL_PREFIX) + \\"rest_[a-zA-Z][a-zA-Z_0-9]*/(v\\\\\\\\d+)/contextservice/\\" + \\".+\\").r\\n}\\n//after modification\\nobject HaContextGatewayRouter {\\n\\n  val CONTEXT_ID_STR: String = \\"contextId\\"\\n\\n  @deprecated(\\"please use RPCConfiguration.CONTEXT_SERVICE_REQUEST_PREFIX\\")\\n  val CONTEXT_SERVICE_REQUEST_PREFIX = RPCConfiguration.CONTEXT_SERVICE_REQUEST_PREFIX\\n\\n  @deprecated(\\"please use RPCConfiguration.CONTEXT_SERVICE_NAME\\")\\n  val CONTEXT_SERVICE_NAME: String =\\n    if (\\n        RPCConfiguration.ENABLE_PUBLIC_SERVICE.getValue && RPCConfiguration.PUBLIC_SERVICE_LIST\\n          .exists(_.equalsIgnoreCase(RPCConfiguration.CONTEXT_SERVICE_REQUEST_PREFIX))\\n    ) {\\n      RPCConfiguration.PUBLIC_SERVICE_APPLICATION_NAME.getValue\\n    } else {\\n      RPCConfiguration.CONTEXT_SERVICE_APPLICATION_NAME.getValue\\n    }\\n\\n  val CONTEXT_REGEX: Regex =\\n    (normalPath(API_URL_PREFIX) + \\"rest_[a-zA-Z][a-zA-Z_0-9]*/(v\\\\\\\\d+)/contextservice/\\" + \\".+\\").r\\n\\n}\\n```\\n\\n#### RPC Service Change\uff08org.apache.linkis.rpc.conf.RPCConfiguration\uff09\\n```scala\\n//before modification\\nval BDP_RPC_BROADCAST_THREAD_SIZE: CommonVars[Integer] = CommonVars(\\"wds.linkis.rpc.broadcast.thread.num\\", new Integer(25))\\n//after modification\\nval BDP_RPC_BROADCAST_THREAD_SIZE: CommonVars[Integer] = CommonVars(\\"wds.linkis.rpc.broadcast.thread.num\\", 25)\\n\\n//before modification\\nval PUBLIC_SERVICE_LIST: Array[String] = CommonVars(\\"wds.linkis.gateway.conf.publicservice.list\\", \\"query,jobhistory,application,configuration,filesystem,udf,variable,microservice,errorcode,bml,datasource\\").getValue .split(\\",\\") \\n//after change \\nval PUBLIC_SERVICE_LIST: Array[String] = CommonVars(\\"wds.linkis.gateway.conf.publicservice.list\\", \\"cs,contextservice,data-source-manager,metadataquery,metadatamanager, query,jobhistory,application,configuration,filesystem,udf,variable,microservice,errorcode,bml,datasource\\").getValue.split(\\",\\") \\n\\n``` \\n#### Configuration file changes \\n```yaml \\n##Remove part #Delete the \\n\\nfollowing configuration files \\nlinkis-dist/package/conf/linkis-ps-cs.properties \\nlinkis-dist/package/conf/linkis-ps-data-source-manager.properties\\nlinkis-dist/package/conf/linkis-ps-metadataquery.properties\\n\\n##modified part\\n\\n#modify linkis-dist/package/conf/linkis-ps-publicservice.properties\\n#restful before modification\\nwds.linkis.server.restful.scan.packages=org.apache.linkis.jobhistory.restful,org.apache.linkis.variable.restful,org.apache.linkis.configuration.restful,org.apache.linkis.udf.api,org.apache.linkis.filesystem.restful,org.apache.linkis.filesystem.restful,org.apache.linkis.instance.label.restful,org.apache.linkis.metadata.restful.api,org.apache.linkis.cs.server.restful,org.apache.linkis.bml.restful,org.apache.linkis.errorcode.server.restful\\n\\n#restful after modification\\nwds.linkis.server.restful.scan.packages=org.apache.linkis.cs.server.restful,org.apache.linkis.datasourcemanager.core.restful,org.apache.linkis.metadata.query.server.restful,org.apache.linkis.jobhistory.restful,org.apache.linkis.variable.restful,org.apache.linkis.configuration.restful,org.apache.linkis.udf.api,org.apache.linkis.filesystem.restful,org.apache.linkis.filesystem.restful,org.apache.linkis.instance.label.restful,org.apache.linkis.metadata.restful.api,org.apache.linkis.cs.server.restful,org.apache.linkis.bml.restful,org.apache.linkis.errorcode.server.restful\\n\\n#mybatis before modification\\nwds.linkis.server.mybatis.mapperLocations=classpath:org/apache/linkis/jobhistory/dao/impl/*.xml,classpath:org/apache/linkis/variable/dao/impl/*.xml,classpath:org/apache/linkis/configuration/dao/impl/*.xml,classpath:org/apache/linkis/udf/dao/impl/*.xml,classpath:org/apache/linkis/instance/label/dao/impl/*.xml,classpath:org/apache/linkis/metadata/hive/dao/impl/*.xml,org/apache/linkis/metadata/dao/impl/*.xml,classpath:org/apache/linkis/bml/dao/impl/*.xml\\n\\nwds.linkis.server.mybatis.typeAliasesPackage=org.apache.linkis.configuration.entity,org.apache.linkis.jobhistory.entity,org.apache.linkis.udf.entity,org.apache.linkis.variable.entity,org.apache.linkis.instance.label.entity,org.apache.linkis.manager.entity,org.apache.linkis.metadata.domain,org.apache.linkis.bml.entity\\n\\nwds.linkis.server.mybatis.BasePackage=org.apache.linkis.jobhistory.dao,org.apache.linkis.variable.dao,org.apache.linkis.configuration.dao,org.apache.linkis.udf.dao,org.apache.linkis.instance.label.dao,org.apache.linkis.metadata.hive.dao,org.apache.linkis.metadata.dao,org.apache.linkis.bml.dao,org.apache.linkis.errorcode.server.dao,org.apache.linkis.publicservice.common.lock.dao\\n\\n#mybatis after modification\\nwds.linkis.server.mybatis.mapperLocations=classpath*:org/apache/linkis/cs/persistence/dao/impl/*.xml,classpath:org/apache/linkis/datasourcemanager/core/dao/mapper/*.xml,classpath:org/apache/linkis/jobhistory/dao/impl/*.xml,classpath:org/apache/linkis/variable/dao/impl/*.xml,classpath:org/apache/linkis/configuration/dao/impl/*.xml,classpath:org/apache/linkis/udf/dao/impl/*.xml,classpath:org/apache/linkis/instance/label/dao/impl/*.xml,classpath:org/apache/linkis/metadata/hive/dao/impl/*.xml,org/apache/linkis/metadata/dao/impl/*.xml,classpath:org/apache/linkis/bml/dao/impl/*.xml\\n\\nwds.linkis.server.mybatis.typeAliasesPackage=org.apache.linkis.cs.persistence.entity,org.apache.linkis.datasourcemanager.common.domain,org.apache.linkis.datasourcemanager.core.vo,org.apache.linkis.configuration.entity,org.apache.linkis.jobhistory.entity,org.apache.linkis.udf.entity,org.apache.linkis.variable.entity,org.apache.linkis.instance.label.entity,org.apache.linkis.manager.entity,org.apache.linkis.metadata.domain,org.apache.linkis.bml.entity\\n\\nwds.linkis.server.mybatis.BasePackage=org.apache.linkis.cs.persistence.dao,org.apache.linkis.datasourcemanager.core.dao,org.apache.linkis.jobhistory.dao,org.apache.linkis. variable.dao,org.apache.linkis.configuration.dao,org.apache.linkis.udf.dao,org.apache.linkis.instance.label.dao,org.apache.linkis.metadata.hive.dao,org. apache.linkis.metadata.dao,org.apache.linkis.bml.dao,org.apache.linkis.errorcode.server.dao,org.apache.linkis.publicservice.common.lock.dao \\n``` \\n\\n#### Deployment script changes (linkis-dist/package/sbin/linkis-start-all.sh) \\n```shell #Service \\nstartup script remove the following part \\n\\n#linkis-ps-cs \\nSERVER_NAME=\\"ps-cs\\" \\nSERVER_IP=$CS_INSTALL_IP \\nstartApp \\n\\nif [ \\"$ENABLE_METADATA_QUERY\\" == \\"true\\" ]; then \\n  #linkis-ps-data-source-manager\\n  SERVER_NAME=\\"ps-data-source-manager\\"\\n  SERVER_IP=$DATASOURCE_MANAGER_INSTALL_IP\\n  startApp\\n\\n  #linkis-ps-metadataquery\\n  SERVER_NAME=\\"ps-metadataquery\\"\\n  SERVER_IP=$METADATA_QUERY_INSTALL_IP\\n  startApp\\nfi\\n\\n#linkis-ps-cs\\nSERVER_NAME=\\"ps-cs\\"\\nSERVER_IP=$CS_INSTALL_IP\\ncheckServer\\n\\nif [ \\"$ENABLE_METADATA_QUERY\\" == \\"true\\" ]; then\\n  #linkis-ps-data-source-manager\\n  SERVER_NAME=\\"ps-data-source-manager\\"\\n  SERVER_IP=$DATASOURCE_MANAGER_INSTALL_IP\\n  checkServer\\n\\n  #linkis-ps-metadataquery\\n  SERVER_NAME=\\"ps-metadataquery\\"\\n  SERVER_IP=$METADATA_QUERY_INSTALL_IP\\n  checkServer\\nfi\\n\\n\\n#Service stop script remove the following part \\n#linkis-ps-cs \\nSERVER_NAME=\\"ps-cs\\" \\nSERVER_IP=$CS_INSTALL_IP \\nstopApp \\n\\nif [ \\"$ENABLE_METADATA_QUERY\\" == \\"true\\" ]; then \\n  #linkis-ps-data-source-manager \\n  SERVER_NAME =\\"ps-data-source-manager\\" \\n  SERVER_IP=$DATASOURCE_MANAGER_INSTALL_IP \\n  stopApp \\n\\n  #linkis-ps-metadataquery \\n  SERVER_NAME=\\"ps-metadataquery\\" \\n  SERVER_IP=$METADATA_QUERY_INSTALL_IP \\n  stopApp \\nfi \\n\\n``` \\nFor more details on service merge changes, see: https://github.com/apache/linkis/pull/2927/files"},{"id":"/2022/09/27/linkis111-deploy","metadata":{"permalink":"/blog/2022/09/27/linkis111-deploy","editUrl":"https://github.com/apache/linkis-website/edit/dev/blog/2022-09-27-linkis111-deploy/index.md","source":"@site/blog/2022-09-27-linkis111-deploy/index.md","title":"Deploy Apache Linkis1.1.1 and DSS1.1.0 based on CDH6.3.2","description":"With the development of business and the update and iteration of community products, we found that Linkis1. X has greatly improved its performance in terms of resource management and engine management, which can better meet the requirements of the construction of data middle stations. Compared with version 0.9.3 and the platform we used before, the user experience has also been greatly improved, and the problems such as the inability to view details on the task failure page have also been improved. Therefore, we decided to upgrade Linkis and the WDS suite. The following are the specific practical operations, which we hope will give you a reference.","date":"2022-09-27T00:00:00.000Z","formattedDate":"September 27, 2022","tags":[{"label":"blog","permalink":"/blog/tags/blog"},{"label":"linki1.1.1","permalink":"/blog/tags/linki-1-1-1"},{"label":"hadoop3.0.0-cdh6.3.2","permalink":"/blog/tags/hadoop-3-0-0-cdh-6-3-2"},{"label":"spark2.4.8","permalink":"/blog/tags/spark-2-4-8"},{"label":"hive2.1.1","permalink":"/blog/tags/hive-2-1-1"}],"readingTime":4.92,"hasTruncateMarker":false,"authors":[{"name":"kevinWdong","title":"contributors","url":"https://github.com/kongslove","imageURL":"https://avatars.githubusercontent.com/u/42604208?v=4","key":"kevinWdong"}],"frontMatter":{"title":"Deploy Apache Linkis1.1.1 and DSS1.1.0 based on CDH6.3.2","authors":["kevinWdong"],"tags":["blog","linki1.1.1","hadoop3.0.0-cdh6.3.2","spark2.4.8","hive2.1.1"]},"prevItem":{"title":"Apache Linkis 1.3.0 PES(Public Enhancement Services) Some Service Merge","permalink":"/blog/2022/10/09/linkis-service-merge"},"nextItem":{"title":"Linkis1.1.1 adapts Hadoop 3.1.1 and deploys other services","permalink":"/blog/2022/08/08/linkis111-compile-integration"}},"content":"With the development of business and the update and iteration of community products, we found that Linkis1. X has greatly improved its performance in terms of resource management and engine management, which can better meet the requirements of the construction of data middle stations. Compared with version 0.9.3 and the platform we used before, the user experience has also been greatly improved, and the problems such as the inability to view details on the task failure page have also been improved. Therefore, we decided to upgrade Linkis and the WDS suite. The following are the specific practical operations, which we hope will give you a reference.\\n\\n# 1.Environment\\n\\n## CDH6.3.2 Component versions\\n\\n* hadoop:3.0.0-cdh6.3.2\\n* hive:2.1.1-cdh6.3.2\\n* spark\uff1a2.4.8\\n\\n## hardware environment\xa0\\n\\n128G cloud physical machine*2\\n\\n# 2. Linkis installation and deployment\\n\\n## 2.1 Compile code or release installation package?\\n\\nThis installation deployment adopts the release installation package method. In order to adapt to the company\'s CDH6.3.2 version, the dependency packages of hadoop and hive need to be replaced with the CDH6.3.2 version. Here, the installation package is directly replaced. The dependent packages and modules to be replaced are shown in the following list.\\n\\n```plain\\n// Modules involved \\n\\nlinkis-engineconn-plugins/spark\\nlinkis-engineconn-plugins/hive\\n/linkis-commons/public-module\\n/linkis-computation-governance/\\n```\\n\\n```plain\\n// List of cdh packages that need to be replaced\\n\\n./lib/linkis-engineconn-plugins/spark/dist/v2.4.8/lib/hive-shims-0.23-2.1.1-cdh6.3.2.jar\\n./lib/linkis-engineconn-plugins/spark/dist/v2.4.8/lib/hive-shims-scheduler-2.1.1-cdh6.3.2.jar\\n./lib/linkis-engineconn-plugins/spark/dist/v2.4.8/lib/hadoop-annotations-3.0.0-cdh6.3.2.jar\\n./lib/linkis-engineconn-plugins/spark/dist/v2.4.8/lib/hadoop-auth-3.0.0-cdh6.3.2.jar\\n./lib/linkis-engineconn-plugins/spark/dist/v2.4.8/lib/hadoop-common-3.0.0-cdh6.3.2.jar\\n./lib/linkis-engineconn-plugins/spark/dist/v2.4.8/lib/hadoop-hdfs-3.0.0-cdh6.3.2.jar\\n./lib/linkis-engineconn-plugins/spark/dist/v2.4.8/lib/hadoop-hdfs-client-3.0.0-cdh6.3.2.jar\\n./lib/linkis-engineconn-plugins/hive/dist/v2.1.1/lib/hadoop-client-3.0.0-cdh6.3.2.jar\\n./lib/linkis-engineconn-plugins/hive/dist/v2.1.1/lib/hadoop-mapreduce-client-common-3.0.0-cdh6.3.2.jar\\n./lib/linkis-engineconn-plugins/hive/dist/v2.1.1/lib/hadoop-mapreduce-client-jobclient-3.0.0-cdh6.3.2.jar\\n./lib/linkis-engineconn-plugins/hive/dist/v2.1.1/lib/hadoop-yarn-api-3.0.0-cdh6.3.2.jar\\n./lib/linkis-engineconn-plugins/hive/dist/v2.1.1/lib/hadoop-yarn-client-3.0.0-cdh6.3.2.jar\\n./lib/linkis-engineconn-plugins/hive/dist/v2.1.1/lib/hadoop-yarn-server-common-3.0.0-cdh6.3.2.jar\\n./lib/linkis-engineconn-plugins/hive/dist/v2.1.1/lib/hadoop-hdfs-client-3.0.0-cdh6.3.2.jar\\n./lib/linkis-engineconn-plugins/hive/dist/v2.1.1/lib/hadoop-mapreduce-client-core-3.0.0-cdh6.3.2.jar\\n./lib/linkis-engineconn-plugins/hive/dist/v2.1.1/lib/hadoop-mapreduce-client-shuffle-3.0.0-cdh6.3.2.jar\\n./lib/linkis-engineconn-plugins/hive/dist/v2.1.1/lib/hadoop-yarn-common-3.0.0-cdh6.3.2.jar\\n./lib/linkis-engineconn-plugins/flink/dist/v1.12.2/lib/hadoop-annotations-3.0.0-cdh6.3.2.jar\\n./lib/linkis-engineconn-plugins/flink/dist/v1.12.2/lib/hadoop-auth-3.0.0-cdh6.3.2.jar\\n./lib/linkis-engineconn-plugins/flink/dist/v1.12.2/lib/hadoop-mapreduce-client-core-3.0.0-cdh6.3.2.jar\\n./lib/linkis-engineconn-plugins/flink/dist/v1.12.2/lib/hadoop-yarn-api-3.0.0-cdh6.3.2.jar\\n./lib/linkis-engineconn-plugins/flink/dist/v1.12.2/lib/hadoop-yarn-client-3.0.0-cdh6.3.2.jar\\n./lib/linkis-engineconn-plugins/flink/dist/v1.12.2/lib/hadoop-yarn-common-3.0.0-cdh6.3.2.jar\\n./lib/linkis-commons/public-module/hadoop-annotations-3.0.0-cdh6.3.2.jar\\n./lib/linkis-commons/public-module/hadoop-auth-3.0.0-cdh6.3.2.jar\\n./lib/linkis-commons/public-module/hadoop-common-3.0.0-cdh6.3.2.jar\\n./lib/linkis-commons/public-module/hadoop-hdfs-client-3.0.0-cdh6.3.2.jar\\n./lib/linkis-computation-governance/linkis-cg-linkismanager/hadoop-annotations-3.0.0-cdh6.3.2.jar\\n./lib/linkis-computation-governance/linkis-cg-linkismanager/hadoop-auth-3.0.0-cdh6.3.2.jar\\n./lib/linkis-computation-governance/linkis-cg-linkismanager/hadoop-yarn-api-3.0.0-cdh6.3.2.jar\\n./lib/linkis-computation-governance/linkis-cg-linkismanager/hadoop-yarn-client-3.0.0-cdh6.3.2.jar\\n./lib/linkis-computation-governance/linkis-cg-linkismanager/hadoop-yarn-common-3.0.0-cdh6.3.2.jar\\n\\n```\\n\\n\\n## 2.2 Problems encountered during deployment\\n\\n### 2.2.1  Kerberos configuration\\n\\nIt needs to be added in the linkis.properties public configuration\\n\\nEach engine conf also needs to be added\\n\\n```plain\\nwds.linkis.keytab.enable=true\\nwds.linkis.keytab.file=/hadoop/bigdata/kerberos/keytab\\nwds.linkis.keytab.host.enabled=false\\nwds.linkis.keytab.host=your_host\\n```\\n\\n### 2.2.2  Error is reported after Hadoop dependency package is replaced\\n\\njava.lang.NoClassDefFoundError:org/apache/commons/configuration2/Configuration\\n\\n![image](./img/config-err.png)\\n\\nCause: Configuration class conflict. Add a commons-configuration2-2.1.1.jar under the linkis commons module to resolve the conflict\\n\\n### \\n\\n### 2.2.3 Running spark, python, etc. in script reports no plugin for XXX\\n\\nPhenomenon: After modifying the version of Spark/Python in the configuration file, the startup engine reports no plugin for XXX\\n\\n![image](./img/pugin-error.png)\\n\\nReason: LabelCommonConfig.java and GovernanceCommonConf In scala, the version of the engine is written dead, the corresponding version is modified, and all jars containing these two classes (linkis computation governance common-1.1.1. jar and linkis label common-1.1.1. jar) in linkis and other components (including scheduleris) are replaced after compilation\\n\\n### 2.2.4 Python engine execution error, initialization failed\\n\\n* Modify python. py and remove the imported pandas module\\n* Configure the python loading directory and modify the python engine\'s linkis-enginecon.properties\\n\\n```plain\\npythonVersion=/usr/local/bin/python3.6\\n```\\n\\n### 2.2.5 Failed to run the pyspark task and reported an error\\n\\n![image](./img/pyspark-error.png)\\n\\nReason: PYSPARK is not set_ VERSION\\n\\nresolvent:\\n\\nSet two parameters in/etc/profile\\n```\\nexport PYSPARK_ PYTHON=/usr/local/bin/python3.6\\nexport PYSPARK_ DRIVER_PYTHON=/usr/local/bin/python3.6\\n```\\n### 2.2.6 Error occurs when executing the pyspark task\\n\\njava.lang.NoSuchFieldError: HIVE_ STATS_ JDBC_ TIMEOUT\\n\\n![image](./img/spark-hive-verion-error.png)\\n\\n \\n\\nReason: Spark 2.4.8 uses the hive1.2.1 package, but our hive has been upgraded to version 2.1.1. This parameter has been removed from hive2. Then the code in spark sql still calls the hive parameter, and then an error is reported,\\n\\nTherefore, HIVE is deleted from the spark sql/hive code_ STATS_ JDBC_ TIMEOUT This parameter is recompiled and packaged to replace the spark hive in spark 2.4.8_ 2.11-2.4.8.jar\\n\\n### 2.2.7 Proxy user exception during jdbc engine execution\\n\\nPhenomenon: User A is used to execute a jdbc task 1. The engine chooses to reuse it. Then I also use User B to execute a jdbc task 2. It is found that the submitter of task 2 is A\\n\\nAnalysis reason:\\n\\nConnectionManager::getConnection\\n\\n![image](./img/jdbc-engine-analyze.png)\\n\\nWhen creating a datasource, we judge whether to create it according to the key. The key is a jdbc url, but this granularity may be a bit large, because different users may access the same datasource, such as hive. Their urls are the same, but their account passwords are different. So when the first user creates a datasource, the username has been specified. When the second user comes in, If the data source is found to exist, it will be used directly instead of creating a new data source. Therefore, the code submitted by user B will be executed by user A.\\n\\nSolution: Reduce the key granularity of the data source cache map, and change it to jdbc. url+jdbc. user.\\n\\n3.  DSS deployment\\n    The installation process refers to the official website documents for installation configuration. The following describes some issues encountered in the installation and debugging process.\\n\\n## 3.1 The database list displayed on the left side of the DSS is incomplete\\n\\nAnalysis: The database information displayed in the DSS data source module is from the hive metabase. However, because of the permission control through the Sentry in CDH6, most of the hive table metadata information does not exist in the hive metastore, so the displayed data is missing.\\n\\nresolvent:\\n\\nThe original logic is transformed into the way of using jdbc to link hive and obtain table data display from jdbc.\\n\\nSimple logic description:\\n\\nThe properties information of jdbc is obtained through the IDE jdbc configuration information configured on the linkis console.\\n\\nDBS: Get the schema through connection. getMetaData()\\n\\nTBS: connection. getMetaData(). getTables() Get the tables under the corresponding db\\n\\nCOLUMNS: Get the columns information of the table by executing describe table\\n\\n## 3.2 Error jdbc is reported when executing jdbc script in DSS workflow name is empty\\n\\nAnalysis: The default creator in the dss workflow is Schedulis. Because the related engine parameters of Schedulis are not configured in the management console, the parameters read are all empty.\\n\\nAdding a category of Schedulis to the console gives an error, \u201dThe Schedulis directory already exists. Because the creator in the scheduling system is schedulis, the Schedulis Category cannot be added. In order to better identify each system, the default creator in the dss workflow is changed to nod_exception. This parameter can add wds. linkis. flow. job. creator. v1=nod_execution in the dss flow execution server. properties."},{"id":"/2022/08/08/linkis111-compile-integration","metadata":{"permalink":"/blog/2022/08/08/linkis111-compile-integration","editUrl":"https://github.com/apache/linkis-website/edit/dev/blog/2022-08-08-linkis111-compile-integration/index.md","source":"@site/blog/2022-08-08-linkis111-compile-integration/index.md","title":"Linkis1.1.1 adapts Hadoop 3.1.1 and deploys other services","description":"Environment and Version","date":"2022-08-08T00:00:00.000Z","formattedDate":"August 8, 2022","tags":[{"label":"blog","permalink":"/blog/tags/blog"},{"label":"linki1.1.1","permalink":"/blog/tags/linki-1-1-1"},{"label":"hadoop3.1.1","permalink":"/blog/tags/hadoop-3-1-1"},{"label":"spark3.0.1","permalink":"/blog/tags/spark-3-0-1"},{"label":"hive3.1.2","permalink":"/blog/tags/hive-3-1-2"},{"label":"flink1.13.2","permalink":"/blog/tags/flink-1-13-2"}],"readingTime":13.675,"hasTruncateMarker":false,"authors":[{"name":"ruY9527","title":"contributors","url":"https://github.com/ruY9527","imageURL":"https://avatars.githubusercontent.com/u/43773582?v=4","key":"ruY9527"}],"frontMatter":{"title":"Linkis1.1.1 adapts Hadoop 3.1.1 and deploys other services","authors":["ruY9527"],"tags":["blog","linki1.1.1","hadoop3.1.1","spark3.0.1","hive3.1.2","flink1.13.2"]},"prevItem":{"title":"Deploy Apache Linkis1.1.1 and DSS1.1.0 based on CDH6.3.2","permalink":"/blog/2022/09/27/linkis111-deploy"},"nextItem":{"title":"Deploy Linkis with Kubernetes","permalink":"/blog/2022/07/16/deploy-linkis-with-kubernetes"}},"content":"## Environment and Version\\n\\n- jdk-8 , maven-3.6.3\\n- node-14.15.0(Compiling the front end requires)\\n- Gradle-4.6(Compile qualitis quality service)\\n- hadoop-3.1.1,Spark-3.0.1,Hive-3.1.2,Flink-1.13.2,Sqoop-1.4.7 (Apache version)\\n- linkis-1.1.1\\n- DataSphereStudio-1.1.0\\n- Schudulis-0.7.0\\n- Qualitis-0.9.2\\n- Visualis-1.0.0\\n- Streamis-0.2.0\\n- Exchangis-1.0.0\\n- Chrome recommends versions below 100\\n\\n## Scenarios and versions of each component\\n\\n| System name      | Version | scene                                                        |\\n| ---------------- | ------- | ------------------------------------------------------------ |\\n| linkis           | 1.1.1   | Engine orchestration, running and executing hive, spark, flinksql, shell, python, etc., unified data source management, etc |\\n| DataSphereStudio | 1.1.0   | Implement DAG scheduling of tasks, integrate the specifications of other systems and provide unified access, and provide sparksql based service API |\\n| Schudulis        | 0.7.0   | Task scheduling, as well as scheduling details and rerouting, and provide trap data based on the selected time |\\n| Qualitis         | 0.9.2   | Provide built-in SQL version and other functions, check common data quality and customizable SQL, verify some data that does not conform to the rules, and write it to the corresponding library |\\n| Exchangis        | 1.0.0   | Hive to MySQL, data exchange between MySQL and hive          |\\n| Streamis         | 0.2.0   | Streaming development and Application Center                 |\\n| Visualis         | 1.0.0   | Visual report display, can share external links              |\\n\\n\\n## Deployment sequence\\n\\n  You can select and adjust the sequence after serial number 3 However, one thing to pay attention to when deploying exchangis is to copy the sqoop engine plug-in of exchangis to the engine plug-in package under lib of linkis\\n  Schedulis, qualitis, exchangis, streamis, visualis and other systems are integrated with DSS through their respective appconn. Note that after integrating the component appconn, restart the service module corresponding to DSS or restart DSS\\n\\n1. linkis\\n1. DataSphereStudio\\n1. Schedulis\\n1. Qualitis\\n1. Exchangis\\n1. Streamis\\n1. Visualis\\n\\n![image.png](./img/sx1.png)\\n\\nIf you integrate skywalking, you can see the service status and connection status in the extended topology diagram, as shown in the following figure:\\n![image.png](./img/sx2.png)\\nAt the same time, you can also clearly see the call link in the trace, as shown in the following figure, which is also convenient for you to locate the error log file of the specific service\\n![image.png](./img/sx3.png)\\n\\n## Dependency adjustment and packaging\\n\\n### linkis\\n\\nSince spark uses version 3. X, Scala also needs to be upgraded to version 12\\n[Original project code address](https://github.com/apache/linkis/tree/release-1.1.1)\\n[Adaptation modification code reference address](https://github.com/ruY9527/linkis/tree/release-1.1.1-hadoop3.x)\\n\\n#### The pom file of linkis\\n\\n```xml\\n<hadoop.version>3.1.1</hadoop.version>\\n<scala.version>2.12.10</scala.version>\\n<scala.binary.version>2.12</scala.binary.version>\\n\\n\x3c!-- hadoop-hdfs replace with hadoop-hdfs-client --\x3e\\n<dependency>\\n    <groupId>org.apache.hadoop</groupId>\\n    <artifactId>hadoop-hdfs-client</artifactId>\\n    <version>${hadoop.version}</version>\\n```\\n\\n#### The pom file of linkis-hadoop-common\\n\\n```xml\\n       \x3c!-- Notice here <version>${hadoop.version}</version> , adjust according to whether you have encountered any errors --\x3e \\n       <dependency>\\n            <groupId>org.apache.hadoop</groupId>\\n            <artifactId>hadoop-hdfs-client</artifactId>\\n            <version>${hadoop.version}</version>\\n        </dependency>\\n```\\n\\n#### The pom file of linkis-engineplugin-hive\\n\\n```xml\\n<hive.version>3.1.2</hive.version>\\n```\\n\\n#### The pom file of linkis-engineplugin-spark\\n\\n```xml\\n<spark.version>3.0.1</spark.version>\\n```\\n\\nThe getfield method in sparkscalaexecutor needs to adjust the following code\\n\\n```java\\nprotected def getField(obj: Object, name: String): Object = {\\n    // val field = obj.getClass.getField(name)\\n    val field = obj.getClass.getDeclaredField(\\"in0\\")\\n        field.setAccessible(true)\\n        field.get(obj)\\n  }\\n```\\n\\n#### The pom file of linkis-engineplugin-flink\\n\\n```xml\\n<flink.version>1.13.2</flink.version>\\n```\\n\\nDue to the adjustment of some classes in Flink 1.12.2 and 1.13.2, we refer to the temporary \\"violence\\" method given by the community students: copy the classes in part 1.12.2 to 1.13.2, adjust the scala version to 12, and compile them by ourselves\\nIt involves the specific modules of flink: flink-sql-client_${scala.binary.version}\\n\\n```\\n-- Note that the following classes are copied from 1.12.2 to 1.13.2\\norg.apache.flink.table.client.config.entries.DeploymentEntry\\norg.apache.flink.table.client.config.entries.ExecutionEntry\\norg.apache.flink.table.client.gateway.local.CollectBatchTableSink\\norg.apache.flink.table.client.gateway.local.CollectStreamTableSink\\n```\\n\\n![image.png](./img/flink1.png)![image.png](./img/flink2.png)\\n\\n#### linkis-engineplugin-python\\n\\n[Reference pr](https://github.com/apache/linkis/commit/7a26e85c53fc7cd55ddefbd78b1748b00f85ddd6)\\nIf resource / Python\'s python In the PY file, there is import pandas as PD. If you do not want to install pandas, you need to remove it\\n\\n#### linkis-label-common\\n\\norg.apache.linkis.manager.label.conf.LabelCommonConfig\\nModify the default version to facilitate the use of subsequent self compilation scheduling components\\n\\n```\\n    public static final CommonVars<String> SPARK_ENGINE_VERSION =\\n            CommonVars.apply(\\"wds.linkis.spark.engine.version\\", \\"3.0.1\\");\\n\\n    public static final CommonVars<String> HIVE_ENGINE_VERSION =\\n            CommonVars.apply(\\"wds.linkis.hive.engine.version\\", \\"3.1.2\\");\\n```\\n\\n#### linkis-computation-governance-common\\n\\norg.apache.linkis.governance.common.conf.GovernanceCommonConf\\nModify the default version to facilitate the use of subsequent self compilation scheduling components\\n\\n```\\n  val SPARK_ENGINE_VERSION = CommonVars(\\"wds.linkis.spark.engine.version\\", \\"3.0.1\\")\\n\\n  val HIVE_ENGINE_VERSION = CommonVars(\\"wds.linkis.hive.engine.version\\", \\"3.1.2\\")\\n```\\n\\n#### Compile\\n\\nEnsure that the above modifications and environments are available and implemented in sequence\\n\\n```shell\\n    cd linkis-x.x.x\\n    mvn -N  install\\n    mvn clean install -DskipTests\\n```\\n\\n#### Compilation error troubleshooting\\n\\n- If there is an error when you compile, try to enter a module to compile separately to see if there is an error and adjust it according to the specific error\\n- For example, the following example (the py4j version does not adapt when the group Friends adapt to the lower version of CDH): if you encounter this problem, you can adjust the version with the corresponding method to determine whether to adapt\\n\\n![image.png](./img/linkis1.png)\\n\\n### DataSphereStudio\\n\\n[Original project code address](https://github.com/WeBankFinTech/DataSphereStudio/tree/1.1.0)\\n[Adaptation modification code reference address](https://github.com/ruY9527/DataSphereStudio/tree/1.1.0-hadoop3.x)\\n\\n#### The pom file of DataSphereStudio\\n\\nSince DSS relies on linkis, all compilers should compile linkis before compiling DSS\\n\\n```xml\\n\x3c!-- scala consistent environment --\x3e\\n<scala.version>2.12.10</scala.version>\\n```\\n\\n#### dss-dolphinschuduler-token\\n\\nDolphinSchedulerTokenRestfulApi: Remove type conversion\\n\\n```\\nresponseRef.getValue(\\"expireTime\\")\\n```\\n\\n#### web tuning\\n\\n [Front end compilation address](https://github.com/WeBankFinTech/DataSphereStudio-Doc/blob/main/zh_CN/%E5%BC%80%E5%8F%91%E6%96%87%E6%A1%A3/%E5%89%8D%E7%AB%AF%E7%BC%96%E8%AF%91%E6%96%87%E6%A1%A3.md)\\n[Reference pr](https://github.com/WeBankFinTech/DataSphereStudio/commit/1dc9d99648e9f78b2dfb4776df4b9f46ef530c8a)\\nOverwrite the contents of the following directories from the master branch, or build the web based on the master branch\\n![image.png](./img/dss1.png)\\n\\n#### Compile\\n\\n```shell\\n    cd DataSphereStudio\\n    mvn -N  install\\n    mvn clean install -DskipTests\\n```\\n\\n### Schedulis\\n\\n[Original project code address](https://github.com/WeBankFinTech/Schedulis/tree/release-0.7.0)\\n[Adaptation modification code reference address](https://github.com/ruY9527/Schedulis/tree/release-0.7.0-hadoop.x)\\n\\n#### The pom file of Schedulis\\n\\n```xml\\n       <hadoop.version>3.1.1</hadoop.version>\\n       <hive.version>3.1.2</hive.version>\\n       <spark.version>3.0.1</spark.version>\\n```\\n\\n####  azkaban-jobtype\\n\\nDownload the jobtype file of the corresponding version (note the corresponding version): [Download address:](https://github.com/WeBankFinTech/Schedulis/blob/master/docs/schedulis_deploy_cn.md)\\nAfter downloading, put the entire jobtypes under jobtypes\\n![image.png](./img/schedulis1.png)\\n\\n### Qualitis\\n\\n[Original project code address](https://github.com/WeBankFinTech/Qualitis/tree/release-0.9.2)\\n\\n#### Forgerock package download\\n\\n[release\u5730\u5740](https://github.com/WeBankFinTech/Qualitis/releases) of [release-0.9.1](https://github.com/WeBankFinTech/Qualitis/releases/tag/release-0.9.1),after decompression, put it under. m2\\\\repository\\\\org\\n\\n#### Compile\\n\\nGradle version 4.6\\n\\n```shell\\ncd Qualitis\\ngradle clean distZip\\n```\\n\\nAfter compiling, there will be a qualitis-0.9.2.zip file under qualitis\\n![image.png](./img/qualitis1.png)\\n\\n#### dss-qualitis-appconn compile\\n\\nCopy the appconn to the appconns under datasphere studio (create the DSS quality appconn folder), as shown in the following figure:\\nCompile the DSS qualitis appconn. The qualitis under out is the package of integrating qualitis with DSS\\n![image.png](./img/qualitis2.png)\\n\\n### Exchangis\\n\\n[Original project code address](https://github.com/WeBankFinTech/Exchangis/tree/release-1.0.0)\\n[Adaptation modification code reference address](https://github.com/ruY9527/Exchangis/tree/release-1.0.0-hadoop3.x)\\n\\n#### The pom file of Exchangis\\n\\n```xml\\n\x3c!-- scala Consistent version --\x3e\\n<scala.version>2.12.10</scala.version>\\n```\\n\\n#### Back end compilation\\n\\n[Official compiled documents](https://github.com/WeBankFinTech/Exchangis/blob/dev-1.0.0/docs/zh_CN/ch1/exchangis_deploy_cn.md)\\nIn the target package of the assembly package, wedatasphere-exchangis-1.0.0.tar.gz is its own service package\\nLinkis engineplug sqoop needs to be put into linkis (lib/linkis enginecon plugins)\\nExchangis-appconn.zip needs to be put into DSS (DSS appconns)\\n\\n```xml\\nmvn clean install \\n```\\n\\n![image.png](./img/exchangis1.png)\\n\\n#### Front end compilation\\n\\nIf you deploy the front-end using nginx yourself, you need to pay attention to the dist folder under dist\\n![image.png](./img/exchangis2.png)\\n\\n### Visualis\\n\\n[Original project code address](https://github.com/WeBankFinTech/Visualis/tree/v1.0.0)\\n[Adaptation modification code reference address](https://github.com/ruY9527/Visualis/tree/v1.0.0-hadoop3.x)\\n\\n#### The pom file of Visualis\\n\\n```xml\\n<scala.version>2.12.10</scala.version>\\n```\\n\\n#### Compile\\n\\n [Official compiled documents](https://github.com/WeBankFinTech/Visualis/blob/master/visualis_docs/zh_CN/Visualis_deploy_doc_cn.md)\\nIn the target under assembly, visuis server zip is the package of its own service\\nThe target of visualis appconn is visualis.zip, which is the package required by DSS (DSS appconns)\\nBuild is the package printed by the front end\\n\\n```xml\\ncd Visualis\\nmvn -N install\\nmvn clean package -DskipTests=true\\n```\\n\\n![image.png](./img/visualis1.png)\\n\\n### Streamis\\n\\n[Original project code address](https://github.com/WeBankFinTech/Streamis/tree/0.2.0)\\n[Adaptation modification code reference address](https://github.com/ruY9527/Streamis/tree/0.2.0-hadoop3.x)\\n\\n#### The pom file of Streamis\\n\\n```xml\\n<scala.version>2.12.10</scala.version>\\n```\\n\\nThe pom file of streamis-project-server\\n\\n```xml\\n       \x3c!-- If you are 1.0.1 here, adjust it to ${dss.version} --\x3e\\n       <dependency>\\n            <groupId>com.webank.wedatasphere.dss</groupId>\\n            <artifactId>dss-sso-integration-standard</artifactId>\\n            <version>${dss.version}</version>\\n            <scope>compile</scope>\\n        </dependency>\\n```\\n\\n#### Compile\\n\\n [Official compiled documents](https://github.com/WeBankFinTech/Streamis/blob/main/docs/zh_CN/0.2.0/Streamis%E5%AE%89%E8%A3%85%E6%96%87%E6%A1%A3.md)\\nUnder assembly, the target package wedatasphere-streams-0.2.0-dist.tar.gz is the package of its own back-end service\\nThe stream.zip package of target under stream appconn is required by DSS (DSS appconns)\\ndist under dist is the front-end package\\n\\n```xml\\ncd ${STREAMIS_CODE_HOME}\\nmvn -N install\\nmvn clean install\\n```\\n\\n![image.png](./img/streamis1.png)\\n\\n## Installation deployment\\n\\n[Official deployment address](https://linkis.apache.org/zh-CN/docs/1.1.1/deployment/quick-deploy)\\n[Common error address](https://linkis.apache.org/zh-CN/blog/2022/02/21/linkis-deploy)\\n\\n### Path unification\\n\\nIt is recommended to deploy the relevant components in the same path (for example, I unzip them all in /home/hadoop/application)\\n![image.png](./img/deploy1.png)\\n\\n### Notes on linkis deployment\\n\\n#### Deploy config folder\\n\\ndb.sh, the address of the links connection configured by mysql, and the metadata connection address of hive\\nlinkis-env.sh\\n\\n```shell\\n-- The path to save the script script. Next time, there will be a folder with the user\'s name, and the script of the corresponding user will be stored in this folder\\nWORKSPACE_USER_ROOT_PATH=file:///home/hadoop/logs/linkis\\n-- Log files for storing materials and engine execution\\nHDFS_USER_ROOT_PATH=hdfs:///tmp/linkis\\n-- Log of each execution of the engine and information related to starting engineconnexec.sh\\nENGINECONN_ROOT_PATH=/home/hadoop/logs/linkis/apps\\n-- Access address of yarn master node (active resource manager)\\nYARN_RESTFUL_URL\\n-- Conf address of Hadoop / hive / spark\\nHADOOP_CONF_DIR\\nHIVE_CONF_DIR\\nSPARK_CONF_DIR\\n-- Specify the corresponding version\\nSPARK_VERSION=3.0.1\\nHIVE_VERSION=3.1.2\\n-- Specify the path after the installation of linkis. For example, I agree to specify the path under the corresponding component here\\nLINKIS_HOME=/home/hadoop/application/linkis/linkis-home\\n```\\n\\n#### flink\\n\\nIf you use Flink, you can try importing it from [flink-engine.sql](./img/flink-engine.sql)  into the database of linkis\\n\\nNeed to modify @Flink_LABEL version is the corresponding version, and the queue of yarn is default by default\\n\\nAt the same time, in this version, if you encounter the error of \\"1g\\" converting digital types, try to remove the 1g unit and the regular check rules. Refer to the following:\\n\\n![flink3.png](./img/flink3.png)\\n\\n#### lzo\\n\\nIf your hive uses LZO, copy the corresponding LZO jar package to the hive path. For example, the following path:\\n\\n```\\nlib/linkis-engineconn-plugins/hive/dist/v3.1.2/lib\\n```\\n\\n#### Frequently asked questions and precautions\\n\\n- The MySQL driver package must be copied to /lib/linkis-commons/public-module/ and /lib/linkis-spring-cloud-services/linkis-mg-gateway/\\n- Initialization password in conf/linkis-mg-gateway.properties -> wds.linkis.admin.password\\n- ps-cs in the startup script,there may be failures, if any,use sh linkis-daemon.sh ps-cs , start it separately\\n- At present, if there is time to back up the log, sometimes if the previous error log cannot be found, it may be backed up to the folder of the corresponding date\\n- At present lib/linkis-engineconn-plugins have only spark/shell/python/hive,If you want appconn, flink and sqoop, go to DSS, linkis and exchangis to get them\\n- Configuration file version check\\n\\n```shell\\nlinkis.properties,flink see if it is used\\nwds.linkis.spark.engine.version=3.0.1\\nwds.linkis.hive.engine.version=3.1.2\\nwds.linkis.flink.engine.version=1.13.2\\n```\\n\\n![image.png](./img/deploy2.png)\\n![image.png](./img/deploy3.png)\\n\\n#### Error record\\n\\n1. Incompatible versions. If you encounter the following error, it is whether the scala version is not completely consistent. Check and compile it\\n\\n![1905943989d7782456c356b6ce0d72b.png](./img/deploy4.png)\\n\\n2. Yarn configures the active node address. If the standby address is configured, the following error will appear:\\n\\n![1ca32f79d940016d72bf1393e4bccc8.jpg](./img/deploy5.jpg)\\n\\n\\n\\n### Considerations for DSS deployment\\n\\n[Official installation document](https://github.com/WeBankFinTech/DataSphereStudio-Doc/tree/main/zh_CN)\\n\\n#### config folder\\n\\ndb.sh: configure the database of DSS\\nconfig.sh\\n\\n```shell\\n-- The installation path of DSS, for example, is defined in the folder under DSS\\nDSS_INSTALL_HOME=/home/hadoop/application/dss/dss\\n```\\n\\n#### conf folder\\n\\ndss.properties\\n\\n```properties\\n# Mainly check whether spark / hive and other versions are available. If not, add\\nwds.linkis.spark.engine.version=3.0.1\\nwds.linkis.hive.engine.version=3.1.2\\nwds.linkis.flink.engine.version=1.13.2\\n```\\n\\ndss-flow-execution-server.properties\\n\\n```properties\\n# Mainly check whether spark / hive and other versions are available. If not, add\\nwds.linkis.spark.engine.version=3.0.1\\nwds.linkis.hive.engine.version=3.1.2\\nwds.linkis.flink.engine.version=1.13.2\\n```\\n\\nIf you want to use dolphin scheduler for scheduling, please add the corresponding spark / hive version to this pr\\n[Reference pr](https://github.com/WeBankFinTech/DataSphereStudio/pull/914/files)\\n\\n#### dss-appconns\\n\\nExchangis, qualitis, streamis and visualis should be obtained from the projects of exchangis, qualitis, streamis and visualis respectively\\n\\n#### Frequently asked questions and precautions\\n\\n- Since we integrate scheduleis, qualitis, exchangis and other components into DSS, all the interfaces of these components will be called synchronously when creating a project, so we ensure that dss_appconn_instance  configuration paths in the instance are correct and accessible\\n- The Chrome browser recommends that the kernel use version 100 or below. Otherwise, there will be a problem that you can separate scdulis, qaulitis and other components, but you cannot log in successfully through DSS\\n- Hostname and IP. If IP access is used, make sure it is IP when executing appconn-install.sh installation Otherwise, when accessing other components, you will be prompted that you do not have login or permission\\n\\n![ec4989a817646f785c59f6802d0fab2.jpg](./img/deploy6.jpg)\\n\\n\\n### Schedulis deployment considerations\\n\\n [Official deployment document](https://github.com/WeBankFinTech/Schedulis/blob/master/docs/schedulis_deploy_cn.md)\\n\\n#### conf folder\\n\\nazkaban.properties\\n\\n```properties\\n# azkaban.jobtype.plugin.dir and executor.global.properties. It\'s better to change the absolute path here\\n# Azkaban JobTypes Plugins\\nazkaban.jobtype.plugin.dir=/home/hadoop/application/schedulis/apps/schedulis_0.7.0_exec/plugins/jobtypes\\n\\n# Loader for projects\\nexecutor.global.properties=/home/hadoop/application/schedulis/apps/schedulis_0.7.0_exec/conf/global.properties\\n\\n# Engine version\\nwds.linkis.spark.engine.version=3.0.1\\nwds.linkis.hive.engine.version=3.1.2\\nwds.linkis.flink.engine.version=1.13.2\\n```\\n\\n#### web modular\\n\\nplugins/viewer/system/conf:  Here, you need to configure the database connection address to be consistent with scheduleis\\nazkaban.properties:  Configuration of user parameters and system management\\n\\n```properties\\nviewer.plugins=system\\nviewer.plugin.dir=/home/hadoop/application/schedulis/apps/schedulis_0.7.0_web/plugins/viewer\\n```\\n\\n#### Frequently asked questions and precautions\\n\\nIf there are resources or there are no static files such as CSS in the web interface, change the relevant path to an absolute path\\nIf the configuration file cannot be loaded, you can also change the path to an absolute path\\nFor example:\\n\\n```\\n### web module\\nweb.resource.dir=/home/hadoop/application/schedulis/apps/schedulis_0.7.0_web/web/\\nviewer.plugin.dir=/home/hadoop/application/schedulis/apps/schedulis_0.7.0_web/plugins/viewer\\n\\n### exec module\\nazkaban.jobtype.plugin.dir=/home/hadoop/application/schedulis/apps/schedulis_0.7.0_exec/plugins/jobtypes\\nexecutor.global.properties=/home/hadoop/application/schedulis/apps/schedulis_0.7.0_exec/conf/global.properties\\n```\\n\\n### Considerations for qualitis deployment\\n\\n[Official deployment document](https://github.com/WeBankFinTech/Qualitis/blob/master/docs/zh_CN/ch1/%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%E6%89%8B%E5%86%8C%E2%80%94%E2%80%94%E5%8D%95%E6%9C%BA%E7%89%88.md)\\n\\n#### conf folder\\n\\napplication-dev.yml\\n\\n```properties\\n  # The correct spark version is configured here\\n  spark:\\n    application:\\n      name: IDE\\n      reparation: 50\\n    engine:\\n      name: spark\\n      version: 3.0.1\\n```\\n\\n### Exchange deployment considerations\\n\\n[Official deployment document](https://github.com/WeBankFinTech/Exchangis/blob/dev-1.0.0/docs/zh_CN/ch1/exchangis_deploy_cn.md)\\n\\n#### Frequently asked questions and precautions\\n\\nIf you click the data source and there is an error that has not been published, you can try to add linkis_ps_dm_datasource_ -> published_version_id Modify the published_version_id value to 1 (if it is null)\\n\\n### Visualis\\n\\n[Official deployment document](https://github.com/WeBankFinTech/Visualis/blob/master/visualis_docs/zh_CN/Visualis_deploy_doc_cn.md)\\n\\n#### Frequently asked questions and precautions\\n\\nIf the preview view is inconsistent, please check whether the bin / phantomjs file is uploaded completely\\nIf you can see the following results, the upload is complete\\n\\n```properties\\n./phantomjs -v\\n2.1.1\\n```\\n\\n### Streamis\\n\\n [Official deployment document](https://github.com/WeBankFinTech/Streamis/blob/main/docs/zh_CN/0.2.0/Streamis%E5%AE%89%E8%A3%85%E6%96%87%E6%A1%A3.md)\\n\\n### dss-appconn\\n\\nQualitis, exchangis, streams and visualis are compiled from various modules, copied to DSS appconns under DSS, and then executed appconn-install.sh under bin to install their components\\nIf you find the following SQL script errors during integration, please check whether there are comments around the wrong SQL. If so, delete the comments and try appconn install again\\n![903ceec2f69fc1c7a2be5f309f69726.png](./img/deploy7.png)\\nFor example, for qualitis, the following IP and host ports are determined according to their specific use\\n\\n```\\nqualitis\\n127.0.0.1\\n8090\\n```\\n\\n## Nginx deployment example\\n\\n linkis.conf:   dss/linkis/visualis front end\\n exchangis.conf:  exchangis front end\\n streamis.conf:      streamis front end\\nScheduling and Qaulitis are in their own projects\\nLinkis / Visualis needs to change the dist or build packaged from the front end to the name of the corresponding component here\\n![image.png](./img/deploy8.png)\\n![image.png](./img/deploy9.png)\\n![image.png](./img/deploy10.png)\\n\\n#### linkis.conf\\n\\n```\\nserver {\\nlisten       8089;# Access port:\\nserver_name  localhost;\\n#charset koi8-r;\\n#access_log  /var/log/nginx/host.access.log  main;\\n\\nlocation /dss/visualis {\\n# Modify to your own front-end path\\nroot   /home/hadoop/application/webs; # Static file directory\\nautoindex on;\\n}\\n\\nlocation /dss/linkis {\\n# Modify to your own front-end path\\nroot   /home/hadoop/application/webs; # linkis Static file directory of management console\\nautoindex on;\\n}\\n\\nlocation / {\\n# Modify to your own front-end path\\nroot   /home/hadoop/application/webs/dist; # Static file directory\\n#root /home/hadoop/dss/web/dss/linkis;\\nindex  index.html index.html;\\n}\\n\\nlocation /ws {\\nproxy_pass http://127.0.0.1:9001;#Address of back-end linkis\\nproxy_http_version 1.1;\\nproxy_set_header Upgrade $http_upgrade;\\nproxy_set_header Connection upgrade;\\n}\\n\\nlocation /api {\\nproxy_pass http://127.0.0.1:9001; #Address of back-end linkis\\nproxy_set_header Host $host;\\nproxy_set_header X-Real-IP $remote_addr;\\nproxy_set_header x_real_ipP $remote_addr;\\nproxy_set_header remote_addr $remote_addr;\\nproxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\\nproxy_http_version 1.1;\\nproxy_connect_timeout 4s;\\nproxy_read_timeout 600s;\\nproxy_send_timeout 12s;\\nproxy_set_header Upgrade $http_upgrade;\\nproxy_set_header Connection upgrade;\\n}\\n\\n#error_page  404              /404.html;\\n# redirect server error pages to the static page /50x.html\\n#\\nerror_page   500 502 503 504  /50x.html;\\nlocation = /50x.html {\\nroot   /usr/share/nginx/html;\\n}\\n}\\n\\n```\\n\\n#### exchangis.conf\\n\\n```\\nserver {\\n            listen       9800; # Access port: if the port is occupied, it needs to be modified\\n            server_name  localhost;\\n            #charset koi8-r;\\n            #access_log  /var/log/nginx/host.access.log  main;\\n            location / {\\n            # Modify to own path\\n            root   /home/hadoop/application/webs/exchangis/dist/dist; #Modify to your own path\\n            autoindex on;\\n            }\\n\\n            location /api {\\n            proxy_pass http://127.0.0.1:9001;  # The address of the backend link needs to be modified\\n            proxy_set_header Host $host;\\n            proxy_set_header X-Real-IP $remote_addr;\\n            proxy_set_header x_real_ipP $remote_addr;\\n            proxy_set_header remote_addr $remote_addr;\\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\\n            proxy_http_version 1.1;\\n            proxy_connect_timeout 4s;\\n            proxy_read_timeout 600s;\\n            proxy_send_timeout 12s;\\n            proxy_set_header Upgrade $http_upgrade;\\n            proxy_set_header Connection upgrade;\\n            }\\n\\n            #error_page  404              /404.html;\\n            # redirect server error pages to the static page /50x.html\\n            #\\n            error_page   500 502 503 504  /50x.html;\\n            location = /50x.html {\\n            root   /usr/share/nginx/html;\\n            }\\n        }\\n\\n```\\n\\n#### streamis.conf\\n\\n```\\nserver {\\n    listen       9088;# Access port: if the port is occupied, it needs to be modified\\n    server_name  localhost;\\n    location / {\\n    # Modify to your own path\\n        root   /home/hadoop/application/webs/streamis/dist/dist;  #Modify to your own path\\n        index  index.html index.html;\\n    }\\n    location /api {\\n    proxy_pass http://127.0.0.1:9001;        # The address of the backend link needs to be modified\\n    proxy_set_header Host $host;\\n    proxy_set_header X-Real-IP $remote_addr;\\n    proxy_set_header x_real_ipP $remote_addr;\\n    proxy_set_header remote_addr $remote_addr;\\n    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\\n    proxy_http_version 1.1;\\n    proxy_connect_timeout 4s;\\n    proxy_read_timeout 600s;\\n    proxy_send_timeout 12s;\\n    proxy_set_header Upgrade $http_upgrade;\\n    proxy_set_header Connection upgrade;\\n    }\\n\\n    #error_page  404              /404.html;\\n    # redirect server error pages to the static page /50x.html\\n    #\\n    error_page   500 502 503 504  /50x.html;\\n    location = /50x.html {\\n    root   /usr/share/nginx/html;\\n    }\\n}\\n```"},{"id":"/2022/07/16/deploy-linkis-with-kubernetes","metadata":{"permalink":"/blog/2022/07/16/deploy-linkis-with-kubernetes","editUrl":"https://github.com/apache/linkis-website/edit/dev/blog/2022-07-16-deploy-linkis-with-kubernetes.md","source":"@site/blog/2022-07-16-deploy-linkis-with-kubernetes.md","title":"Deploy Linkis with Kubernetes","description":"1. Dependencies and versions","date":"2022-07-16T00:00:00.000Z","formattedDate":"July 16, 2022","tags":[{"label":"github","permalink":"/blog/tags/github"}],"readingTime":2.03,"hasTruncateMarker":false,"authors":[{"name":"jacktao","title":"contributors","url":"https://github.com/jacktao007","imageURL":"https://avatars.githubusercontent.com/u/1073365?v=4","key":"jacktao"}],"frontMatter":{"title":"Deploy Linkis with Kubernetes","authors":["jacktao"],"tags":["github"]},"prevItem":{"title":"Linkis1.1.1 adapts Hadoop 3.1.1 and deploys other services","permalink":"/blog/2022/08/08/linkis111-compile-integration"},"nextItem":{"title":"How to add a GitHub Action for the GitHub repository","permalink":"/blog/2022/07/04/how-to-add-auto-bot"}},"content":"**1. Dependencies and versions**\\n\\nkind github\uff1ahttps://github.com/kubernetes-sigs/kind\\n\\nkind website\uff1a[kind.sigs.k8s.io/](https://kind.sigs.k8s.io/)\\n\\nversion:\\n\\nkind 0.14.0\\n\\ndocker  20.10.17\\n\\nnode v16.0.0\\n\\nNote:\\n\\n1. Ensure that the front and back ends can compile properly\\n\\n2. Ensure that the component depends on the version\\n\\n3. Kind refers to the machine that uses docker container to simulate nodes. When the machine is restarted, the scheduler does not work because the container is changed.\\n\\n \\n\\n**2.Install the docker**\\n\\n\uff081\uff09Install the tutorial\\n\\n```\\nsudo yum install -y yum-utils device-mapper-persistent-data lvm2\\n\\nsudo yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\\n\\nsudo sed -i \'s+download.docker.com+mirrors.aliyun.com/docker-ce+\' /etc/yum.repos.d/docker-ce.repo\\n\\nsudo yum makecache fast\\n\\nsudo yum -y install docker-ce\\n\\nsystemctl start docker\\n\\nsystemctl enable docker\\n```\\n\\n\\n\\n\uff082\uff09setting image mirrors\\n\\n```\\nvi /etc/docker/daemon.json\\n\\n{\\n\\n\\"registry-mirrors\\": [\\"http://hub-mirror.c.163.com\\"],\\n\\n\\"insecure-registries\\": [\\"https://registry.mydomain.com\\",\\"http://hub-mirror.c.163.com\\"]\\n\\n}\\n```\\n\\n\\n\\n**3.install the kind**\\n\\n\uff081\uff09Manually download the Kind binary\\n\\nhttps://github.com/kubernetes-sigs/kind/releases\\n\\n\uff082\uff09Install kind binary\\n\\n```\\nchmod +x ./kind\\n\\nmv kind-linux-amd64 /usr/bin/kind\\n```\\n\\n\\n\\n**4.Install the JDK and Maven**\\n\\n\uff081\uff09Refer to the general installation tutorial to install the following components\\n\\njdk 1.8\\n\\nmavne 3.5+\\n\\n**5.Install the NodeJS**\\n\\n\uff081\uff09version\\n\\nnode v16.0.0\\n\\n\uff082\uff09install the nvm\\n\\n```\\nexport http_proxy=http://10.0.0.150:7890\\n\\nexport https_proxy=http://10.0.0.150:7890\\n\\ncurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.1/install.sh | bash\\n\\nexport NVM_DIR=\\"$HOME/.nvm\\"\\n\\n[ -s \\"$NVM_DIR/nvm.sh\\" ] && \\\\. \\"$NVM_DIR/nvm.sh\\"  # This loads nvm\\n\\n[ -s \\"$NVM_DIR/bash_completion\\" ] && \\\\. \\"$NVM_DIR/bash_completion\\"  # This loads nvm bash_completion\\n```\\n\\n\\n\\n\uff083\uff09install the nodejs\\n\\n```\\nnvm ls-remote\\n\\nnvm install v14.19.3\\n```\\n\\n\\n\\n\uff084\uff09setting NPM\\n\\n```\\nnpm config set registry https://registry.npmmirror.com\\n\\nnpm config set sass_binary_site https://registry.npmmirror.com/binary.html?path=node-sass/\\n```\\n\\n\\n\\n\uff085\uff09Compiler front-end\\n\\n```\\nnpm install -g yarn\\n\\nyarn\\n\\nyarn build\\n\\nyarn \\n```\\n\\n\\n\\n**6.Compile linkis**\\n\\n```\\n# 1. When compiling for the first time, execute the following command first\\n\\n./mvnw -N install\\n\\n# 2. make the linkis distribution package\\n\\n# - Option 1: make the linkis distribution package only\\n\\n./mvnw clean install -Dmaven.javadoc.skip=true -Dmaven.test.skip=true\\n\\n# - Option 2: make the linkis distribution package and docker image\\n\\n./mvnw clean install -Pdocker -Dmaven.javadoc.skip=true -Dmaven.test.skip=true\\n\\n# - Option 3: linkis distribution package and docker image (included web)\\n\\n./mvnw clean install -Pdocker -Dmaven.javadoc.skip=true -Dmaven.test.skip=true -Dlinkis.build.web=true\\n```\\n\\n\\n\\n\\n\\n**7.Create the cluster**\\n\\n```\\ndos2unix ./linkis-dist/helm/scripts/*.sh\\n\\n./linkis-dist/helm/scripts/create-test-kind.sh\\n```\\n\\n\\n\\n\\n\\n**8.install the helm charts**\\n\\n```\\n ./scripts/install-charts.sh linkis linkis-demo\\n```\\n\\n\\n\\n\\n\\n**9.Visit the Linkis page**\\n\\n```\\nkubectl port-forward -n linkis  --address=0.0.0.0 service/linkis-demo-web 8087:8087\\n\\nhttp://10.0.2.101:8087\\n```\\n\\n\\n\\n\\n\\n**10.Test using the Linkis client**\\n\\n```\\nkubectl -n linkis exec -it linkis-demo-ps-publicservice-77d7685d9-f59ht -- bash\\n./linkis-cli -engineType shell-1 -codeType shell -code \\"echo \\\\\\"hello\\\\\\" \\"  -submitUser hadoop -proxyUser hadoop\\n```\\n\\n\\n\\n**11.install the kubectl**\\n\\n```\\ncat <<EOF > /etc/yum.repos.d/kubernetes.repo\\n[kubernetes]\\nname=Kubernetes\\nbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/\\nenabled=1\\ngpgcheck=1\\nrepo_gpgcheck=1\\ngpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\\nEOF\\n\\nyum install -y --nogpgcheck kubectl\\n\\nkubectl config view\xa0\xa0\\nkubectl config get-contexts\xa0\xa0\\nkubectl cluster-info\xa0\xa0\\n```"},{"id":"/2022/07/04/how-to-add-auto-bot","metadata":{"permalink":"/blog/2022/07/04/how-to-add-auto-bot","editUrl":"https://github.com/apache/linkis-website/edit/dev/blog/2022-07-04-how-to-add-auto-bot.md","source":"@site/blog/2022-07-04-how-to-add-auto-bot.md","title":"How to add a GitHub Action for the GitHub repository","description":"1 Summary","date":"2022-07-04T00:00:00.000Z","formattedDate":"July 4, 2022","tags":[{"label":"github","permalink":"/blog/tags/github"}],"readingTime":8.91,"hasTruncateMarker":false,"authors":[{"name":"BeaconTown","title":"Student","url":"https://github.com/Beacontownfc/","imageURL":"https://avatars.githubusercontent.com/u/89081023?s=40&v=4","key":"BeaconTown"}],"frontMatter":{"title":"How to add a GitHub Action for the GitHub repository","authors":["BeaconTown"],"tags":["github"]},"prevItem":{"title":"Deploy Linkis with Kubernetes","permalink":"/blog/2022/07/16/deploy-linkis-with-kubernetes"},"nextItem":{"title":"Apache Linkis Meet up","permalink":"/blog/2022/06/09/meetup-content-review"}},"content":"## 1 Summary\\nAs you know, [continuous integration](#21-what-is-continuous-integration) consists of many operations, such as capturing code, running tests, logging in to remote servers, publishing to third-party services, and so on. GitHub calls these operations as Actions. Many operations are similar in different projects and can be shared. GitHub noticed this and came up with a wonderful idea to allow developers to write each operation as an independent script file and store it in the code repository so that other developers can reference it. If you need an action, you don\'t have to write a complex script by yourself. You can directly reference the action written by others. The whole continuous integration process becomes a combination of actions. This is the most special part of GitHub Actions.\\n\\nGitHub provides a [Github Action Market](https://github.com/marketplace) for developers, we can find the GitHub Action we want from this market and configure it into the `workflow` of the repository to realize automatic operation. Of course, the GitHub Action that this market can provide is limited. In some cases, we can\'t find a GitHub Action that can meet our needs. I will also teach you how to write GitHub Action by yourself later in this blog.\\n\\n## 2 Some terms\\n#### 2.1 What is continuous integration\\nIn short, it is an automated program. For example, every time the front-end programmer submits code to GitHub\'s repository, GitHub will automatically create a virtual machine (MAC / Windows / Linux) to execute one or more instructions (determined by us), for example:\\n\\n```bash\\nnpm install\\nnpm run build\\n```\\n\\n#### 2.2 What is YAML\\nThe way we integrate GitHub Action is to create a `Github/workflow` directory, with a `* yaml` file - this `yaml` file is the file we use to configure GitHub Action. It is a very easy scripting language. For users who are not familiar with `yaml`, you can refer to it [here](https://www.codeproject.com/Articles/1214409/Learn-YAML-in-five-minutes).\\n\\n## 3 Start writing the first Workflow\\n#### 3.1 How to customize the name of Workflow\\nGitHub displays the name of the Workflow on the action page of the repository. If we omit name, GitHub will set it as the Workflow file path relative to the repository root directory.\\n\\n```yaml\\nname: \\n  Say Hello\\n```\\n\\n#### 3.2 How to customize the trigger event of Workflow\\nThere are many events, for example, the user submits a pull request to the repository, the user submits an issue to the repository, or the user closes an issue, etc. We hope that when some events occur, the Workflow will be automatically executed, which requires the definition of trigger events. The following is an example of a custom trigger event:\\n\\n```yml\\nname: \\n  Say Hello\\non: \\n  pull_request\\n```\\n\\nThe above code can trigger workflow when the user submits a pull request. For multiple events, we enclose them in square brackets, for example:\\n\\n```yml\\nname: \\n  Say Hello\\non: \\n  [pull_request,pull]\\n```\\n\\nOf course, we hope that the triggering event can be more specific, such as triggering Workflow when a pull request is closed or reopened:\\n\\n```yml\\nname: \\n  Say Hello\\non: \\n  pull_request:\\n    type: \\n      [reopend,closed]\\n```\\n\\nFor more trigger events, please refer to [document](https://docs.github.com/en/actions/using-workflows/events-that-trigger-workflows#about-workflow-events) here.\\n\\n#### 3.3 How to define a job\\nA Workflow is composed of one or more jobs, which means that a continuous integration run can complete multiple tasks. Here is an example:\\n\\n```yml\\nname: \\n  Say Hello\\non: \\n  pull_request\\njobs:\\n  my_first_job:\\n    name: My first job\\n  my_second_job:\\n    name: My second job\\n```\\n\\nEach job must have an ID associated with it. Above `my_ first_ Job` and `my_ second_ Job` is the ID of the job.\\n\\n#### 3.4 How to specify the running environment of a job\\nSpecify the running environment for running jobs. The operating systems available on Workflow are:\\n- Windows\\n- macos\\n- linux\\n\\nThe following is an example of a specified running environment:\\n```yml\\n# Limited by space, the previous code is omitted\\njobs:\\n  my_first_job:\\n    name: My first job\\n  runs-on: macos-10.15\\n```\\n\\n#### 3.5 The use of step\\nEach job is composed of multiple steps, which will be executed from top to bottom. Step can run commands (such as linux commands) and actions.\\n\\nThe following is an example of outputting \\"Hello World\\":\\n```yml\\n# Limited by space, the previous code is omitted\\njobs:\\n  my_first_job:\\n    name: My first job\\n  runs-on: macos-10.15\\n  step:\\n    - name: Print a greeting\\n    # Define the environment variables of step\\n      env:\\n        FIRST_WORD: Hello\\n        SECOND_WORD: WORD\\n      # Run instructions: output environment variables\\n      run: |\\n        echo $FIRST_WORD $SECOND_WORD.\\n```\\n\\nNext is the use of action, which is actually a command. For example, GitHub officially gives us some [default commands](https://github.com/marketplace?type=actions&query=actions). We can directly use these commands to reduce the amount of Workflow code in the repository. The most common action is [Checkout](https://link.zhihu.com/?target=https%3A//github.com/marketplace/actions/checkout), it can clone the latest code in the repository into the Workflow workspace.\\n```yml\\n# Limited by space, the previous code is omitted\\n  step:\\n    - name: Check out git repository \\n      uses: actions/checkout@v2\\n```\\nSome actions require additional parameters to be passed in. Generally, `with` is used to set the parameter value:\\n```yml\\n# Limited by space, the previous code is omitted\\n  step:\\n    - name: Check out git repository \\n      uses: actions/checkout@v2\\n      uses: actions/setup-node@v2.2.0\\n        with:\\n          node-version: 14\\n```\\n\\n## 4 How to write your own action\\n#### 4.1 Configuration of action.yml\\nWhen we can\'t find the action we want in the GitHub Action Market, we can write an action to meet our needs by ourselves. The customized action needs to be created a new `\\"actions\\"` directory under the `\\".gitHub/workflow\\"` directory, and then create a directory with a custom action name. Each action needs an action configuration file: `action.yml`. The `runs` section of `action.yml` specifies the starting mode of the operation. There are three startup methods: `node.js Script`, `Docker Image`, and `Composite Script`. The common parameters of `action.yml` are described below:\\n- name: Customize the name of the action\\n- description: Declare the parameters or outputs that need to be passed in for action\\n- inputs: Customize the parameters to be input\\n- outputs: Output variables\\n- runs: Startup mode\\n\\nThe following is a configuration example of `action.yml`\uff1a\\n\\n```yml\\nname: \\"example action\\"\\n\\ndescription: \\"This is an example action\\"\\n\\ninputs:\\n  param1:\\n    description: \\"The first param of this action\\"\\n    required: true  #Required parameters must be set to true\\n\\n  param2:\\n    description: \\"The second param of this action\\"\\n    required: true\\n\\noutputs:\\n  out1:\\n    description: \\"The outputs of this action\\"\\n\\nruns:\\n  using: node16\\n  main: dist/index.js\\n  post: dist/index.js\\n```\\nSetting `runs.using` to `node16` or `node12` can be specified as the starting `node.js` script. The script file named `main` is the startup file. The way to start is similar to running the command `node main.js` directly. Therefore, dependency will not be installed from `package.json`. During development, we usually use the packaging tool to package the dependencies together, output a separate `JS` file, and then use this file as the entry point. The `runs.post` can specify the cleanup work, and the content here will be run at the end of the Workflow.\\n\\n#### 4.2 Using Docker Image\\nIf Docker is used, we need to modify the `runs` in `action.yml` to:\\n\\n```yml\\nruns:\\n  using: docker\\n  image: Dockerfile\\n```\\n`runs.image` specifies the dockerfile required for image startup, which is specified here as the dockerfile under the project root directory. In the dockerfile, specify the startup script with `ENTRYPOINT` or `CMD`. For example, define a program that runs scripts in `Python`:\\n```docker\\nFROM python:3\\n\\nRUN pip install --no-cache-dir requests\\n\\nCOPY . .\\n\\nCMD [ \\"python\\", \\"/main.py\\"]\\n```\\nHere we can see the advantages of using docker: you can customize the running environment, and you can use other program languages.\\n\\n## 5 GitHub Action project practice\\nIn this section, I will describe how to write your own GitHub Action with a specific example.\\n##### Problem\\nAssuming that there are many issues to be processed in our GitHub repository, each pull request submitted by the user may be associated with an issue. If you have to manually close an issue after merging a pull request, it will be quite cumbersome.\\n##### Resolve\\nThen workflow comes in handy. We can listen to the closed event of pull request and determine whether the closed event is closed by merged or non merged. If it is merged, the associated issue will be closed.\\n\\nBut there is still a problem here, how to obtain the associated issue? We can ask the user to add the issue that needs to be associated in the description part when submitting the pull request, such as `#345`, and then extract the issue number of `345`. How to realize this function? We can write GitHub Action by ourselves. In order to make the GitHub Action program more concise, here I use docker to start GitHub Action. First, prepare `action.yml`:\\n```yml\\n# The name of Github Action \\nname: \\"Auto_close_associate_issue\\"\\n# The description of action\\ndescription: \\"Auto close an issue which associate with a PR.\\"\\n\\n# Define parameters to be input\\ninputs:\\n  # The name of first param is prbody\\n  prbody: \\n    # The definition of the param\\n    description: \\"The body of the PR to search for related issues\\"\\n    # Required param\\n    required: true\\n\\noutputs:\\n  #The name of output param\\n  issurNumber:\\n    description: \\"The issue number\\"\\n\\nruns:\\n  # Using Docker Image\\n  using: \\"docker\\"\\n  image: \\"Dockerfile\\"\\n```\\n\\nThe next step is to write script files, where I use `node.js`. The idea of this script is: first obtain the variable value from the environment, extract the issue number, and then output it to the environment. The corresponding script (named main.js) is as follows:\\n```javascript\\n// Get environment variables. All parameters passed to GitHub Action are capitalized and the prefix INPUT_ is required, which is specified by GitHub\\nlet body = process.env[\'INPUT_PRBODY\']; \\n// Extract the number of issue by regular expression\\nlet pattern = /#\\\\d+/;\\nlet issueNumber = body.match(pattern)[0].replace(\'#\', \'\');\\n// Output the issue number to the environment\\nconsole.log(`::set-output name=issueNumber::${issueNumber}`);\\n```\\n\\nNext is the image file of Docker (the file name is `Dockerfile`):\\n```yml\\nFROM node:10.15\\n\\nCOPY . .\\n\\nCMD [ \\"node\\", \\"/main.js\\"]\\n```\\n\\nFinally, `action.yml`, `Dockerfile` and `main.js` is under the directory `.github/actions/Auto_close_associate_issue`, and the writing of an action is over.\\n\\nThe last step is to write Workflow. The configuration of Workflow is described in detail in [Start Writing the First Workflow](#3-start-writing-the-first-workflow), so I won\'t repeat it here. The specific configuration is as follows\uff1a\\n```yml\\nname: Auto close issue when PR is merged\\n\\non:\\n  pull_request_target:\\n    types: [ closed ]\\n\\njobs:\\n  close-issue:\\n    runs-on: ubuntu-latest\\n    steps:\\n      - uses: actions/checkout@v2\\n\\n      - name: \\"Auto issue closer\\"\\n        uses: ./.github/actions/Auto_close_associate_issue/\\n        id: Closer\\n        with:\\n          prbody: ${{ github.event.pull_request.body }}\\n\\n      - name: Close Issue\\n        uses: peter-evans/close-issue@v2\\n        if: ${{ github.event.pull_request.merged }}\\n        with:\\n          issue-number: ${{ steps.Closer.outputs.issueNumber }}\\n          comment: The associated PR has been merged, this issue is automatically closed, you can reopend if necessary.\\n        env:\\n          Github_Token: ${{ secrets.GITHUB_TOKEN }}\\n          PRNUM: ${{ github.event.pull_request.number }}\\n```"},{"id":"/2022/06/09/meetup-content-review","metadata":{"permalink":"/blog/2022/06/09/meetup-content-review","editUrl":"https://github.com/apache/linkis-website/edit/dev/blog/2022-06-09-meetup-content-review.md","source":"@site/blog/2022-06-09-meetup-content-review.md","title":"Apache Linkis Meet up","description":"|Data|Topic & Video|Scripts|","date":"2022-06-09T00:00:00.000Z","formattedDate":"June 9, 2022","tags":[{"label":"meetup","permalink":"/blog/tags/meetup"}],"readingTime":1.29,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Apache Linkis Meet up","tags":["meetup"]},"prevItem":{"title":"How to add a GitHub Action for the GitHub repository","permalink":"/blog/2022/07/04/how-to-add-auto-bot"},"nextItem":{"title":"How to Download Engine Plugins Not Included in the Installation Package By Default","permalink":"/blog/2022/04/15/how-to-download-engineconn-plugin"}},"content":"|**Data**|**Topic & Video**|**Scripts**|\\n|:----|:----|:----|\\n|8/27/2022|[Meetup 04\u671f\uff1a2022 \u4e00\u7ad9\u5f0f\u5f00\u6e90\u5927\u6570\u636e\u5e73\u53f0WeDataSphere\u793e\u533a\u5e74\u4e2d\u5927\u4f1a\u89c6\u9891\u96c6\u5408](https://space.bilibili.com/598542776/channel/collectiondetail?sid=681893)<br/>[Meetup 04\uff1a2022 Community mid-year conference video collection of WeDataSphere One-stop Open Source Big Data Platform](https://space.bilibili.com/598542776/channel/collectiondetail?sid=681893)|    [ \u6f14\u8bb2\u7a3f\u5408\u96c6 A collection of speeches ]( https://pan.baidu.com/s/1_Ectkxf5WpRcaLQXWxu72g?pwd=eeik)\\n|6/27/2022|[ Meetup 03\u671f\uff1aLinkis-1.1.0\u65b0\u529f\u80fd\u4ecb\u7ecd\u3001\u4e0a\u6d77\u5408\u5408\u4fe1\u606f\u5408\u6570\u636e\u5de5\u574aIDS](https://space.bilibili.com/598542776/channel/seriesdetail?sid=2649896)<br/>[Meetup 03: Linkis-1.1.0 New function introduction, Shanghai HEHE Information and Data Workshop IDS](https://space.bilibili.com/598542776/channel/seriesdetail?sid=2649896)|    [ \u6f14\u8bb2\u7a3f\u5408\u96c6 A collection of speeches ](https://pan.baidu.com/s/1TpN4qJFdjA666PWmWyiw6A?pwd=na3h)\\n|4/13/2022|[Meetup 02\u671f\uff1aQualitis \u6570\u636e\u8d28\u91cf 0.9.0\u7248\u672c\u4ecb\u7ecd](https://www.bilibili.com/video/BV1r5411U7Dx/)<br/>[Meetup 02\uff1aIntroduction to Data Quality Platform Qualitis\xa0V0.9.0 ](https://www.bilibili.com/video/BV1r5411U7Dx/?spm_id_from=333.788.recommend_more_video.-1)|    [Qualitis \u6570\u636e\u8d28\u91cf0.9.0\u7248\u672c\u4ecb\u7ecd.pptx](https://uploader.shimo.im/f/bpeDQgadrMVFUiNL.pptx?fileGuid=m8AZV9z6xMsOevAb)|\\n|4/13/2022|[Meetup 02\u671f\uff1aProphecis \u673a\u5668\u5b66\u4e60Studio\u7248\u672c\u66f4\u65b0\u4ecb\u7ecd](https://www.bilibili.com/video/BV1WS4y127ma/)<br/>[Meetup 02\uff1aIntroduction to Mechine Learning Platform Prophecis Studio](https://www.bilibili.com/video/BV1r5411U7Dx/?spm_id_from=333.788.recommend_more_video.-1)|[Prophecis \u673a\u5668\u5b66\u4e60Studio\u7248\u672c\u66f4\u65b0\u4ecb\u7ecd(V0.3.0).pptx](https://uploader.shimo.im/f/eL3HOVtYTgeCGOtr.pptx?fileGuid=m8AZV9z6xMsOevAb)|\\n|4/13/2022|[Meetup 02\u671f\uff1a\u8c03\u5ea6\u7cfb\u7edf Schedulis 0.6.2 \u53d1\u5e03](https://www.bilibili.com/video/BV1vY4y1H7j2/)<br/>[Meetup 02](https://www.bilibili.com/video/BV1vY4y1H7j2)[: The Scheduling System Schedulis is Release v0.6.2](https://www.bilibili.com/video/BV1vY4y1H7j2)|[Schedulis 0.6.2 \u53d1\u5e03.pptx](https://uploader.shimo.im/f/rD98D9SwNavhWga3.pptx?fileGuid=m8AZV9z6xMsOevAb)|\\n|4/13/2022|[Meetup 02\u671f\uff1aDataSphereStudio 1.0 \u7cfb\u7edf\u4ecb\u7ecd](https://www.bilibili.com/video/BV1pF411G7AH/)<br/>[Meetup 02:\xa0Introduciton to DataSphereStudio 1.0 System](https://www.bilibili.com/video/BV1pF411G7AH) |[DSS 1.0.1 \u7cfb\u7edf\u4ecb\u7ecd-WeDataSphere meetup 202204(1).pptx](https://uploader.shimo.im/f/HluZrVJDRpEsSYy4.pptx?fileGuid=m8AZV9z6xMsOevAb)|\\n|2/24/2022|[Meetup 01:Linkis 1.0.3 \u65b0\u7248\u672c\u53d1\u5e03](https://www.bilibili.com/video/BV1La411h7Pf)<br/>[Meetup 01:Linkis 1.0.3 New Version Released ](https://www.bilibili.com/video/BV1La411h7Pf)|[\u57fa\u4e8eLinkis\u7684\u4f01\u4e1a\u5927\u6570\u636e\u5e73\u53f0\u6539\u9020\u4e4b\u8def_\u5f20\u5ef6\u53ec.pdf](https://uploader.shimo.im/f/qM7PpEBm8bUowyUW.pdf?fileGuid=m8AZV9z6xMsOevAb)<br/>[Apache Linkis V1.0.3 \u4ecb\u7ecd-v0.2-\u90b8\u5e05.pdf](https://uploader.shimo.im/f/GcYE1BwtDKHFfN8W.pdf?fileGuid=m8AZV9z6xMsOevAb)|"},{"id":"/2022/04/15/how-to-download-engineconn-plugin","metadata":{"permalink":"/blog/2022/04/15/how-to-download-engineconn-plugin","editUrl":"https://github.com/apache/linkis-website/edit/dev/blog/2022-04-15-how-to-download-engineconn-plugin.md","source":"@site/blog/2022-04-15-how-to-download-engineconn-plugin.md","title":"How to Download Engine Plugins Not Included in the Installation Package By Default","description":"This article mainly guides you how to download the non-default engine installation plug-in package corresponding to each version.","date":"2022-04-15T00:00:00.000Z","formattedDate":"April 15, 2022","tags":[{"label":"engine","permalink":"/blog/tags/engine"},{"label":"guide","permalink":"/blog/tags/guide"}],"readingTime":1.985,"hasTruncateMarker":false,"authors":[{"name":"Casion","title":"Development Engineer of WeBank","url":"https://github.com/casionone/","imageURL":"https://avatars.githubusercontent.com/u/7869972?v=4","key":"Casion"}],"frontMatter":{"title":"How to Download Engine Plugins Not Included in the Installation Package By Default","authors":["Casion"],"tags":["engine","guide"]},"prevItem":{"title":"Apache Linkis Meet up","permalink":"/blog/2022/06/09/meetup-content-review"},"nextItem":{"title":"Implementation of OpenLookEng Engine","permalink":"/blog/2022/03/20/openlookeng"}},"content":"> _This article mainly guides you how to download the non-default engine installation plug-in package corresponding to each version. _\\n\\nConsidering the size of the release package and the use of plug-ins, the binary installation package released by linkis only contains some common engines /hive/spark/python/shell.\\nVery useful engine, there are corresponding modules `flink/io_file/pipeline/sqoop` in the project code (there may be differences between different versions),\\nIn order to facilitate everyone\'s use, based on the release branch code of each version of linkis: https://github.com/apache/linkis, this part of the engine is compiled for everyone to choose and use.\\n\\n ## Download link\\n| **linkis version** | **engines included** |**engine material package download link** |\\n|:---- |:---- |:---- |\\n|1.3.2|jdbc<br/>pipeline<br/>io_file<br/>flink<br/>openlookeng<br/>sqoop<br/>presto<br/>elasticsearch<br/>trino<br/>seatunnel<br/>|[1.3.2-engineconn-plugin.tar](https://osp-1257653870.cos.ap-guangzhou.myqcloud.com/WeDatasphere/Linkis/engineconn-plugin/1.3.2-engineconn-plugin.tar)|\\n|1.3.1|jdbc<br/>pipeline<br/>io_file<br/>flink<br/>openlookeng<br/>sqoop<br/>presto<br/>elasticsearch<br/>trino<br/>seatunnel<br/>|[1.3.1-engineconn-plugin.tar](https://osp-1257653870.cos.ap-guangzhou.myqcloud.com/WeDatasphere/Linkis/engineconn-plugin/1.3.1-engineconn-plugin.tar)|\\n|1.3.0|jdbc<br/>pipeline<br/>io_file<br/>flink<br/>openlookeng<br/>sqoop<br/>presto<br/>elasticsearch<br/>|[1.3.0-engineconn-plugin.tar](https://osp-1257653870.cos.ap-guangzhou.myqcloud.com/WeDatasphere/Linkis/engineconn-plugin/1.3.0-engineconn-plugin.tar)|\\n|1.2.0|jdbc<br/>pipeline<br/>flink<br/>openlookeng<br/>sqoop<br/>presto<br/>elasticsearch<br/>|[1.2.0-engineconn-plugin.tar](https://osp-1257653870.cos.ap-guangzhou.myqcloud.com/WeDatasphere/Linkis/engineconn-plugin/1.2.0-engineconn-plugin.tar)|\\n|1.1.3|jdbc<br/>pipeline<br/>flink<br/>openlookeng<br/>sqoop|[1.1.3-engineconn-plugin.tar](https://osp-1257653870.cos.ap-guangzhou.myqcloud.com/WeDatasphere/Linkis/engineconn-plugin/1.1.3-engineconn-plugin.tar)|\\n|1.1.2|jdbc<br/>pipeline<br/>flink<br/>openlookeng<br/>sqoop|[1.1.2-engineconn-plugin.tar](https://osp-1257653870.cos.ap-guangzhou.myqcloud.com/WeDatasphere/Linkis/engineconn-plugin/1.1.2-engineconn-plugin.tar)|\\n|1.1.1|jdbc<br/>pipeline<br/>flink<br/>openlookeng<br/>|[1.1.1-engineconn-plugin.tar](https://osp-1257653870.cos.ap-guangzhou.myqcloud.com/WeDatasphere/Linkis/engineconn-plugin/1.1.1-engineconn-plugin.tar)|\\n|1.1.0|jdbc<br/>pipeline<br/>flink<br/>|[1.1.0-engineconn-plugin.tar](https://osp-1257653870.cos.ap-guangzhou.myqcloud.com/WeDatasphere/Linkis/engineconn-plugin/1.1.0-engineconn-plugin.tar)|\\n|1.0.3|jdbc<br/>pipeline<br/>flink<br/>|[1.0.3-engineconn-plugin.tar](https://osp-1257653870.cos.ap-guangzhou.myqcloud.com/WeDatasphere/Linkis/engineconn-plugin/1.0.3-engineconn-plugin.tar)|\\n\\n## engine type\\n\\n| **Engine name** | **Support underlying component version<br/>(default dependency version)** | **Linkis Version Requirements** | **Included in Release Package By Default** | **Description** |\\n|:---- |:---- |:---- |:---- |:---- |\\n|Spark|Apache 2.0.0~2.4.7, <br/>CDH >= 5.4.0, <br/>(default Apache Spark 2.4.3)|\\\\>=1.0.3|Yes|Spark EngineConn, supports SQL , Scala, Pyspark and R code|\\n|Hive|Apache >= 1.0.0, <br/>CDH >= 5.4.0, <br/>(default Apache Hive 2.3.3)|\\\\>=1.0.3|Yes|Hive EngineConn, supports HiveQL code|\\n|Python|Python >= 2.6, <br/>(default Python2*)|\\\\>=1.0.3|Yes|Python EngineConn, supports python code|\\n|Shell|Bash >= 2.0|\\\\>=1.0.3|Yes|Shell EngineConn, supports Bash shell code|\\n|JDBC|MySQL >= 5.0, Hive >=1.2.1, <br/>(default Hive-jdbc 2.3.4)|\\\\>=1.0.3|No |JDBC EngineConn, already supports Mysql,Oracle,KingBase,PostgreSQL,SqlServer,DB2,Greenplum,DM,Doris,ClickHouse,TiDB,Starrocks,GaussDB and OceanBase, can be extended quickly Support other engines with JDBC Driver package, such as SQLite|\\n|Flink |Flink >= 1.12.2, <br/>(default Apache Flink 1.12.2)|\\\\>=1.0.2|No |Flink EngineConn, supports FlinkSQL code, also supports starting a new Yarn in the form of Flink Jar Application|\\n|Pipeline|-|\\\\>=1.0.2|No|Pipeline EngineConn, supports file import and export|\\n|openLooKeng|openLooKeng >= 1.5.0, <br/>(default openLookEng 1.5.0)|\\\\>=1.1.1|No|openLooKeng EngineConn, supports querying data virtualization engine with Sql openLooKeng|\\n|Sqoop| Sqoop >= 1.4.6, <br/>(default Apache Sqoop 1.4.6)|\\\\>=1.1.2|No|Sqoop EngineConn, support data migration tool Sqoop engine|\\n|Presto|Presto >= 0.180|\\\\>=1.2.0|No|Presto EngineConn, supports Presto SQL code|\\n|ElasticSearch|ElasticSearch >=6.0|\\\\>=1.2.0|No|ElasticSearch EngineConn, supports SQL and DSL code|\\n|Trino | Trino >=371 | >=1.3.1 | No |   Trino EngineConn\uff0c supports Trino SQL code |\\n|Seatunnel |Seatunnel >=2.1.2 | >=1.3.1 | No | Seatunnel EngineConn\uff0c supportt Seatunnel SQL code |\\n\\n## Install engine guide\\n\\nAfter downloading the material package of the engine, unzip the package\\n```html\\ntar -xvf 1.0.3-engineconn-plugin.tar\\ncd 1.0.3-engineconn-plugin\\n\\n````\\n\\nCopy the engine material package to be used to the engine plug-in directory of linkis, and then refresh the engine material.\\n\\nFor the detailed process, refer to [Installing the EngineConnPlugin Engine](https://linkis.apache.org/zh-CN/docs/latest/deployment/install-engineconn)."},{"id":"/2022/03/20/openlookeng","metadata":{"permalink":"/blog/2022/03/20/openlookeng","editUrl":"https://github.com/apache/linkis-website/edit/dev/blog/2022-03-20-openlookeng.md","source":"@site/blog/2022-03-20-openlookeng.md","title":"Implementation of OpenLookEng Engine","description":"Overview","date":"2022-03-20T00:00:00.000Z","formattedDate":"March 20, 2022","tags":[{"label":"engine","permalink":"/blog/tags/engine"}],"readingTime":2.47,"hasTruncateMarker":false,"authors":[{"name":"Peacewong","title":"Development Engineer of WeBank","url":"https://github.com/peacewong/","imageURL":"https://avatars.githubusercontent.com/u/11496700?v=4","key":"peacewong"}],"frontMatter":{"title":"Implementation of OpenLookEng Engine","authors":["peacewong"],"tags":["engine"]},"prevItem":{"title":"How to Download Engine Plugins Not Included in the Installation Package By Default","permalink":"/blog/2022/04/15/how-to-download-engineconn-plugin"},"nextItem":{"title":"Linkis Deployment Troubleshooting","permalink":"/blog/2022/02/21/linkis-deploy"}},"content":"## Overview\\nopenLooKeng is an \\"out of the box\\" engine that supports in-situ analysis of any data, anywhere, including geographically remote data sources. It provides a global view of all data through a SQL 2003 interface. openLooKeng features high availability, auto-scaling, built-in caching and indexing support, providing the reliability needed for enterprise workloads.\\n\\nopenLooKeng is used to support data exploration, ad hoc query and batch processing with near real-time latency of 100+ milliseconds to minutes without moving data. openLooKeng also supports hierarchical deployment, enabling geographically remote openLooKeng clusters to participate in the same query. With its cross-region query plan optimization capabilities, queries involving remote data can achieve near \\"local\\" performance.\\nLinkis implements the openLooKeng engine to enable Linkis to have the ability to virtualize data and support the submission of cross-source heterogeneous queries, cross-domain and cross-DC query tasks. As a computing middleware, Linkis can connect more low-level computing and storage components by using openLooKeng\'s connector based on the connectivity capability of Linkis\' EngineConn.\\n\\n## Development implementation\\nThe implementation of openLooKeng ec is extended based on the EngineConn Plugin (ECP) of Linkis. Because the OpengLooKeng service supports multiple users to query through the Client, the implementation mode is the implementation mode of the multi-user concurrent engine.\\nThat is, tasks submitted by multiple users can run in one EC process at the same time, which can greatly reuse EC resources and reduce resource waste. The specific class diagram is as follows:\\n\\n\u3010Missing picture\u3011\\n\\nThe specific implementation is that openLooKengEngineConnExecutor inherits from ConcurrentComputationExecutor, supports multi-user multi-task concurrency, and supports docking to multiple different openLooKeng clusters.\\n## Architecture\\nArchitecture diagram:\\n![image](https://user-images.githubusercontent.com/7869972/166736911-c0f50968-3996-40d0-afdf-52b35d4cd71c.png)\\n\\n\\nThe task flow diagram is as follows:\\n  ![image](https://user-images.githubusercontent.com/7869972/166737177-57f8f84a-b16d-44bd-b7cf-a61fc2cc160c.png)\\n\\nThe capabilities based on Linkis and openLooKeng can provide the following capabilities:\\n- 1. The connection capability of the computing middleware layer based on Linkis allows upper-layer application tools to quickly connect to openLooKeng, submit tasks, and obtain logs, progress, and results.\\n- 2. Based on the public service capability of Linkis, it can complete custom variable substitution, UDF management, etc. for openLooKeng\'s sql\\n- 3. Based on the context capability of Linkis, the results of OpengLooKeng can be passed to downstream ECs such as Spark and Hive for query\\n- 4. Linkis-based resource management and multi-tenancy capabilities can isolate tasks from tenants and use openLooKeng resources\\n- 5. Based on OpengLooKeng\'s connector capability, the upper-layer application tool can complete the task of submitting cross-source heterogeneous query, cross-domain and cross-DC query type, and get a second-level return.\\n\\n## Follow-up plans\\nIn the future, the two communities will continue to cooperate and plan to launch the following functions:\\n- 1.Linkis supports openLooKeng on Yarn mode\\n- 2. Linkis has completed the resource management and control of openLooKeng, tasks can now be queued by Linkis, and submitted only when resources are sufficient\\n- 3. Based on the mixed computing ability of openLooKeng, the ability of Linkis Orchestrator is optimized to complete the mixed computing ability between ECs in the subsequent plan."},{"id":"/2022/02/21/linkis-deploy","metadata":{"permalink":"/blog/2022/02/21/linkis-deploy","editUrl":"https://github.com/apache/linkis-website/edit/dev/blog/2022-02-21-linkis-deploy.md","source":"@site/blog/2022-02-21-linkis-deploy.md","title":"Linkis Deployment Troubleshooting","description":"Linkis deployment instructions and precautions","date":"2022-02-21T00:00:00.000Z","formattedDate":"February 21, 2022","tags":[],"readingTime":14.57,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Linkis Deployment Troubleshooting"},"prevItem":{"title":"Implementation of OpenLookEng Engine","permalink":"/blog/2022/03/20/openlookeng"},"nextItem":{"title":"How to Write a Blog","permalink":"/blog/2022/02/08/how-to-user-blog"}},"content":"> Linkis deployment instructions and precautions\\n\\n## 1. Precautions for preparation\\n\\n### 1.1 linux server\\n\\n**Hardware requirements**  \\nInstall nearly 10 linkis microservices. The default configuration of each microservice is to start the jvm-Xmx memory size of 512M (if the memory is not enough, you can try to reduce it to 256/128M, and you can also increase it if the memory is sufficient)\\n\\n**Software Requirements**  \\nbasic software environment\\nUse the following command to check whether the corresponding software has been installed, if not, please install it first\\n```shell\\n#java environment recommended >=1.0.8\\ncommand -v java\\n\\n#yum package management tool, mainly when the linkis web management console installation script is executed, the nginx/policycoreutils-python software will be installed through yum\\ncommand -v yum\\n\\n#When the database of linkis is initialized, it will use the mysql client to connect to the database and execute the sql statement\\ncommand -v mysql\\n\\ncommand -v telnet\\n\\n#Related installation packages for decompressing linkis\\ncommand -v tar\\n\\n#The initial service configuration file linkis-*.properties is replaced by sed in the properties file according to the data configured by `deploy-config/*sh` when the installation script install.sh is executed\\ncommand -v sed\\n\\n````\\n\\n### 1.2 Add deployment user\\n\\nDeployment user: the startup user of the linkis core process, and this user will be granted administrator privileges by default, and the corresponding administrator login password will be generated during the deployment process, which is located in the conf/linkis-mg-gateway.properties file\\n\\nlinkis supports specifying the user who submits and executes. The main process service of linkis will switch to the corresponding execution user through sudo -u ${linkis-user}, and execute the corresponding engine startup command, so the process user of the engine process linkis-engine is the execution owner user belonging to the task\\n\\nTake hadoop user as an example:\\n\\nFirst check whether there is already a hadoop user in the system. If it already exists, you can directly authorize it; if not, create a user first, and then authorize.\\n\\nCheck if a hadoop user already exists\\n```shell script\\n$ id hadoop\\nuid=2001(hadoop) gid=2001(hadoop) groups=2001(hadoop)\\n````\\n\\nIf it does not exist, you need to create a hadoop user and join the hadoop user group\\n```shell script\\n$ sudo useradd hadoop -g hadoop\\n$ vi /etc/sudoers\\n#Add configuration\\nhadoop ALL=(ALL) NOPASSWD: NOPASSWD: ALL\\n````\\n\\nModify the environment variables of the installation user, `vim /home/hadoop/.bash_rc` configure the environment variables, the environment variables are as follows:\\n```shell script\\nexport JAVA_HOME=/opt/jdk1.8\\nexport HADOOP_HOME=/opt/install/hadoop\\nexport HADOOP_CONF_DIR=/etc/conf/hadoop\\nexport HIVE_HOME=/opt/install/hive\\nexport HIVE_CONF_DIR=/etc/conf/conf\\nexport SPARK_HOME=/opt/install/spark\\nexport SPARK_CONF_DIR=/etc/spark/conf\\nexport PYSPARK_ALLOW_INSECURE_GATEWAY=1 # If it is Pyspark, you must add a second parameter\\n````\\n\\nrefresh configuration\\n```shell script\\n$ source /home/hadoop/.bash_rc\\n````\\n\\nCheck if it works\\n```shell script\\n$ sudo -su hadoop\\n$ echo $JAVA_HOME\\n$ echo $HADOOP_HOME\\n````\\n\\n<font color=\'red\'>The following operations are performed under the hadoop user</font>\\n\\n### 1.3 Installation package preparation\\n\\nlinkis installation package, it is recommended to use version 1.X and above\\nThe versions of 0.X and 1.X are quite different. Before 1.0.3, it was the package name of com.webank.wedatasphere.linkis, and linkis>=1.0.3 was the package name of org.apache.linkis.\\n\\n[Download address](https://linkis.apache.org/download/main/): https://linkis.apache.org/download/main/\\n\\n\\n### 1.4 Low-level dependency checking\\n\\nYou can execute the corresponding command to see if it is supported\\n```shell script\\nspark/hive/hdfs/python/\\n$ spark-submit --version //spark tasks will be submitted to YARN for execution through this command\\n$ python --version\\n$ hdfs version\\n$ hive --version\\n````\\n\\n### 1.5 Resource dependencies\\nAccessible mysql database resources Database used to store business data of linkis itself  \\nAccessible yarn resource queues The execution of spark/hive/flink engines requires yarn queue resources  \\nAccessible hive matedata database resources (mysql as an example) Required for hive engine execution  \\n    \\nNote: If the version of hive spark is quite different from the default version, it is best to re-edit the relevant hive/spark version that linkis depends on for compilation\\n\\n\\n\\n## 2. Install\\n### 2.1 Unzip the installation package  \\nAfter uploading the installation package `apache-linkis-1.0.3-incubating-bin.tar.gz`, decompress the installation package  \\n\\n```shell script\\n$ tar -xvf apache-linkis-1.0.3-incubating-bin.tar.gz\\n$ pwd\\n/data/Install/1.0.3\\n````\\n\\nThe unzipped directory structure is as follows\\n```shell script\\n-rw-r--r-- 1 hadoop hadoop 531847342 Feb 21 10:10 apache-linkis-1.0.3-incubating-bin.tar.gz\\ndrwxrwxr-x 2 hadoop hadoop 4096 Feb 21 10:13 bin //Script to perform environment check and install\\ndrwxrwxr-x 2 hadoop hadoop 4096 Feb 21 10:13 deploy-config // Environment configuration information such as DB that depends on deployment\\n-rw-r--r-- 1 hadoop hadoop 1707 Jan 22 2020 DISCLAIMER-WIP\\n-rw-r--r-- 1 hadoop hadoop 66058 Jan 22 2020 LICENSE\\ndrwxrwxr-x 2 hadoop hadoop 16384 Feb 21 10:13 licenses\\ndrwxrwxr-x 7 hadoop hadoop 4096 Feb 21 10:13 linkis-package // The actual package, including lib/service startup script tool/db initialization script/microservice configuration file, etc.\\n-rw-r--r-- 1 hadoop hadoop 83126 Jan 22 2020 NOTICE\\n-rw-r--r-- 1 hadoop hadoop 7900 Jan 22 2020 README_CN.md\\n-rw-r--r-- 1 hadoop hadoop 8184 Jan 22 2020 README.md\\n\\n````\\n\\n\\n### 2.2 Configure linkis database information\\n```shell script\\nvim deploy-config/db.sh\\n\\nExample:\\nMYSQL_HOST=xx.xx.xx.xx\\nMYSQL_PORT=3306\\nMYSQL_DB=linkis_test\\nMYSQL_USER=test\\nMYSQL_PASSWORD=xxxxx\\n\\n````\\n\\n### 2.3 Configure basic environment variables\\n\\nThe file is located at `deploy-config/linkis-env.sh`\\n\\n#### Basic directory configuration\\n> Please confirm that the deployment user deployUser has read and write permissions to these configuration directories\\n\\n```shell script\\ndeployUser=hadoop #The user who executes the deployment, the previously created user hadoop\\n\\nWORKSPACE_USER_ROOT_PATH=file:///tmp/linkis # Specify the directory path used by the user, which is generally used to store the user\'s script files and log files, and is the user\'s workspace. The corresponding configuration file configuration item is wds.linkis.filesystem.root.path(linkis.properties)\\n\\nRESULT_SET_ROOT_PATH=file:///tmp/linkis # Result set log and other file paths, used to store the Job result set file wds.linkis.resultSet.store.path(linkis-cg-entrance.properties) //If not configured, use Configuration of HDFS_USER_ROOT_PATH\\n\\nHDFS_USER_ROOT_PATH=hdfs:///tmp/linkis # Result set log and other file paths, used to store Job result set files wds.linkis.filesystem.hdfs.root.path(linkis.properties)\\n\\nENGINECONN_ROOT_PATH=/appcom/tmp #Store the working path of the execution engine, a local directory with write permissions for the deployment user wds.linkis.engineconn.root.dir(linkis-cg-engineconnmanager.properties)\\n````\\nNote: Confirm whether the deployment user has read and write permissions for the corresponding file directory\\n\\n#### META configuration for HIVE\\n```shell script\\nHIVE_META_URL=jdbc:mysql://127.0.0.1:3306/hive_meta_demo?useUnicode=true&amp;characterEncoding=UTF-8 # URL of HiveMeta meta database\\nHIVE_META_USER=demo # User of HiveMeta Metabase\\nHIVE_META_PASSWORD=demo123 # HiveMeta metabase password\\n````\\n\\n#### Yarn\'s ResourceManager address\\n\\n```shell script\\n#You can confirm whether it is normal by visiting http://xx.xx.xx.xx:8088/ws/v1/cluster/scheduler interface  \\nYARN_RESTFUL_URL=http://xx.xx.xx.xx:8088  \\n````\\nWhen executing spark tasks, you need to use the ResourceManager of yarn. Linkis does not enable permission verification by default. If password permission verification is enabled for ResourceManager, please modify the `linkis_cg_engine_conn_plugin_bml_resources` table data after installation and deployment (or see (#todo))\\n\\n#### LDAP login authentication\\n>Linkis uses static users and passwords by default. Static users are deployment users. Static passwords will randomly generate a password string during deployment and store them in {InstallPath}/conf/linkis-mg-gateway.properties (>=1.0.3 version).\\n```shell script\\n#LDAP configuration, Linkis only supports deployment user login by default, if you need to support multi-user login, you can use LDAP, you need to configure the following parameters\\nnumber:\\n#LDAP_URL=ldap://localhost:1389/\\n#LDAP_BASEDN=dc=webank,dc=com\\n````\\n\\n\\n#### Basic component environment information\\n> It is best to configure it through the user\'s system environment variables (step 1.2 Adding a deployment user has been explained), you can directly comment it out without configuring in the deploy-config/linkis-env.sh configuration file\\n```shell script\\n###HADOOP CONF DIR\\n#HADOOP_CONF_DIR=/appcom/config/hadoop-config\\n###HIVE CONF DIR\\n#HIVE_CONF_DIR=/appcom/config/hive-config\\n###SPARK CONF DIR\\n#SPARK_CONF_DIR=/appcom/config/spark-config\\n````\\n\\n#### Engine version information\\n:::caution\\nIf the official release package used does not need to be modified, if it is compiled by modifying the Spark/Hive engine version, it needs to be modified.\\n:::\\nIf spark is not version 2.4.3, you need to modify the parameters:\\n```shell script\\n## Engine version conf\\n#SPARK_VERSION, If the installed Spark version is not 2.4.3, it needs to be modified to the corresponding version, such as 3.1.1\\nSPARK_VERSION=3.1.1\\n```\\nIf hive is not version 2.3.3, you need to modify the parameters:\\n```shell script\\n## Engine version conf\\n##HIVE_VERSION, If the installed Hive version is not 2.3.3, it needs to be modified to the corresponding version, such as 2.3.4\\nHIVE_VERSION=2.3.4\\n```\\n\\nIf configured, it will actually be updated in the `{linkisInstallPath}/conf/linkis.properties` file after the installation and deployment are performed\\n```shell script\\n#wds.linkis.spark.engine.version=\\n#wds.linkis.hive.engine.version=\\n#wds.linkis.python.engine.version=\\n````\\n\\n#### jvm memory configuration\\n>The microservice starts the jvm memory configuration, which can be adjusted according to the actual situation of the machine. If the machine memory resources are few, you can try to adjust it to 256/128M\\n```shell script\\n## java application default jvm memory\\nexport SERVER_HEAP_SIZE=\\"512M\\"\\n````\\n\\n#### Installation directory configuration\\n\\n>linkis will eventually be installed in this directory, if not configured, the default is the same level directory as the current installation package\\n>\\n```shell script\\n##The decompression directory and the installation directory need to be inconsistent\\nLINKIS_HOME=/appcom/Install/LinkisInstall\\n````\\n\\n## 3. Deployment process\\n\\n### 3.1 Execute the deployment script\\n```shell script\\nsh bin/install.sh\\n````\\n\\ntip: If an error occurs and you are not sure what command to execute to report the error, you can add the -v parameter `sh -v bin/install.sh` to print the shell script execution process log, which is convenient for locating the problem.\\n\\n\\n### 3.2 Possible problems\\n#### 1.Permission problem mkdir: cannot create directory \u2018xxxx\u2019: Permission denied  \\nThe prompt for successful execution is as follows\uff1a        \\nCongratulations! You have installed Linkis 1.0.3 successfully, please use sh /data/Install/linkis/sbin/linkis-start-all.sh to start it!      \\nYour default account password is \\\\[hadoop/5e8e312b4]    \\n\\n### 3.3 Configuration modification\\nAfter the installation is complete, if you need to modify the configuration, you can re-execute the installation, or modify the corresponding ${InstallPath}/conf/*properties file and restart the corresponding service\\n\\n\\n### 3.4 Add mysql driver (>=1.0.3) version\\nBecause of the license, mysql-connector-java is removed from the release package of linkis itself (the family bucket integrated by dss will be included, no need to manually add it), which needs to be added manually.  \\nFor details, see [Add mysql driver package](docs/1.0.3/deployment/quick-deploy#-44-Add mysql driver package)\\n\\n### 3.5 Start the service\\n```shell script\\nsh sbin/linkis-start-all.sh\\n````\\n\\n### 3.6 Check whether the service starts normally\\nVisit the eureka service page (http://eurekaip:20303), version 1.0.x, the following services must be started normally\\n```shell script\\nLINKIS-CG-ENGINECONNMANAGER\\nLINKIS-CG-ENGINEPLUGIN\\nLINKIS-CG-ENTRANCE\\nLINKIS-CG-LINKISMANAGER\xa0\xa0\xa0\\nLINKIS-MG-EUREKA\xa0\\nLINKIS-MG-GATEWAY\\nLINKIS-PS-CS\\nLINKIS-PS-PUBLICSERVICE\\n````\\nIf any services are not started, you can view detailed exception logs in the corresponding log/${service name}.log file.\\n\\n\\n## 4. Install the web frontend\\n>Mainly perform YARN related configuration\\n\\nDownload the front-end installation package and unzip it\\ntar -xvf apache-linkis-1.0.3-incubating-web-bin.tar.gz\\n\\nModify configuration config.sh\\n```shell script\\n#Port for console access http://localhost:8088\\nlinkis_port=\\"8088\\"\\n#linkis-mg-gatewayService Address\\nlinkis_url=\\"http://localhost:9020\\"\\n````\\n\\nPerform front-end deployment\\n\\n```shell script\\nsudo sh install\\n````\\nAfter installation, the nginx configuration file of linkis defaults to /etc/nginx/conf.d/linkis.conf\\nnginx log files are in /var/log/nginx/access.log and /var/log/nginx/error.log\\n````nginx\\n\\n        server {\\n            listen 8188;# access port\\n            server_name localhost;\\n            #charset koi8-r;\\n            #access_log /var/log/nginx/host.access.log main;\\n            location /linkis/visualis {\\n            root /appcom/Install/linkis-web/linkis/visualis; # static file directory\\n            autoindex on;\\n            }\\n            location / {\\n            root /appcom/Install/linkis-web/dist; # static file directory\\n            index index.html index.html;\\n            }\\n            location /ws {\\n            proxy_pass http://localhost:9020;#Address of backend Linkis\\n            proxy_http_version 1.1;\\n            proxy_set_header Upgrade $http_upgrade;\\n            proxy_set_header Connection upgrade;\\n            }\\n\\n            location /api {\\n            proxy_pass http://localhost:9020; #Address of backend Linkis\\n            proxy_set_header Host $host;\\n            proxy_set_header X-Real-IP $remote_addr;\\n            proxy_set_header x_real_ipP $remote_addr;\\n            proxy_set_header remote_addr $remote_addr;\\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\\n            proxy_http_version 1.1;\\n            proxy_connect_timeout 4s;\\n            proxy_read_timeout 600s;\\n            proxy_send_timeout 12s;\\n            proxy_set_header Upgrade $http_upgrade;\\n            proxy_set_header Connection upgrade;\\n            }\\n\\n            #error_page 404 /404.html;\\n            # redirect server error pages to the static page /50x.html\\n            #\\n            error_page 500 502 503 504 /50x.html;\\n            location = /50x.html {\\n            root /usr/share/nginx/html;\\n            }\\n        }\\n````\\n\\nIf you need to modify the port or static resource directory, etc., please modify the /etc/nginx/conf.d/linkis.conf file and execute the `sudo nginx -s reload` command\\n\\nLog in to the web terminal to view information\\nhttp://xx.xx.xx.xx:8188/#/login\\nUsername/Password (check in {InstallPath}/conf/linkis-mg-gateway.properties)\\n\\n\\n```shell script\\n#When not using LDAP configuration\\nwds.linkis.admin.user= #User\\nwds.linkis.admin.password= #Password\\n````\\n\\nAfter logging in, check whether the yarn queue resources can be displayed normally (if you want to use the spark/hive/flink engine)\\nNormally as shown below:    \\n![yarn-normal](https://user-images.githubusercontent.com/7869972/159955494-2f305a38-a3d6-4798-83aa-58cde23bc436.png)\\n\\nIf it cannot be displayed:  \\n#### 1 Check whether the yarn address is configured correctly\\nDatabase table linkis_cg_rm_external_resource_provider\\nInsert yarn data information\\n```sql\\nINSERT INTO `linkis_cg_rm_external_resource_provider`\\n(`resource_type`, `name`, `labels`, `config`) VALUES\\n(\'Yarn\', \'sit\', NULL,\\n\'{\\\\r\\\\n\\"rmWebAddress\\": \\"http://xx.xx.xx.xx:8088\\",\\\\r\\\\n\\"hadoopVersion\\": \\"2.7.2\\",\\\\r\\\\n\\"authorEnable\\":false, \\\\r\\\\n\\"user\\":\\"hadoop\\",\\\\r\\\\n\\"pwd\\":\\"1234\\n56\\"\\\\r\\\\n}\'\\n);\\n\\nconfig field properties\\n\\n\\"rmWebAddress\\": \\"http://xx.xx.xx.xx:8088\\", #need to bring http and port\\n\\"hadoopVersion\\": \\"2.7.2\\",\\n\\"authorEnable\\":true, //Whether authentication is required You can verify the username and password by visiting http://xx.xx.xx.xx:8088 in the browser\\n\\"user\\":\\"user\\",//username\\n\\"pwd\\":\\"pwd\\"//Password\\n\\n````\\n\\nAfter the update, because the cache is used in the program, if you want to take effect immediately, you need to restart the linkis-cg-linkismanager service\\n```shell script\\nsh sbin/linkis-daemon.sh restart cg-linkismanager\\n````\\n\\n\\n#### 2 Check whether the yarn queue is correct  \\nException message: desc: queue ide is not exists in YARN.  \\nThe configuration yarn queue does not exist and needs to be adjusted  \\n\\nModification method: linkis management console/parameter configuration > global settings > yarn queue name [wds.linkis.rm.yarnqueue] Modify a usable yarn queue  \\nAvailable yarn queues can be viewed at rmWebAddress:http://xx.xx.xx.xx:8088\\n\\n## 5. Check if the engine material resource is uploaded successfully\\n```sql\\n#Login to the linkis database\\nselect * from linkis_cg_engine_conn_plugin_bml_resources\\n````\\nnormal as follows  \\n![bml](https://user-images.githubusercontent.com/29391030/156343249-9f6dca8f-4e0d-438b-995f-4f469270a22d.png)\\n\\n\\nCheck whether the material record of the engine exists (if there is an update, check whether the update time is correct).  \\nIf it does not exist or is not updated, first try to manually refresh the material resource (for details, see [Engine Material Resource Refresh](docs/latest/deployment/install-engineconn#23-Engine Refresh)). Check the `log/linkis-cg-engineplugin.log` log to check the specific reasons for the failure of the material. In many cases, it may be caused by the lack of permissions in the hdfs directory. Check whether the gateway address configuration is correct `conf/linkis.properties:wds.linkis.gateway.url`\\n\\nThe material resources of the engine are uploaded to the hdfs directory by default as `/apps-data/${deployUser}/bml`\\n```shell script\\nhdfs dfs -ls /apps-data/hadoop/bml\\n#If there is no such directory, please manually create the directory and grant ${deployUser} read and write permissions\\nhdfs dfs -mkdir /apps-data\\nhdfs dfs -chown hadoop:hadoop/apps-data\\n````\\n\\n##Verify basic functions\\n````\\n#The version number of the engineType of the engine must match the actual one\\n\\nsh bin/linkis-cli -submitUser hadoop -engineType shell-1 -codeType shell -code \\"whoami\\"\\nsh bin/linkis-cli -submitUser hadoop -engineType hive-2.3.3 -codeType hql -code \\"show tables\\"\\nsh bin/linkis-cli -submitUser hadoop -engineType spark-2.4.3 -codeType sql -code \\"show tables\\"\\nsh bin/linkis-cli -submitUser hadoop -engineType python-python2 -codeType python -code \'print(\\"hello, world!\\")\'\\n````\\n\\n\\nView supported versions of each engine\\n\\nMethod 1: View the directory packaged by the engine\\n````\\n$ tree linkis-package/lib/linkis-engineconn-plugins/ -L 3\\nlinkis-package/lib/linkis-engineconn-plugins/\\n\u251c\u2500\u2500 hive\\n\u2502 \u251c\u2500\u2500 dist\\n\u2502 \u2502 \u2514\u2500\u2500 v2.3.3 #version is 2.3.3 engineType is hive-2.3.3\\n\u2502 \u2514\u2500\u2500 plugin\\n\u2502 \u2514\u2500\u2500 2.3.3\\n\u251c\u2500\u2500 python\\n\u2502 \u251c\u2500\u2500 dist\\n\u2502 \u2502 \u2514\u2500\u2500 vpython2\\n\u2502 \u2514\u2500\u2500 plugin\\n\u2502 \u2514\u2500\u2500 python2 #version is python2 engineType is python-python2\\n\u251c\u2500\u2500 shell\\n\u2502 \u251c\u2500\u2500 dist\\n\u2502 \u2502 \u2514\u2500\u2500 v1\\n\u2502 \u2514\u2500\u2500 plugin\\n\u2502 \u2514\u2500\u2500 1\\n\u2514\u2500\u2500 spark\\n    \u251c\u2500\u2500 dist\\n    \u2502 \u2514\u2500\u2500 v2.4.3\\n    \u2514\u2500\u2500 plugin\\n        \u2514\u2500\u2500 2.4.3\\n````\\n\\nMethod 2: View the database table of linkis\\nselect * from linkis_cg_engine_conn_plugin_bml_resources\\n\\n\\n## 6. Troubleshooting installation and deployment common problems\\n\\n### 6.1 Version compatibility issues\\n\\n The engine supported by linkis by default, for compatibility with dss, you can view this document https://github.com/apache/linkis/blob/master/README.md\\n\\n### 6.2 How to locate the server exception log\\n\\n    Linkis has many microservices. If you are unfamiliar with the system, sometimes you cannot locate the specific module that has an exception. You can search through the global log.  \\n    tail -f log/* |grep -5n exception (or tail -f log/* |grep -5n ERROR)  \\n    less log/* |grep -5n exception (or less log/* |grep -5n ERROR)  \\n\\n### 6.3 Execution of abnormal troubleshooting of engine tasks\\n\\n  step1: Find the startup deployment directory of the engine  \\n    Method 1: If it is displayed in the execution log, you can view it on the management console as shown below:     \\n    ![engine-log](https://user-images.githubusercontent.com/29391030/156343802-9d47fa98-dc70-4206-b07f-df439b291028.png)    \\n    Method 2: If it is not found in method 1, you can find the parameter of `wds.linkis.engineconn.root.dir` configured in `conf/linkis-cg-engineconnmanager.properties`, which is the directory where the engine is started and deployed. The user of the execution engine is isolated (taskId). If you do not know the taskid, you can select it after sorting by time. ll -rt /appcom/tmp/${executing user}/workDir\\n    \\n    cd /appcom/tmp/${executing user}/workDir/${taskId}  \\n    # The directory is roughly as follows  \\n    conf -> /appcom/tmp/engineConnPublickDir/6a09d5fb-81dd-41af-a58b-9cb5d5d81b5a/v000002/conf #engine configuration file  \\n    engineConnExec.sh #Generated engine startup script  \\n    lib -> /appcom/tmp/engineConnPublickDir/45bf0e6b-0fa5-47da-9532-c2a9f3ec764d/v000003/lib #Engine dependent packages  \\n    logs #Engine startup and execution related logs  \\n\\n  step2: View the log of the engine  \\n    less logs/stdout\\n\\n  step3: try to execute the script manually (if needed)  \\n   Debugging can be done by trying to execute the script manually  \\n   sh engineConnExec.sh  \\n\\n### 6.4 Notes on CDH adaptation version\\n\\n  CDH itself is not an official standard hive/spark package. When adapting, it is best to modify the hive/spark version dependencies in the source code of linkis to recompile and deploy.\\n  For details, please refer to the CDH adaptation blog post  \\n    [[Linkis1.0 - Installation and Stepping in the CDH5 Environment]](https://mp.weixin.qq.com/s/__QxC1NoLQFwme1yljy-Nw)  \\n    [[DSS1.0.0+Linkis1.0.2\u2014\u2014Trial record in CDH5 environment]](https://mp.weixin.qq.com/s/9Pl9P0hizDWbbTBf1yzGJA)  \\n    [[DSS1.0.0 and Linkis1.0.2\u2014\u2014Summary of JDBC engine related issues]](https://mp.weixin.qq.com/s/vcFge4BNiEuW-7OC3P-yaw)  \\n    [[DSS1.0.0 and Linkis1.0.2\u2014\u2014Summary of Flink engine related issues]](https://mp.weixin.qq.com/s/VxZ16IPMd1CvcrvHFuU4RQ)  \\n\\n### 6.6 Debugging of Http interface\\n\\n  Method 1 can open the [Login-Free Mode Guide] (docs/latest/api/login-api#2 Login-Free Configuration)  \\n  Method 2: Add a static Token to the http request header\\n  ```shell script\\n  Token-User:hadoop\\n  Token-Code: BML-AUTH\\n````\\n\\n### 6.7 Troubleshooting process for abnormal problems\\n\\n  First, follow the above steps to check whether the service/environment, etc. are all started normally  \\n  Troubleshoot basic problems according to some of the scenarios listed above  \\n  [QA documentation](https://docs.qq.com/doc/DSGZhdnpMV3lTUUxq) Find out if there is a solution, link: https://docs.qq.com/doc/DSGZhdnpMV3lTUUxq  \\n  See if you can find a solution by searching the content in the issue    \\n  ![issues](https://user-images.githubusercontent.com/29391030/156343419-81cc25e0-aa94-4c06-871c-bb036eb6d4ff.png)   \\n  Through the official website document search, for some problems, you can search for keywords through the official website, such as searching for \\"deployment\\". (If 404 appears,   please refresh your browser)     \\n  ![search](https://user-images.githubusercontent.com/29391030/156343459-7911bd05-4d8d-4a7b-b9f8-35c152d52c41.png)     \\n\\n\\n## 7. How to obtain relevant information\\n\\nLinkis official website documents are constantly improving, you can view/keyword search related documents on this official website.\\n\\nRelated blog post links  \\n- Linkis technical blog collection https://github.com/apache/linkis/issues/1233  \\n- Official account technical blog post https://mp.weixin.qq.com/mp/homepage?__biz=MzI4MDkxNzUxMg==&hid=1&sn=088cbf2bbed1c80d003c5865bc92ace8&scene=18  \\n- Official website documentation https://linkis.apache.org/docs/latest/about/introduction/  \\n- bilibili technology sharing video https://space.bilibili.com/598542776?spm_id_from=333.788.b_765f7570696e666f.2"},{"id":"/2022/02/08/how-to-user-blog","metadata":{"permalink":"/blog/2022/02/08/how-to-user-blog","editUrl":"https://github.com/apache/linkis-website/edit/dev/blog/2022-02-08-how-to-user-blog.md","source":"@site/blog/2022-02-08-how-to-user-blog.md","title":"How to Write a Blog","description":"This article mainly guides you how to publish blog posts on the Linkis official website. You are welcome to submit blog post documents about Apache Linkis, including but not limited to Linkis installation/source code analysis/architecture/experience sharing.","date":"2022-02-08T00:00:00.000Z","formattedDate":"February 8, 2022","tags":[{"label":"blog","permalink":"/blog/tags/blog"},{"label":"guide","permalink":"/blog/tags/guide"}],"readingTime":3.94,"hasTruncateMarker":true,"authors":[{"name":"Casion","title":"Development Engineer of WeBank","url":"https://github.com/casionone/","imageURL":"https://avatars.githubusercontent.com/u/7869972?v=4","key":"Casion"}],"frontMatter":{"title":"How to Write a Blog","authors":["Casion"],"tags":["blog","guide"]},"prevItem":{"title":"Linkis Deployment Troubleshooting","permalink":"/blog/2022/02/21/linkis-deploy"}},"content":"> _This article mainly guides you how to publish blog posts on the Linkis official website. You are welcome to submit blog post documents about Apache Linkis, including but not limited to Linkis installation/source code analysis/architecture/experience sharing. _\\n\\nThis article mainly refers to Docusaurus\' official [blog post specifications and examples] (https://docusaurus.io/zh-CN/blog). The guidelines and specifications may not be perfect. Any comments or suggestions are welcome.\\n\\n\x3c!--truncate--\x3e\\n## Resource path\\n\\n- Chinese blog post warehouse path: https://github.com/apache/linkis-website/tree/dev/i18n/zh-CN/docusaurus-plugin-content-blog\\n- English blog post warehouse path: https://github.com/apache/linkis-website/tree/dev/blog\\n\\nEach blog post needs to support both Chinese and English. Please do not omit the corresponding English documents when submitting.\\n\\n## File naming\\nThe framework will automatically parse the release date in YYYY-MM-DD format from the directory/file name\\n- eg: blog/2021-02-08-how-to-user-blog.md\\n- The access path of http is: http://xxxxx/blog/2021/02/08/how-to-user-blog\\n\\nThe blog post release log will automatically parse the corresponding date according to the file: 2021-02-08, so the default is to sort by date.\\n\\n## Graphic blog specification\\n- \u26a0 If the blog post involves image resources and needs to load local image resources, please use the form of a folder, so that the resources such as images required by the blog can be conveniently put together with the Markdown document.\\n- \u26a0 If it is a picture of the architecture/process, the original project file, such as .vsdx file, please upload it to the img directory for subsequent modification. Please ensure that no Chinese pictures appear in English blog posts.\\n\\n```shell script\\n|-- blog\\n| |--2021-02-08-how-to-user-blog\\n| | |-- img //Store the image\\n| | |-- index.md //Blog content\\n````\\n\\nReference example:\\n- Source: https://github.com/facebook/docusaurus/tree/main/website/blog/2022-01-24-docusaurus-2021-recap\\n- Visual Effects: https://docusaurus.io/zh-CN/blog/2022/01/24/docusaurus-2021-recap\\n\\n## Summary\\nThe blog\'s home page ( /blog by default) is the blog list page, which displays all blog posts.\\n\\nUse \x3c!--truncate--\x3e in blog posts to mark article abstracts. \x3c!--truncate--\x3e The above content will become a summary and will be displayed on the blog homepage.\\nfor example:\\n````markdown\\n---\\ntitle: Abstract example\\n---\\n\\nAll these will be part of the blog post summary.\\n\\nEven include this line.\\n\\n\x3c!--truncate--\x3e\\n\\nBut this line and the content below this line will not be truncated.\\n\\nThis line will not.\\n\\nNeither will this line.\\n````\\n\\n## Metadata information\\n\\nMarkdown documents can use the following Markdown frontend metadata fields, enclosed by lines on either side of ---.\\n\\n````markdown\\n---\\ntitle: Welcome Docusaurus v2\\ndescription: This is my first post on Docusaurus 2.\\ndata:2022-02-01\\nslug: welcome-docusaurus-v2\\nauthors:\\n  - name: Joel Marcey\\n    title: Co-creator of Docusaurus 1\\n    url: https://github.com/JoelMarcey\\n    image_url: https://github.com/JoelMarcey.png\\n  - name: S\xe9bastien Lorber\\n    title: Docusaurus maintainer\\n    url: https://sebastienlorber.com\\n    image_url: https://github.com/slorber.png\\ntags: [hello, docusaurus-v2]\\nimage: https://i.imgur.com/mErPwqL.png\\nhide_table_of_contents: false\\n---\\n\\nWelcome to this blog. This blog is built using [**Docusaurus 2**](https://docusaurus.io/).\\n\\n\x3c!--truncate--\x3e\\n\\nThis is my first blog post.\\n\\nBelow is a list of content.\\n````\\nCommon parameters\\n\\n| Name | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| `authors` | `Authors` | `undefined` | List of blog post authors (or unique author).|\\n| `authors.url` | `string` | `undefined` | The URL that the author\'s name will be linked to. This could be a GitHub, Twitter, Facebook profile URL, etc. |\\n| `authors.image_url` | `string` | `undefined` | The URL to the author\'s thumbnail image. |\\n| `authors.title` | `string` | `undefined` | A description of the author. |\\n| `title` | `string` | Markdown title | The blog post title. |\\n| `date` | `string` | File name or file creation time | The blog post creation date. If not specified, this can be extracted from the file or folder name, eg, `2021-04-15-blog-post. mdx`, `2021-04-15-blog-post/index.mdx`, `2021/04/15/blog-post.mdx`. Otherwise, it is the Markdown file creation time. |\\n| `tags` | `Tag[]` | `undefined` | A list of strings or objects of two string fields `label` and `permalink` to tag to your post. |\\n| `keywords` | `string[]` | `undefined` | Keywords meta tag, which will become the `<meta name=\\"keywords\\" content=\\"keyword1,keyword2,...\\"/>` in `<head> `, used by search engines. |\\n| `description` | `string` | The first line of Markdown content | The description of your document, which will become the `<meta name=\\"description\\" content=\\"...\\"/>` and `<meta property= \\"og:description\\" content=\\"...\\"/>` in `<head>`, used by search engines. |\\n| `image` | `string` | `undefined` | Cover or thumbnail image that will be used when displaying the link to your post. |\\n| `slug` | `string` | File path | Allows to customize the blog post url (`/<routeBasePath>/<slug>`). Support multiple patterns: `slug: my-blog-post`, `slug: / my/path/to/blog/post`, slug: `/`. |\\n\\n\\n## Author information\\n\\nFor the average blog post author, maintaining author information inline in each blog post can be tedious.\\nCan be globally in the config file declare these authors:\\n`blog/authors.yml`\\n````yaml\\nCasion:\\n   name: Casion\\n   title: Development Engineer of WeBank\\n   url: https://github.com/casionone/\\n   image_url: https://avatars.githubusercontent.com/u/7869972?v=4\\n````"}]}')}}]);