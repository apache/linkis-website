"use strict";(self.webpackChunklinkis_web_apache=self.webpackChunklinkis_web_apache||[]).push([[10756],{3905:(e,n,t)=>{t.d(n,{Zo:()=>d,kt:()=>h});var i=t(67294);function o(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function a(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);n&&(i=i.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,i)}return t}function r(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?a(Object(t),!0).forEach((function(n){o(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):a(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,i,o=function(e,n){if(null==e)return{};var t,i,o={},a=Object.keys(e);for(i=0;i<a.length;i++)t=a[i],n.indexOf(t)>=0||(o[t]=e[t]);return o}(e,n);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(i=0;i<a.length;i++)t=a[i],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(o[t]=e[t])}return o}var l=i.createContext({}),p=function(e){var n=i.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):r(r({},n),e)),t},d=function(e){var n=p(e.components);return i.createElement(l.Provider,{value:n},e.children)},c="mdxType",u={inlineCode:"code",wrapper:function(e){var n=e.children;return i.createElement(i.Fragment,{},n)}},m=i.forwardRef((function(e,n){var t=e.components,o=e.mdxType,a=e.originalType,l=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),c=p(t),m=o,h=c["".concat(l,".").concat(m)]||c[m]||u[m]||a;return t?i.createElement(h,r(r({ref:n},d),{},{components:t})):i.createElement(h,r({ref:n},d))}));function h(e,n){var t=arguments,o=n&&n.mdxType;if("string"==typeof e||o){var a=t.length,r=new Array(a);r[0]=m;var s={};for(var l in n)hasOwnProperty.call(n,l)&&(s[l]=n[l]);s.originalType=e,s[c]="string"==typeof e?e:o,r[1]=s;for(var p=2;p<a;p++)r[p]=t[p];return i.createElement.apply(null,r)}return i.createElement.apply(null,t)}m.displayName="MDXCreateElement"},83053:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>a,metadata:()=>s,toc:()=>p});var i=t(87462),o=(t(67294),t(3905));const a={title:"Linkis 1.3.0 adapts to Huawei MRS+SCRIPTIS actual combat sharing",authors:["livi12138"],tags:["blog","linki1.3.0","hadoop3.1.1","spark3.0.1","hive3.1.0"]},r=void 0,s={permalink:"/blog/2024/01/26/linkis130-adaptation-Huawei-MRS-share",editUrl:"https://github.com/apache/linkis-website/edit/dev/blog/2024-01-26-linkis130-adaptation-Huawei-MRS-share.md",source:"@site/blog/2024-01-26-linkis130-adaptation-Huawei-MRS-share.md",title:"Linkis 1.3.0 adapts to Huawei MRS+SCRIPTIS actual combat sharing",description:"Overview",date:"2024-01-26T00:00:00.000Z",formattedDate:"January 26, 2024",tags:[{label:"blog",permalink:"/blog/tags/blog"},{label:"linki1.3.0",permalink:"/blog/tags/linki-1-3-0"},{label:"hadoop3.1.1",permalink:"/blog/tags/hadoop-3-1-1"},{label:"spark3.0.1",permalink:"/blog/tags/spark-3-0-1"},{label:"hive3.1.0",permalink:"/blog/tags/hive-3-1-0"}],readingTime:7.52,hasTruncateMarker:!1,authors:[{name:"livi12138",title:"contributors",url:"https://github.com/livi12138",imageURL:"https://avatars.githubusercontent.com/u/156271765?v=4",key:"livi12138"}],frontMatter:{title:"Linkis 1.3.0 adapts to Huawei MRS+SCRIPTIS actual combat sharing",authors:["livi12138"],tags:["blog","linki1.3.0","hadoop3.1.1","spark3.0.1","hive3.1.0"]},nextItem:{title:"Linkis 1.3.2 Integration with OceanBase",permalink:"/blog/2023/03/08/linkis-integration-with-oceanbase"}},l={authorsImageUrls:[void 0]},p=[{value:"Overview",id:"overview",level:2},{value:"environment and version",id:"environment-and-version",level:2},{value:"dependence adjustment and packaging",id:"dependence-adjustment-and-packaging",level:2},{value:"Linkis outermost adjustment pom file",id:"linkis-outermost-adjustment-pom-file",level:4},{value:"linkis-engineplugin-hive\u7684pom\u6587\u4ef6",id:"linkis-engineplugin-hive\u7684pom\u6587\u4ef6",level:4},{value:"linkis-engineplugin-spark\u7684pom\u6587\u4ef6",id:"linkis-engineplugin-spark\u7684pom\u6587\u4ef6",level:4},{value:"linkis-hadoop-common\u7684pom\u6587\u4ef6",id:"linkis-hadoop-common\u7684pom\u6587\u4ef6",level:4},{value:"Linkis-Label-Common",id:"linkis-label-common",level:4},{value:"Linkis-computation-Governance-Common",id:"linkis-computation-governance-common",level:4},{value:"Compilation",id:"compilation",level:4},{value:"Compile Error",id:"compile-error",level:4},{value:"DatasphereStudio pom file",id:"dataspherestudio-pom-file",level:4},{value:"deployment",id:"deployment",level:2},{value:"linkis deployment",id:"linkis-deployment",level:3},{value:"Linkis Deployment Points Attention Point",id:"linkis-deployment-points-attention-point",level:4},{value:"Install web front end",id:"install-web-front-end",level:3},{value:"scriptis tool installation",id:"scriptis-tool-installation",level:3},{value:"Nginx deployment for example",id:"nginx-deployment-for-example",level:2},{value:"nginx.conf",id:"nginxconf",level:4},{value:"How to check the question",id:"how-to-check-the-question",level:2}],d={toc:p},c="wrapper";function u(e){let{components:n,...t}=e;return(0,o.kt)(c,(0,i.Z)({},d,t,{components:n,mdxType:"MDXLayout"}),(0,o.kt)("h2",{id:"overview"},"Overview"),(0,o.kt)("p",null,"  The team needs to use SQL and Python syntax to analyze the data at the same time on the page. During the investigation, I found that Linkis can meet the needs. As a Huawei MRS is used, it is different from the open source software.\nIt also carried out secondary development and adaptation. This article will share the experience, hoping to help students in need."),(0,o.kt)("h2",{id:"environment-and-version"},"environment and version"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"JDK-1.8.0_112, Maven-3.5.2"),(0,o.kt)("li",{parentName:"ul"},"Hadoop-3.1.1, spark-3.1.1, hive-3.1.0, zookerper-3.5.9 (Huawei MRS version)"),(0,o.kt)("li",{parentName:"ul"},"Linkis-1.3.0"),(0,o.kt)("li",{parentName:"ul"},"Scriptis-Web 1.1.0")),(0,o.kt)("h2",{id:"dependence-adjustment-and-packaging"},"dependence adjustment and packaging"),(0,o.kt)("p",null,"   First download the source code of 1.3.0 from the Linkis official website, and then adjust the dependent version"),(0,o.kt)("h4",{id:"linkis-outermost-adjustment-pom-file"},"Linkis outermost adjustment pom file"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-xml"},"<hadoop.version>3.1.1</hadoop.version>\n<zookerper.version>3.5.9</zookerper.version>\n<curaor.version>4.2.0</curaor.version>\n<guava.version>30.0-jre</guava.version>\n<json4s.version>3.7.0-M5</json4s.version>\n<scala.version>2.12.15</scala.version>\n<scala.binary.version>2.12</scala.binary.version>\n")),(0,o.kt)("h4",{id:"linkis-engineplugin-hive\u7684pom\u6587\u4ef6"},"linkis-engineplugin-hive\u7684pom\u6587\u4ef6"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-xml"},"<hive.version>3.1.2</hive.version>\n")),(0,o.kt)("h4",{id:"linkis-engineplugin-spark\u7684pom\u6587\u4ef6"},"linkis-engineplugin-spark\u7684pom\u6587\u4ef6"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-xml"},"<spark.version>3.1.1</spark.version>\n")),(0,o.kt)("h4",{id:"linkis-hadoop-common\u7684pom\u6587\u4ef6"},"linkis-hadoop-common\u7684pom\u6587\u4ef6"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-xml"},"<dependency>\n        <groupId>org.apache.hadoop</groupId>\n        <artifactId>hadoop-hdfs</artifactId>  <!-Just replace the line and replace it with <Arttifactid> Hadoop-HDFS-Client </Artifactid>->->\n        <version>${hadoop.version}</version>\n</dependency>\nModify the Hadoop-HDFS to:\n <dependency>\n        <cepid> org.apache.hadoop </groupid>\n        <Artifactid> Hadoop-HDFS-Client </Artifactid>\n        <Version> $ {Hadoop.Version} </version>\n</dependency>\n")),(0,o.kt)("h4",{id:"linkis-label-common"},"Linkis-Label-Common"),(0,o.kt)("p",null,"org.apache.linkis.manager.label.conf.labelcommonconfig\nModify the default version, which is convenient for subsequent self -compiling scheduling components"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},'    Public Static Final Commonvars <string> Spark_engine_Version =\n            Commonvars.apply ("wds.linkis.spark.engine.version", "3.1.1");\n\n    Public Static Final Commonvars <string> Hive_engine_Version =\n            Commonvars.apply ("wds.linkis.hive.engine.version", "3.1.2");\n')),(0,o.kt)("h4",{id:"linkis-computation-governance-common"},"Linkis-computation-Governance-Common"),(0,o.kt)("p",null,"org.apache.linkis.governance.Common.conf.governanceCommonConf\nModify the default version, which is convenient for subsequent self -compiling scheduling components"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},'  Val spark_engine_version = Commonvars ("wds.linkis.spark.engine.version", "3.1.1")\n\n  VAL HIVE_ENGINE_VERSION = Commonvars ("wds.linkis.hive.engine.version", "3.1.2")\n')),(0,o.kt)("h4",{id:"compilation"},"Compilation"),(0,o.kt)("p",null,"After the above configuration is adjusted, you can start compiling full amount, and execute the following commands in turn"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"    cd linkis-x.x.x\n    MVN -N Install\n    MVN CLEAN Install -DSKIPTESTS\n")),(0,o.kt)("h4",{id:"compile-error"},"Compile Error"),(0,o.kt)("p",null,"-If when you compile it, there is an error, try to enter a module alone to compile, see if there are errors, and adjust according to specific errors\n-Since the SCALA language is used in Linkis for code writing, it is recommended that you can configure the SCALA environment first to facilitate reading the source code\n-Aar package conflict is the most common problem, especially after upgrading Hadoop, please adjust the dependent version patiently"),(0,o.kt)("h4",{id:"dataspherestudio-pom-file"},"DatasphereStudio pom file"),(0,o.kt)("p",null,"As we upgrade the version of Scala, the error will be reported when deploying.\nConn to BML Now Exit Java.net.socketexception: Connection Reset. Here you need to modify the SCALA version and re -compile."),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Delete the low version of the DSS-Gateway-SUPPPPORT JAR package,"),(0,o.kt)("li",{parentName:"ol"},"Modify the scala version in DSS 1.1.0 to 2.12, compile it, get the new DSS-Gateway-SUPPPORT -.1.0.JAR, replace the linkis_installhome/lib/linkis-spaint-service/linkis-mg-gateway The original jar package of the Central Plains")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-xml"},"<!-The SCALA environment is consistent->\n<scala.version> 2.12.15 </scala.version>\n")),(0,o.kt)("p",null,"According to the adjustment of the dependent version above, you can solve most of the problems. If you have any problems, you need to carefully adjust the corresponding log.\nIf a complete package can be compiled, it represents the full compilation of Linkis and can be deployed."),(0,o.kt)("h2",{id:"deployment"},"deployment"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"In in order to allow the engine node to have sufficient resource execution script, we have adopted multiple server deployments, and the general deployment structure is as follows.\n-SLB 1 load balancing is rotary inquiry\n-E ECS-WEB 2 Nginx, static resource deployment, background agent forwarding\n-ECS-APP 2 micro-service governance, computing governance, public enhancement and other node deployment\n-ECS-APP 4 Engineconnmanager node deployment")),(0,o.kt)("h3",{id:"linkis-deployment"},"linkis deployment"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"At the use of multiple node deployments, we did not peel the code, or put the full amount on the server, but just modified the startup script to make it only start the service required")),(0,o.kt)("p",null,"Refer to the official website single machine deployment example: https: //linkis.apache.org/zh-docs/1.3.0/dePlayment/dePlay-qick"),(0,o.kt)("h4",{id:"linkis-deployment-points-attention-point"},"Linkis Deployment Points Attention Point"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("ol",{parentName:"li"},(0,o.kt)("li",{parentName:"ol"},"Deployment user: The startup user of the core process of Linkis. At the same time, this user will default as an administrator permissions. During the deployment process, the corresponding administrator login password, located in the linkis support specified in CONF/LINKIS-MG-Gateway.properties file file Submitted and executed users. The main process service of Linkis will be switched to the corresponding user through the SUDO -U $ {linkis-user}, and then executes the corresponding engine startup command, so the user of the engine linkis -ngine processes is the executor of the task.\n-The user defaults to the submission and executor of the task, if you want to change to the login user, you need to modify\norg.apache.linkis.entRance.Restful.entRANCERESTFAPI class\njson.put (taskConstant.execute_user, moduleuseuserutills.GetOperationUser (REQ));\njson.put (taskConstant.submit_user, SecurityFilter.getLoginusername (REQ));\nChange the above settings to the user and execute user to the Scriptis page to log in to the user"))),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("ol",{parentName:"li",start:2},(0,o.kt)("li",{parentName:"ol"},"Sudo -U $ {linkis -user} Switch to the corresponding user. If you use the login user, this command may fail, and you need to modify the command here."))),(0,o.kt)("li",{parentName:"ul"},"org.apache.linkis.ecm.server.operator.EngineConnYarnLogOperator.sudoCommands")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-scala"},'private def sudoCommands(creator: String, command: String): Array[String] = {\n    Array(\n      "/bin/bash",\n      "-c",\n      "sudo su " + creator + " -c \\"source ~/.bashrc 2>/dev/null; " + command + "\\""\n    )\n  } change into\n  private def sudoCommands(creator: String, command: String): Array[String] = {\n    Array(\n      "/bin/bash",\n      "-c",\n      "\\"source ~/.bashrc 2>/dev/null; " + command + "\\""\n    )\n  }\n')),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("ol",{parentName:"li",start:3},(0,o.kt)("li",{parentName:"ol"},"Mysql's driver package must be Copy to/lib/linkis-commons/public-module/and/lib/linkis-spring-cloud-services/linkis-mg-gateway/"))),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("ol",{parentName:"li",start:4},(0,o.kt)("li",{parentName:"ol"},"The default is to use static users and passwords. Static users are deploying users. Static passwords will generate a password string in execution deployment, stored at $ {linkis_home} /conf/linkis-mg-gateway.properties"))),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("ol",{parentName:"li",start:5},(0,o.kt)("li",{parentName:"ol"},"database script execution, linkis itself needs to use the database, but when we execute the script of the inserted data of Linkis 1.3.0, we found an error. We directly deleted the data of the error part at that time."))),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("ol",{parentName:"li",start:6},(0,o.kt)("li",{parentName:"ol"},"Yarn's certification. When performing the spark task, the task will be submitted to the queue. The resource information of the queue will be obtained first to determine whether there is a resource to submit.\nFor certification, if the file authentication is enabled, the file needs to be placed in the corresponding directory of the server, and the information is updated in the linkis_cg_rm_extRNAL_Resource_Provider library table.")))),(0,o.kt)("h3",{id:"install-web-front-end"},"Install web front end"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"WEB side uses nginx as a static resource server, download the front -end installation package and decompress it, and place it on the directory corresponding to the Nginx server")),(0,o.kt)("h3",{id:"scriptis-tool-installation"},"scriptis tool installation"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Scriptis is a pure front -end project. As a component integrates in the web code component of DSS, we only need to compile the DSSWEB project for separate Scriptis modules, upload the compiled static resources to Visit, note: Linkis stand -by -machine deployment defaults to use session for verification. You need to log in to the Linkis management desk first, and then log in to Scriptis to use.")),(0,o.kt)("h2",{id:"nginx-deployment-for-example"},"Nginx deployment for example"),(0,o.kt)("h4",{id:"nginxconf"},"nginx.conf"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"upstream linkisServer{\n    server ip:port;\n    server ip:port;\n}\nServer {\nListen 8088;# Access port\nServer_name localhost;\n#Charset Koi8-R;\n#access_log /var/log/nginx/host.access.log main;\n#Scriptis static resources\nlocal /scriptis {\n# Modify to your own front path\nalias/home/nginx/scriptis-weight; # static file directory\n#Root/Home/Hadoop/DSS/Web/DSS/Linkis;\nindex index.html index.html;\n}\n#The default resource path points to the static resource of the front end of the management platform\nlocation / {\n# Modify to your own front path\nroot/Home/Nginx/Linkis-Web/DIST; # r r r r\n#Root/Home/Hadoop/DSS/Web/DSS/Linkis;\nindex index.html index.html;\n}\n\nlocal /ws {\nProxy_pass http:// linkisserver/api #back -end linkis address\nproxy_http_version 1.1;\nproxy_set_header upgrade $ http_upgrade;\nproxy_set_header connection upgrade;\n}\n\nlocation /api {\nproxy_pass http:// linkisserver/api; #The address of the back end linkis\nproxy_set_header Host $host;\nproxy_set_header X-Real-IP $remote_addr;\nproxy_set_header x_real_ipP $remote_addr;\nproxy_set_header remote_addr $remote_addr;\nproxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\nproxy_http_version 1.1;\nproxy_connect_timeout 4s;\nproxy_read_timeout 600s;\nproxy_send_timeout 12s;\nproxy_set_header Upgrade $http_upgrade;\nproxy_set_header Connection upgrade;\n}\n\n#error_page  404              /404.html;\n# redirect server error pages to the static page /50x.html\n#\nerror_page   500 502 503 504  /50x.html;\nlocation = /50x.html {\nroot   /usr/share/nginx/html;\n}\n}\n\n")),(0,o.kt)("h2",{id:"how-to-check-the-question"},"How to check the question"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("ol",{parentName:"li"},(0,o.kt)("li",{parentName:"ol"},"There are more than 100 modules in Linkis, and the final service has 7 services, which are linkis-cg -ngineconnmanager, linkis-cg -ngineplugin, linkis-cg-entrance, linkis-cg-linkisManager,\nLinkis-Mg-Gateway, Linkis-Mg-Eureka, Linkis-PS-PublicService, each module has this different features. Among them, Linkis-CG-ENGINECONNMANAGER is responsible for managing the start-engine service, which will generate the corresponding engine script to pull up the engine. Services, so our team launched the Linkis-CG-ENGINECONNMANAGER alone on the server for sufficient resources to execute on the server."))),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("ol",{parentName:"li",start:2},(0,o.kt)("li",{parentName:"ol"},"The execution of engines like JDBC, Spark.hedu and other engines require some JAR package support. When the linkis species is called material, these jar packs will be hit in the linkis-cg-oblmphin engine when packaging , Conf and LIB directory will appear. When starting this service, two packages will be uploaded to the configuration directory, which will generate two ZIP files. We use OSS to store these material information. Download it to the Linkis-CG-ENGINECONNMANAGER service, and then configure the following two configurations of wds.linkis.enginecoon.public.dir and wds.linkis.enginecoon.root.dir, then the bag will be pulled to WDS. Linkis.engineCoon.public.dir is the directory of wds.linkis.enginecoon.root.dir. .dir."))),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("ol",{parentName:"li",start:3},(0,o.kt)("li",{parentName:"ol"},"If you want to check the engine log, you can see the directory under wds.linkis.enginecoon.root.dir configuration. Of course, the log information will be displayed on the log of the scriptis page. Just paste it to find it.")))))}u.isMDXComponent=!0}}]);