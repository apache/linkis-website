<!doctype html>
<html class="docs-version-current" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.6">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Apache Linkis Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Apache Linkis Blog Atom Feed">
<link rel="search" type="application/opensearchdescription+xml" title="Apache Linkis" href="/opensearch.xml"><title data-react-helmet="true">FQA | Apache Linkis</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://linkis.incubator.apache.org/faq/main"><meta data-react-helmet="true" name="docsearch:language" content="en"><meta data-react-helmet="true" name="docsearch:version" content="current"><meta data-react-helmet="true" name="docsearch:docusaurus_tag" content="docs-faq-current"><meta data-react-helmet="true" property="og:title" content="FQA | Apache Linkis"><meta data-react-helmet="true" name="description" content="Linkis1.0 common problems and solutions//docs.qq.com/doc/DWlN4emlJeEJxWlR0"><meta data-react-helmet="true" property="og:description" content="Linkis1.0 common problems and solutions//docs.qq.com/doc/DWlN4emlJeEJxWlR0"><link data-react-helmet="true" rel="shortcut icon" href="/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://linkis.incubator.apache.org/faq/main"><link data-react-helmet="true" rel="alternate" href="https://linkis.incubator.apache.org/faq/main" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://linkis.incubator.apache.org/zh-CN/faq/main" hreflang="zh-CN"><link data-react-helmet="true" rel="alternate" href="https://linkis.incubator.apache.org/faq/main" hreflang="x-default"><link data-react-helmet="true" rel="preconnect" href="https://AE29KQB3IA-dsn.algolia.net" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.43be7c54.css">
<link rel="preload" href="/assets/js/runtime~main.c21e8335.js" as="script">
<link rel="preload" href="/assets/js/main.1e191043.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div><a href="#" class="skipToContent_OuoZ">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><img src="/img/logo.png" alt="Apache Linkis Logo" class="themedImage_TMUO themedImage--light_4Vu1 navbar__logo"><img src="/img/logo.png" alt="Apache Linkis Logo" class="themedImage_TMUO themedImage--dark_uzRr navbar__logo"><b class="navbar__title">Apache Linkis(Incubating)</b></a><a class="navbar__item navbar__link" href="/download/main">Download</a><a class="navbar__item navbar__link" href="/community/how-to-subscribe">Community</a><a class="navbar__item navbar__link" href="/blog">Blog</a><a class="navbar__item navbar__link" href="/team">Team</a><a class="navbar__item navbar__link" href="/user">Users</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/faq/main">FAQ</a><div class="navbar__item dropdown dropdown--hoverable"><a class="navbar__link">ASF</a><ul class="dropdown__menu"><li><a href="https://www.apache.org/" target="_blank" rel="noopener noreferrer" class="dropdown__link">Foundation</a></li><li><a href="https://www.apache.org/licenses/" target="_blank" rel="noopener noreferrer" class="dropdown__link">License</a></li><li><a href="https://www.apache.org/events/current-event" target="_blank" rel="noopener noreferrer" class="dropdown__link">Events</a></li><li><a href="https://www.apache.org/security/" target="_blank" rel="noopener noreferrer" class="dropdown__link">Security</a></li><li><a href="https://www.apache.org/foundation/sponsorship.html" target="_blank" rel="noopener noreferrer" class="dropdown__link">Sponsorship</a></li><li><a href="https://www.apache.org/foundation/policies/privacy.html" target="_blank" rel="noopener noreferrer" class="dropdown__link">Privacy</a></li><li><a href="https://www.apache.org/foundation/thanks.html" target="_blank" rel="noopener noreferrer" class="dropdown__link">Thanks</a></li></ul></div></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a class="navbar__link">Doc</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/latest/introduction">1.3.0</a></li><li><a class="dropdown__link" href="/docs/1.2.0/introduction">1.2.0</a></li><li><a class="dropdown__link" href="/docs/1.1.1/introduction">1.1.1</a></li><li><a class="dropdown__link" href="/docs/1.3.1/about/introduction">Next(1.3.1)</a></li><li><a class="dropdown__link" href="/versions">All Version</a></li></ul></div><a href="https://github.com/apache/incubator-linkis" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link" aria-label="GitHub"></a><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" class="navbar__link"><span><svg t="1631348384596" class="iconLanguage_EbrZ" viewBox="0 0 1024 1024" version="1.1" p-id="557" width="20" height="20"><path d="M547.797333 638.208l-104.405333-103.168 1.237333-1.28a720.170667 720.170667 0 0 0 152.490667-268.373333h120.448V183.082667h-287.744V100.906667H347.605333v82.218666H59.818667V265.386667h459.178666a648.234667 648.234667 0 0 1-130.304 219.946666 643.242667 643.242667 0 0 1-94.976-137.728H211.541333a722.048 722.048 0 0 0 122.453334 187.434667l-209.194667 206.378667 58.368 58.368 205.525333-205.525334 127.872 127.829334 31.232-83.84m231.424-208.426667h-82.218666l-184.96 493.312h82.218666l46.037334-123.306667h195.242666l46.464 123.306667h82.218667l-185.002667-493.312m-107.690666 287.744l66.56-178.005333 66.602666 178.005333z" fill="currentColor" p-id="558"></path></svg><span>English</span></span></a><ul class="dropdown__menu"><li><a href="/faq/main" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" style="text-transform:capitalize">English</a></li><li><a href="/zh-CN/faq/main" target="_self" rel="noopener noreferrer" class="dropdown__link" style="text-transform:capitalize">简体中文</a></li></ul></div><div class="searchBox_Bc3W"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper docs-wrapper docs-doc-page"><div class="docPage_lDyR"><button class="clean-btn backToTopButton_i9tI" type="button"><svg viewBox="0 0 24 24" width="28"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z" fill="currentColor"></path></svg></button><aside class="docSidebarContainer_0YBq"><div class="sidebar_a3j0"><nav class="menu thin-scrollbar menu_cyFh menuWithAnnouncementBar_+O1J"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link menu__list-item"><a class="menu__link menu__link--active" aria-current="page" href="/faq/main">FQA</a></li></ul></nav></div></aside><main class="docMainContainer_r8cw"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_zHA2"><div class="docItemContainer_oiyr"><article><div class="tocCollapsible_aw-L theme-doc-toc-mobile tocMobile_Tx6Y"><button type="button" class="clean-btn tocCollapsibleButton_zr6a">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>FQA</h1></header><blockquote><p>Linkis1.0 common problems and solutions: <a href="https://docs.qq.com/doc/DWlN4emlJeEJxWlR0" target="_blank" rel="noopener noreferrer">https://docs.qq.com/doc/DWlN4emlJeEJxWlR0</a></p></blockquote><header><h1>1. Use problems</h1></header><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q1-the-ps-cs-service-log-of-linkis-reports-this-error-figservletwebserverapplicationcontext-559"></a>Q1: The ps-cs service log of linkis reports this error: figServletWebServerApplicationContext (559)<a class="hash-link" href="#q1-the-ps-cs-service-log-of-linkis-reports-this-error-figservletwebserverapplicationcontext-559" title="Direct link to heading">#</a></h2><div class="codeBlockContainer_J+bg"><div class="codeBlockContent_csEI"><pre tabindex="0" class="prism-code language-undefined codeBlock_rtdJ thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#F8F8F2"><span class="token plain">[refresh] - Exception encountered during context initi</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">alization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name &#x27;eurekaInstanceLabelClient&#x27;: Invocation of initkaba method failed; nested exception is java.lang.RuntimeException: com.netflix.client.ClientException: Load balancer does not have available server for client: linkis-ps-publicservice</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><p>A: This is because the publicservice service did not start successfully. It is recommended to manually restart the publicservice sh/sbin/linkis-dameo.sh restart ps-publicservice</p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q2-linkis-eureka-debugging-instructions"></a>Q2: Linkis-eureka debugging instructions<a class="hash-link" href="#q2-linkis-eureka-debugging-instructions" title="Direct link to heading">#</a></h2><p>A: If you need to debug the eureka program, you need to do some configuration first, as shown below
application-eureka.yml needs to remove part of the comment configuration, and the normal startup configuration is as follows:
<img alt="1639466558031" src="/assets/images/q2_1-697a210422acb1835076820356a56df5.png"></p><p><img alt="1639466558031" src="/assets/images/q2_2-0b84185f007116f30800e4c4f5639004.png"></p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q3-eureka-automatically-stops-when-it-starts-for-the-first-time-and-needs-to-be-restarted-manually"></a>Q3: eureka automatically stops when it starts for the first time, and needs to be restarted manually<a class="hash-link" href="#q3-eureka-automatically-stops-when-it-starts-for-the-first-time-and-needs-to-be-restarted-manually" title="Direct link to heading">#</a></h2><p>A: This is because eureka does not use nohup when starting the Java process. After the session exits, the task is automatically cleaned up by the operating system. You need to modify the eureka startup script and add nohup:</p><p><img src="/assets/images/q3_1-d1ab012f4d757efb95241370f4e2e3e1.png"></p><p>You can refer to PR: <a href="https://github.com/apache/incubator-linkis/pull/837/files" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-linkis/pull/837/files</a></p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q4-linkis-entrance-logwriter-missing-dependencies"></a>Q4: Linkis Entrance LogWriter missing dependencies<a class="hash-link" href="#q4-linkis-entrance-logwriter-missing-dependencies" title="Direct link to heading">#</a></h2><p>A: Hadoop 3 needs to modify the linkis-hadoop-common pom file, see: <a href="https://linkis.apache.org/zh-CN/docs/next/development/linkis-compile-and-package/" target="_blank" rel="noopener noreferrer">https://linkis.apache.org/zh-CN/docs/next/development/linkis-compile-and-package/</a></p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q5-when-linkis10-executes-tasks-the-ecp-service-throws-the-following-error-caused-by-javautilnosuchelementexception-noneget"></a>Q5: When Linkis1.0 executes tasks, the ECP service throws the following error: Caused by: java.util.NoSuchElementException: None.get?<a class="hash-link" href="#q5-when-linkis10-executes-tasks-the-ecp-service-throws-the-following-error-caused-by-javautilnosuchelementexception-noneget" title="Direct link to heading">#</a></h2><p>Error detailed log:</p><p>Solution:
At this time, because the corresponding engine version material does not have a corresponding record in the database table, it may be caused by an error when the ecp service is started. You can restart the ecp service to see if there is an error when uploading the BML. The corresponding table is: linkis_cg_engine_conn_plugin_bml_resources</p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q6-linkis1x-general-troubleshooting-method-for-insufficient-resources"></a>Q6: Linkis1.X general troubleshooting method for insufficient resources<a class="hash-link" href="#q6-linkis1x-general-troubleshooting-method-for-insufficient-resources" title="Direct link to heading">#</a></h2><p>Insufficient resources fall into two situations:</p><ol><li>Insufficient resources of the server itself</li><li>The user&#x27;s own resources are insufficient (linkis will control user resources).
These two resources are recorded in linkis_cg_manager_label_resource and linkis_cg_manager_linkis_resource in linkis1.X, the former is the association table of label and resource, and the latter is the resource table
Normally, linkis1.0 is safe for high concurrency control of resources, and it is not recommended to forcibly reset user resource records by modifying table records. However, due to the difference in the execution environment of linkis during the installation and debugging process, engine startup failures may occur, or repeated restarts of microservices during the engine startup process lead to unsafe release of resources, or the monitor does not have time to automatically clean up (some hours) level delay), there may be a problem of insufficient resources, and in severe cases, most of the user&#x27;s resources will be locked. Therefore, you can refer to the following steps to troubleshoot insufficient resources:
a. Confirm on the management console whether the remaining resources of the ECM are greater than the requested resources of the engine. If the remaining resources of the ECM are very small, it will cause the request for a new engine to fail. You need to manually turn off some idle engines in the ECM. There is also a mechanism for automatic release when idle, but this time is set relatively long by default.
b. If the ECM resources are sufficient, it must be that the remaining resources of the user are not enough to request a new engine. First, determine the label generated when the user executes the task. For example, if the user hadoop executes the spark2.4.3 script on Scriptis, it will be corresponding in the linkis_cg_manager_label table next record
We get the id value of this label, find the corresponding resourceId in the association table linkis_cg_manager_label_resource, and find the corresponding resource record of the label in linkis_cg_manager_linkis_resource through resourceId, you can check the remaining resources in this record</li></ol><p>If this resource is checked and determined to be abnormal, that is, it does not match the resources generated by the actual engine startup. The following operations can be performed to recover:
After confirming that all engines under the label have been shut down, you can directly delete this resource and the associated record corresponding to the association table linkis_cg_manager_label_resource, and this resource will be automatically reset when you request again.
Note: All engines of this label have been shut down. In the previous example, it means that the spark2.4.3 engines started by the hadoop user on Scriptis have all been shut down. You can see all the engines started by the user in the resource management of the management console. instance. Otherwise, the resource record exception of the label may also occur.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q7-linkis-startup-error-nosuchmethoderrorgetsessionmanagerlorgeclipsejettyserversessionmanager"></a>Q7: linkis startup error: NoSuchMethodErrorgetSessionManager()Lorg/eclipse/jetty/server/SessionManager<a class="hash-link" href="#q7-linkis-startup-error-nosuchmethoderrorgetsessionmanagerlorgeclipsejettyserversessionmanager" title="Direct link to heading">#</a></h2><p>Specific stack:</p><div class="codeBlockContainer_J+bg"><div class="codeBlockContent_csEI"><pre tabindex="0" class="prism-code language-undefined codeBlock_rtdJ thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#F8F8F2"><span class="token plain">startup of context o.s.b.w.e.j.JettyEmbeddedWebAppContext@6c6919ff{application,/,[file:///tmp/jetty-docbase.9102.6375358926927953589/],UNAVAILABLE} java.lang.NoSuchMethodError: org.eclipse.jetty.server.session.SessionHandler.getSessionManager()Lorg/eclipse/jetty/server/SessionManager;</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">at org.eclipse.jetty.servlet.ServletContextHandler\$Context.getSessionCookieConfig(ServletContextHandler.java:1415) ~[jetty-servlet-9.3.20.v20170531.jar:9.3.20.v20170531]</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><p>Solution: jetty-servlet and jetty-security versions need to be upgraded from 9.3.20 to 9.4.20;</p><header><h1>2. Environmental issues</h1></header><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q1-linkis10-execution-task-report-select-list-is-not-in-group-by-clause"></a>Q1: Linkis1.0 execution task report: select list is not in group by clause<a class="hash-link" href="#q1-linkis10-execution-task-report-select-list-is-not-in-group-by-clause" title="Direct link to heading">#</a></h2><p><img alt="1639466558031" src="/assets/images/q5_1-0ee7a3e215d17b5032fba9039eaf41f9.jpg"></p><p><img alt="1639466558031" src="/assets/images/q5_2-7c44e577054f2a65c49d197babf7a702.png"></p><p>This problem is caused by the default value of the global setting mode in mysql 5.8 version, you need to execute the following line in the mysql cli:</p><div class="codeBlockContainer_J+bg"><div class="codeBlockContent_csEI"><pre tabindex="0" class="prism-code language-undefined codeBlock_rtdJ thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#F8F8F2"><span class="token plain">SET GLOBAL sql_mode=(SELECT REPLACE(@@sql_mode,&#x27;ONLY_FULL_GROUP_BY&#x27;,&#x27;&#x27;));</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q2-when-executing-scripts-after-deployment-executing-commands-and-collecting-results-i-encountered-such-an-error-ioexception-file-header-type-must-be-dolphin"></a>Q2: When executing scripts after deployment, executing commands, and collecting results, I encountered such an error, IOException: File header type must be dolphin:<a class="hash-link" href="#q2-when-executing-scripts-after-deployment-executing-commands-and-collecting-results-i-encountered-such-an-error-ioexception-file-header-type-must-be-dolphin" title="Direct link to heading">#</a></h2><p>A: This should be caused by repeated installations, resulting in the result set being written in the same file. The previous Linkis 0.X version used the result set to write append, and 1.0 has been modified to add a new one. You can clean up the result set Directory: The configuration parameter is wds.linkis.resultSet.store.path, you can clean up this directory</p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q3-the-json4s-package-conflict-caused-by-inconsistent-spark-versions-the-error-is-as-follows-error-message-caused-by-javalangnosuchmethoderror-orgjson4sjacksonjsonmethod"></a>Q3: The json4s package conflict caused by inconsistent Spark versions, the error is as follows: Error message: caused by: java.lang.NoSuchMethodError: org.json4s.jackson.jsonMethod$<a class="hash-link" href="#q3-the-json4s-package-conflict-caused-by-inconsistent-spark-versions-the-error-is-as-follows-error-message-caused-by-javalangnosuchmethoderror-orgjson4sjacksonjsonmethod" title="Direct link to heading">#</a></h2><p>solution:
This is because of Spark jars&#x27; json4s and lib/linkis-engineplugins/spark/dist/version/lib
The json4s version in the package is inconsistent. When the official release is released, the supported version of Spark will be indicated later. If it is inconsistent, this problem will exist.
The solution is to replace the json4s package in Spark jars with lib/linkis-engineplugins/spark/dist/version/lib
The json4s version inside the package. In addition, there may be conflicts in the netty package, which can be handled according to the method of Json4s. Then restart the ecp service: sh sbin/linkis-damon.sh restart cg-engineplugin</p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q4-when-linkis1x-submits-spark-sql-tasks-in-version-cdh5161-how-to-troubleshoot-404-problems"></a>Q4: When Linkis1.X submits spark sql tasks in version CDH5.16.1, how to troubleshoot 404 problems<a class="hash-link" href="#q4-when-linkis1x-submits-spark-sql-tasks-in-version-cdh5161-how-to-troubleshoot-404-problems" title="Direct link to heading">#</a></h2><p>The main error message is as follows:</p><div class="codeBlockContainer_J+bg"><div class="codeBlockContent_csEI"><pre tabindex="0" class="prism-code language-undefined codeBlock_rtdJ thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#F8F8F2"><span class="token plain">21304, Task is Failed,errorMsg: errCode: 12003 ,desc: ip:port_x Failed to async get EngineNode FeignException.NotFound: status 404 reading RPCReceiveRemote#receiveAndReply(Message) ,ip: xxxxx ,port: 9104 ,serviceKind: linkis-cg-entrance</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">org.apache.jasper.servlet.JspServlet 89 warn - PWC6117: File &quot;/home/hadoop/dss1.0/tmp/hadoop/workDir/7c3b796f-aadd-46a5-b515-0779e523561a/tmp/jetty-docbase.1802511762054502345.46019/api/rest_j/v1/rpc/receiveAndReply&quot; not found</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><p>The above error message is mainly caused by the jar conflict in the cdh environment variable. You need to find the jar package where the org.apache.jasper.servlet.JspServlet class is located. The local cdh environment variable path is: /opt/cloudera/parcels/CDH -5.16.1-1.cdh5.16.1.p0.3/jars, delete the corresponding jasper-compile-${version}.jar and jsp-${version}.jar jar packages under this directory, The spark sql task can be run again without restarting the service, and the problem is solved.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q5-running-error-report-missing-package-matplotlib"></a>Q5: running error report missing package matplotlib<a class="hash-link" href="#q5-running-error-report-missing-package-matplotlib" title="Direct link to heading">#</a></h2><p>In the standard python environment, anaconda2 and anaconda3 need to be installed, and the default anaconda is anaconda2. This contains most common python libraries.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q6-when-the-microservice-linkis-ps-cs-is-started-debuggclasswriter-overrides-final-method-visit-is-reported"></a>Q6: When the microservice linkis-ps-cs is started, DebuggClassWriter overrides final method visit is reported<a class="hash-link" href="#q6-when-the-microservice-linkis-ps-cs-is-started-debuggclasswriter-overrides-final-method-visit-is-reported" title="Direct link to heading">#</a></h2><p>Specific exception stack:</p><p>Solution: jar package conflict, delete asm-5.0.4.jar;</p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q7-when-the-shell-engine-schedules-execution-the-engine-execution-directory-reports-the-following-error-binjava-no-such-file-or-directory"></a>Q7: When the shell engine schedules execution, the engine execution directory reports the following error /bin/java: No such file or directory:<a class="hash-link" href="#q7-when-the-shell-engine-schedules-execution-the-engine-execution-directory-reports-the-following-error-binjava-no-such-file-or-directory" title="Direct link to heading">#</a></h2><p><img src="/assets/images/q24_1-ba0715c637d27f81c926e66cc6858102.png"></p><p>Solution: There is a problem with the environment variables of the local java, and a symbolic link needs to be made to the java command.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q8-when-the-hive-engine-is-scheduled-the-error-log-of-engineconnmanager-is-as-follows-method-did-not-exist-sessionhandler"></a>Q8: When the hive engine is scheduled, the error log of engineConnManager is as follows: method did not exist: SessionHandler:<a class="hash-link" href="#q8-when-the-hive-engine-is-scheduled-the-error-log-of-engineconnmanager-is-as-follows-method-did-not-exist-sessionhandler" title="Direct link to heading">#</a></h2><p><img src="/assets/images/q27_1-9604d3aee4192bf09b279225c011357b.png"></p><p>Solution: Under the hive engine lib, the jetty jar package conflicts, replace jetty-security and jetty-server with 9.4.20;</p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q9-when-the-hive-engine-is-executing-the-following-error-is-reported-lcomgooglecommoncollectunmodifiableiterator"></a>Q9: When the hive engine is executing, the following error is reported Lcom/google/common/collect/UnmodifiableIterator:<a class="hash-link" href="#q9-when-the-hive-engine-is-executing-the-following-error-is-reported-lcomgooglecommoncollectunmodifiableiterator" title="Direct link to heading">#</a></h2><div class="codeBlockContainer_J+bg"><div class="codeBlockContent_csEI"><pre tabindex="0" class="prism-code language-undefined codeBlock_rtdJ thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#F8F8F2"><span class="token plain">2021-03-16 13:32:23.304 ERROR [pool-2-thread-1]com.webank.wedatasphere.linkis.engineplugin.hive.executor.HiveEngineConnExecutor 140 run - query failed, reason : java.lang.AccessError: tried to access method com.google.common.collect.Iterators.emptyIterator() Lcom/google/common/collect/UnmodifiableIterator; from class org.apache.hadoop.hive.ql.exec.FetchOperator </span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">at org.apache.hadoop.hive.ql.exec.FetchOperator.&lt;init&gt;(FetchOperator.java:108) ~[hive-exec-2.1.1-cdh6.1.0.jar:2.1.1-cdh6.1.0]</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">at org.apache.hadoop.hive.ql.exec.FetchTask.initialize(FetchTask.java:86) ~[hive-exec-2.1.1-cdh6.1.0.jar:2.1.1-cdh6.1.0]</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">at org.apache.hadoop.hive.ql..compile(Driver.java:629) ~[hive-exec-2.1.1-cdh6.1.0.jar:2.1.1-cdh6.1.0]</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1414) ~[hive-exec-2.1.1-cdh6.1.0.jar:2.1.1-cdh6.1.0]</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1543) ~[hive-exec-2.1.1-cdh6.1.0.jar:2.1.1-cdh6.1.0]</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1332) ~[hive-exec-2.1.1-cdh6.1.0.jar:2.1.1-cdh6.1.0]</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1321) ~[hive-exec-2.1.1-cdh6.1.0.jar:2.1.1-cdh6.1.0]</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">atcom.webank.wedatasphere.linkis.engineplugin.hive.executor.HiveEngineConnExecutor$$anon$1.run(HiveEngineConnExecutor.scala:152) [linkis-engineplugin-hive-dev-1.0.0.jar:?]</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">atcom.webank.wedatasphere.linkis.engineplugin.hive.executor.HiveEngineConnExecutor$$anon$1.run(HiveEngineConnExecutor.scala:126) [linkis-engineplugin-hive-dev-1.0.0.jar:?]</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><p>Solution: guava package conflict, delete guava-25.1-jre.jar under hive/dist/v1.2.1/lib;</p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q10-during-engine-scheduling-the-following-error-is-reported-python-processes-is-not-alive"></a>Q10: During engine scheduling, the following error is reported: Python processes is not alive:<a class="hash-link" href="#q10-during-engine-scheduling-the-following-error-is-reported-python-processes-is-not-alive" title="Direct link to heading">#</a></h2><p><img src="/assets/images/q29_1-1787e33057b87430ad3e23f25c945720.png"></p><p>Solution: Install the anaconda3 package manager on the server. After debugging python, two problems were found: (1) lack of pandas and matplotlib modules, which need to be installed manually; (2) when the new version of the python engine is executed, it depends on a higher version of python, and python3 is installed first. Next, make a symbolic link (as shown below), and restart the engineplugin service.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q11-when-the-spark-engine-is-running-the-following-error-is-reported-noclassdeffounderror-orgapachehadoophiveqlioorcorcfile"></a>Q11: When the spark engine is running, the following error is reported: NoClassDefFoundError: org/apache/hadoop/hive/ql/io/orc/OrcFile:<a class="hash-link" href="#q11-when-the-spark-engine-is-running-the-following-error-is-reported-noclassdeffounderror-orgapachehadoophiveqlioorcorcfile" title="Direct link to heading">#</a></h2><div class="codeBlockContainer_J+bg"><div class="codeBlockContent_csEI"><pre tabindex="0" class="prism-code language-undefined codeBlock_rtdJ thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#F8F8F2"><span class="token plain">2021-03-19 15:12:49.227 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler 57 logInfo -ShuffleMapStage 5 (show at &lt;console&gt;:69) failed in 21.269 s due to Job aborted due to stage failure: Task 1 in stage 5.0 failed 4 times, most recent failure: Lost task 1.3 in stage 5.0 (TID 139, cdh03, executor 6):java.lang.NoClassDefFoundError: org/apache/hadoop/hive/ql/io/orc/OrcFile </span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><p>Solution: The classpath of the cdh6.3.2 cluster spark engine only has /opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/spark/jars, you need to add hive-exec-2.1.1- cdh6.1.0.jar, and restart spark.</p><header><h1>Three, configuration problems</h1></header><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q1-the-database-on-the-left-side-of-the-script-cannot-be-refreshed"></a>Q1: The database on the left side of the script cannot be refreshed<a class="hash-link" href="#q1-the-database-on-the-left-side-of-the-script-cannot-be-refreshed" title="Direct link to heading">#</a></h2><p>solution:
a. The reason may be that the linkis-metatdata service did not read the HIVE_CONF_DIR error, you can configure the parameters of linkis-metadata: corresponding to the JDBC connection string of the metadata database</p><div class="codeBlockContainer_J+bg"><div class="codeBlockContent_csEI"><pre tabindex="0" class="prism-code language-undefined codeBlock_rtdJ thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#F8F8F2"><span class="token plain">hive.meta.url=</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">hive.meta.user=</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">hive.meta.password=</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q2-the-right-side-of-scriptis-cannot-refresh-the-database-and-it-has-been-refreshing-it-should-be-noted-that-the-metadata-of-linkis-does-not-support-docking-sentry-and-ranger-for-the-time-being-only-supports-native-permission-control-of-hive-error-message-the-front-end-database-tab-has-been-refreshing-state"></a>Q2: The right side of Scriptis cannot refresh the database, and it has been refreshing (it should be noted that the metadata of linkis does not support docking sentry and Ranger for the time being, only supports native permission control of hive), error message: the front-end database tab has been refreshing state<a class="hash-link" href="#q2-the-right-side-of-scriptis-cannot-refresh-the-database-and-it-has-been-refreshing-it-should-be-noted-that-the-metadata-of-linkis-does-not-support-docking-sentry-and-ranger-for-the-time-being-only-supports-native-permission-control-of-hive-error-message-the-front-end-database-tab-has-been-refreshing-state" title="Direct link to heading">#</a></h2><p>solution:
This is because we have restricted permissions for the database on the right, and this relies on hive to enable authorized access:</p><div class="codeBlockContainer_J+bg"><div class="codeBlockContent_csEI"><pre tabindex="0" class="prism-code language-undefined codeBlock_rtdJ thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#F8F8F2"><span class="token plain">hive.security.authorization.enabled=true; </span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><p>For details, please refer to: <a href="https://blog.csdn.net/yancychas/article/details/84202400" target="_blank" rel="noopener noreferrer">https://blog.csdn.net/yancychas/article/details/84202400</a>
If you have configured this parameter and have not yet enabled it, you need to grant the user the corresponding database table permission and execute the grant statement. You can refer to the same link authorization section.</p><div class="codeBlockContainer_J+bg"><div class="codeBlockContent_csEI"><pre tabindex="0" class="prism-code language-undefined codeBlock_rtdJ thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Authorization reference Take hadoop as an example:</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block">
</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Enter the hive client to check the hadoop user database authorization status:</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block">
</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">show grant user hadoop on database default;</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block">
</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Authorize the user database:</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block">
</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">grant all on database default to user hadoop;</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block">
</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><p>If you don&#x27;t want to enable permission control, that is, every user can see the library table, you can modify: com/webank/wedatasphere/linkis/metadata/hive/dao/impl/HiveMetaDao.xml sql remove the permission control part</p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q3-scriptisworkspace-when-i-log-in-to-scriptis-the-root-directory-does-not-exist-and-there-are-two-root-directories-workspace-and-hdfs-error-message-after-logging-in-the-front-end-pops-up-the-following-message-the-users-local-directory-does-not-exist-please-contact-admin-to-add"></a>Q3: [Scriptis][Workspace] When I log in to Scriptis, the root directory does not exist, and there are two root directories: workspace and HDFS: Error message: After logging in, the front end pops up the following message (the user’s local directory does not exist, please Contact admin to add)<a class="hash-link" href="#q3-scriptisworkspace-when-i-log-in-to-scriptis-the-root-directory-does-not-exist-and-there-are-two-root-directories-workspace-and-hdfs-error-message-after-logging-in-the-front-end-pops-up-the-following-message-the-users-local-directory-does-not-exist-please-contact-admin-to-add" title="Direct link to heading">#</a></h2><p>solution:</p><ul><li>a. Confirm the linkis.properties parameter in the conf directory of linkis-ps-publicservice: wds.linkis.workspace.filesystem.localuserrootpath=file:///tmp/linkis/ Does it start with file://?</li><li>b. Confirm whether wds.linkis.workspace.filesystem.hdfsuserrootpath.prefix=hdfs:///tmp/linkis/ starts with hdfs://</li><li>c. Confirm whether there is a user directory under the /tmp/linkis directory. The user here refers to the front-end login user, such as hadoop user login, then create: /tmp/linkis/hadoop directory, if the directory exists, confirm the directory permission login user It can be operated. If it still doesn’t work, you can refer to the error report of publicservice. The error will explain the permission or the path problem.</li></ul><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q4-management-consolesettings-how-to-adjust-the-yarn-queue-used-by-the-task-error-message-when-executing-the-sql-task-it-is-reported-that-1-obtaining-the-yarn-queue-information-is-abnormal-or-user-xx-cannot-submit-to-the-queue"></a>Q4: [Management Console][Settings] How to adjust the yarn queue used by the task? Error message: When executing the sql task, it is reported that 1. Obtaining the Yarn queue information is abnormal or user XX cannot submit to the queue<a class="hash-link" href="#q4-management-consolesettings-how-to-adjust-the-yarn-queue-used-by-the-task-error-message-when-executing-the-sql-task-it-is-reported-that-1-obtaining-the-yarn-queue-information-is-abnormal-or-user-xx-cannot-submit-to-the-queue" title="Direct link to heading">#</a></h2><p>solution:
In the front end - management console - settings - general settings - Yarn queue configuration login user has permission queue</p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q5-when-hive-queries-it-reports-can-not-find-zk-related-classes-such-as-orgapachecurator-error-message-when-executing-hive-tasks-the-log-report-cannot-find-the-class-starting-with-orgapachecurator--classnotfound"></a>Q5: When Hive queries, it reports: Can not find zk-related classes such as: org.apache.curator.<em>, error message: When executing hive tasks, the log report cannot find the class starting with org.apache.curator.</em> , classNotFound<a class="hash-link" href="#q5-when-hive-queries-it-reports-can-not-find-zk-related-classes-such-as-orgapachecurator-error-message-when-executing-hive-tasks-the-log-report-cannot-find-the-class-starting-with-orgapachecurator--classnotfound" title="Direct link to heading">#</a></h2><p>solution:
This is because the hive transaction is enabled, you can modify the hive-site.xml on the linkis machine to turn off the transaction configuration, refer to the hive transaction: <a href="https://www.jianshu.com/p/aa0f0fdd234c" target="_blank" rel="noopener noreferrer">https://www.jianshu.com/p/aa0f0fdd234c</a>
Or put the relevant package into the engine plugin directory lib/linkis-engineplugins/hive/dist/version/lib</p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q6-how-does-linkis-support-kerberos"></a>Q6: How does Linkis support kerberos<a class="hash-link" href="#q6-how-does-linkis-support-kerberos" title="Direct link to heading">#</a></h2><p>solution:
Obtaining Hadoop&#x27;s FileSystem in linkis is implemented through the HDFSUtils class, so we put kerberos in this class, and users can see the logic of this class. The login modes currently supported are as follows:</p><div class="codeBlockContainer_J+bg"><div class="codeBlockContent_csEI"><pre tabindex="0" class="prism-code language-undefined codeBlock_rtdJ thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#F8F8F2"><span class="token plain">if(KERBEROS_ENABLE.getValue) {</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      val path = new File(</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      TAB_FILE.getValue ， userName + &quot;.keytab&quot;).getPath</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      val user = getKerberosUser(userName)</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      UserGroupInformation.setConfiguration(getConfiguration(userName))</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      UserGroupInformation.loginUserFromKeytabAndReturnUGI(user， path)</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    } else {</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      UserGroupInformation.createRemoteUser(userName)</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><p>Users only need to configure the following parameters in the configuration file linkis.properties:</p><div class="codeBlockContainer_J+bg"><div class="codeBlockContent_csEI"><pre tabindex="0" class="prism-code language-undefined codeBlock_rtdJ thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#F8F8F2"><span class="token plain">wds.linkis.keytab.enable=true</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">wds.linkis.keytab.file=/appcom/keytab/ #keytab placement directory, which stores the username.keytab files of multiple users</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">wds.linkis.keytab.host.enabled=false #Whether to bring principle client authentication</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">wds.linkis.keytab.host=127.0.0.1 #principle authentication needs to bring the client IP</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q7-regarding-linkis-besides-supporting-deployment-user-login-can-other-user-logins-be-configured"></a>Q7: Regarding Linkis, besides supporting deployment user login, can other user logins be configured?<a class="hash-link" href="#q7-regarding-linkis-besides-supporting-deployment-user-login-can-other-user-logins-be-configured" title="Direct link to heading">#</a></h2><p>solution:
sure. Deployment users are for convenience only. linkis-mg-gateway supports access by configuring LDAP service and SSO service. It does not have a user verification system. For example, to enable LDAP service access, you only need to configure linkis-mg-gateway.properties. The configuration of your LDAP server is as follows:</p><div class="codeBlockContainer_J+bg"><div class="codeBlockContent_csEI"><pre tabindex="0" class="prism-code language-undefined codeBlock_rtdJ thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#F8F8F2"><span class="token plain">wds.linkis.ldap.proxy.url=ldap://127.0.0.1:389/#Your LDAP service URL</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">wds.linkis.ldap.proxy.baseDN=dc=webank，dc=com#Configuration of your LDAP service</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><p>If the user needs to perform tasks, a user with the corresponding user name needs to be created on the Linux server. If it is a standard version, the user needs to be able to perform Spark and hive tasks, and needs to establish a corresponding user in the local workspace and HDFS directory /tmp/linkis directory.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q8-how-to-open-linkis-management-console-administrator-page-ecm-and-microservice-management"></a>Q8: How to open Linkis management console, administrator page ECM and microservice management?<a class="hash-link" href="#q8-how-to-open-linkis-management-console-administrator-page-ecm-and-microservice-management" title="Direct link to heading">#</a></h2><p><img src="/assets/images/q15_1-7df6cf4928b02cb4cf2335fabbdaa193.png"></p><p>Solution: You need to set the administrator in the conf/linkis.properties file:</p><div class="codeBlockContainer_J+bg"><div class="codeBlockContent_csEI"><pre tabindex="0" class="prism-code language-undefined codeBlock_rtdJ thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#F8F8F2"><span class="token plain">wds.linkis.governance.station.admin=hadoop,peacewong</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><p>After the setting is complete, restart the publicservice service</p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q9-when-starting-the-microservice-linkis-ps-publicservice-kjdbcutilsgetdriverclassname-npe"></a>Q9: When starting the microservice linkis-ps-publicservice, kJdbcUtils.getDriverClassName NPE<a class="hash-link" href="#q9-when-starting-the-microservice-linkis-ps-publicservice-kjdbcutilsgetdriverclassname-npe" title="Direct link to heading">#</a></h2><p>Specific exception stack: ExternalResourceProvider</p><p><img src="/assets/images/q23_1-9745a8c9c95053318ee900d53cb50771.png"></p><p>Solution: Caused by linkis-ps-publicservice configuration problem, modify the three parameters at the beginning of linkis.properties hive.meta:</p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q10-when-scheduling-the-hive-engine-the-following-error-is-reported-engineconnpluginnotfoundexception-errorcode-70063"></a>Q10: When scheduling the hive engine, the following error is reported: EngineConnPluginNotFoundException: errorCode: 70063<a class="hash-link" href="#q10-when-scheduling-the-hive-engine-the-following-error-is-reported-engineconnpluginnotfoundexception-errorcode-70063" title="Direct link to heading">#</a></h2><p><img src="/assets/images/q25_1-feffb49fb454a8fc0b0fd32dd2fe8c1a.png"></p><p>Solution: The version of the corresponding engine was not modified during installation, so the engine type inserted into the db by default is the default version, and the compiled version is not the default version.</p><p>Specific modification steps:</p><div class="codeBlockContainer_J+bg"><div class="codeBlockContent_csEI"><pre tabindex="0" class="prism-code language-undefined codeBlock_rtdJ thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#F8F8F2"><span class="token plain">cd /appcom/Install/dss-linkis/linkis/lib/linkis-engineconn-plugins/，</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Modify the v2.1.1 directory name in the dist directory to v1.2.1</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Modify the subdirectory name 2.1.1 under the plugin directory to the default version 1.2.1</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">If it is Spark, you need to modify dist/v2.4.3 and plugin/2.4.3 accordingly</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Finally restart the engineplugin service.</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q11-when-the-hive-engine-schedules-execution-the-following-error-is-reported-opertion-failed-nullpointerexception"></a>Q11: When the hive engine schedules execution, the following error is reported: opertion failed NullPointerException:<a class="hash-link" href="#q11-when-the-hive-engine-schedules-execution-the-following-error-is-reported-opertion-failed-nullpointerexception" title="Direct link to heading">#</a></h2><p><img src="/assets/images/q26_1-3ce3dfd6f40af09fce2c891b647df7d1.png"></p><p>Solution: The server lacks environment variables, and /etc/profile adds <code>export HIVE_CONF_DIR=/etc/hive/conf;</code></p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q12-when-the-spark-engine-starts-an-error-is-reported-get-the-queue-information-excepiton-obtaining-yarn-queue-information-exception-and-http-link-exception"></a>Q12: When the spark engine starts, an error is reported get the queue information excepiton. (obtaining Yarn queue information exception) and http link exception<a class="hash-link" href="#q12-when-the-spark-engine-starts-an-error-is-reported-get-the-queue-information-excepiton-obtaining-yarn-queue-information-exception-and-http-link-exception" title="Direct link to heading">#</a></h2><p>Solution: To migrate the address configuration of yarn to the DB configuration, the following configuration needs to be added:</p><div class="codeBlockContainer_J+bg"><div class="codeBlockContent_csEI"><pre tabindex="0" class="prism-code language-undefined codeBlock_rtdJ thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#F8F8F2"><span class="token plain">INSERT INTO `linkis_cg_rm_external_resource_provider` (`resource_type`, `name`, `labels`, `config`) VALUES (&#x27;Yarn&#x27;, &#x27;sit&#x27;, NULL, &#x27;{\r\n&quot;rmWebAddress&quot;: &quot;http://xxip:xxport&quot;,\r\n&quot;hadoopVersion&quot;: &quot;2.7.2&quot;,\r\n&quot;authorEnable&quot;:true,\r\n&quot;user&quot;:&quot;hadoop&quot;,\r\n&quot;pwd&quot;:&quot;xxxx&quot;\r\n}&#x27;);</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block">
</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">config field example</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">{</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">&quot;rmWebAddress&quot;: &quot;http://10.10.10.10:8080&quot;,</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">&quot;hadoopVersion&quot;: &quot;2.7.2&quot;,</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">&quot;authorEnable&quot;:true,</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">&quot;user&quot;:&quot;hadoop&quot;,</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">&quot;pwd&quot;:&quot;passwordxxx&quot;</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><p><img src="/assets/images/q31_1-82ad7f3ad31c7ce507e41fa27f939e2a.png"></p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q13-pythonspark-scheduling-execution-error-initialize-python-executor-failed-classnotfoundexception-orgslf4jimplstaticloggerbinder"></a>Q13: pythonspark scheduling execution, error: initialize python executor failed ClassNotFoundException org.slf4j.impl.StaticLoggerBinder<a class="hash-link" href="#q13-pythonspark-scheduling-execution-error-initialize-python-executor-failed-classnotfoundexception-orgslf4jimplstaticloggerbinder" title="Direct link to heading">#</a></h2><p>details as follows:</p><p><img src="/assets/images/q32_2-6771db328f6fedb7edde749b18e26be6.png"></p><p>Solution: The reason is that the spark server lacks slf4j-log4j12-1.7.25.jar, copy the above jar to /opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/spark/jars .</p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q14-common-package-conflicts"></a>Q14: Common package conflicts:<a class="hash-link" href="#q14-common-package-conflicts" title="Direct link to heading">#</a></h2><ol><li><p>java.lang.NoSuchMethodError: javax.ws.rs.core.Application.getProperties()Ljava/util/Map;
The conflict package is: jsr311-api-1.1.1.jar may also conflict with jessery</p></li><li><p>java.lang.BootstrapMethodError: java.lang.NoSuchMethodError: javax.servlet.ServletContext.setInitParameter(Ljava/lang/String;Ljava/lang/String;)Z</p><p>The conflict package is: servlet-api.jar</p></li><li><p>org/eclipse/jetty/util/processorUtils
The conflicting package is: jetty-util-9.4.11.v20180605.jar This is the correct version</p></li><li><p>java.lang.NoClassDefFoundError: Could not initialize class dispatch.Http$</p><p>The conflicting package needs to be copied in: netty-3.6.2.Final.jar</p></li><li><p>Conflicts caused by other jars brought in by hive-exec Calcite-avatica-1.6.0.jar may also bring conflicts in the jackson package, resulting in errors related to com.fasterxml.jackson.databind
Cannot inherit from final class is caused by calcite-avatica-1.6.0.jar</p></li><li><p>LZO compression problem hadoop-lzo jar
7.org.eclipse.jetty.server.session.SessionHandler.getSessionManager()Lorg/eclipse/jetty/server/SessionManager;
Need to replace conflicting packages: jetty-servlet and jetty-security with 9.4.20</p></li></ol><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q15-the-mysql-script-running-scripts-reports-an-errorsql-engine-reports-an-error"></a>Q15: The Mysql script running Scripts reports an error\sql engine reports an error<a class="hash-link" href="#q15-the-mysql-script-running-scripts-reports-an-errorsql-engine-reports-an-error" title="Direct link to heading">#</a></h2><p>MYSQL script: run sql error:</p><div class="codeBlockContainer_J+bg"><div class="codeBlockContent_csEI"><pre tabindex="0" class="prism-code language-undefined codeBlock_rtdJ thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#F8F8F2"><span class="token plain">com.webank.wedatasphere.linkis.orchestrator.ecm.exception.ECMPluginErrorException: errCode: 12003 ,desc: localhost:9101_0 Failed  to async get EngineNode RMErrorException: errCode: 11006 ,desc: Failed to request external resourceClassCastException: org.json4s.JsonAST$JNothing$ cannot be cast to org.json4s.JsonAST$JString ,ip: localhost ,port: 9101 ,serviceKind: linkis-cg-linkismanager ,ip: localhost ,port: 9104 ,serviceKind: linkis-cg-entrance</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    at com.webank.wedatasphere.linkis.orchestrator.ecm.ComputationEngineConnManager.getEngineNodeAskManager(ComputationEngineConnManager.scala:157) ~[linkis-orchestrator-ecm-plugin-1.0.2.jar:?]</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><p>Solution: modify the correct yarn address in the linkis_cg_rm_external_resource_provider table</p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q16-the-waiting-time-for-scriptis-to-execute-the-script-is-long"></a>Q16: The waiting time for scriptis to execute the script is long<a class="hash-link" href="#q16-the-waiting-time-for-scriptis-to-execute-the-script-is-long" title="Direct link to heading">#</a></h2><p><img src="/assets/images/q35_1-c00cc3df2f86dd1c3eee9a152d93bb2a.png"></p><p>scriptis execution script waits for a long time, and reports Failed to async get EngineNode TimeoutException:
Solution: You can check the linkismanager log, usually because the engine startup timed out</p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q17-scriptis-executes-jdbc-script-and-reports-an-error"></a>Q17: scriptis executes jdbc script and reports an error<a class="hash-link" href="#q17-scriptis-executes-jdbc-script-and-reports-an-error" title="Direct link to heading">#</a></h2><p>scriptis executes the jdbc script and reports an error</p><div class="codeBlockContainer_J+bg"><div class="codeBlockContent_csEI"><pre tabindex="0" class="prism-code language-undefined codeBlock_rtdJ thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#F8F8F2"><span class="token plain">Failed  to async get EngineNode ErrorException: errCode: 0 ,desc: operation failed(操作失败)s！the reason(原因)：EngineConnPluginNotFoundException: errCode: 70063 ,desc: No plugin foundjdbc-4please check your configuration </span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><p>Solution
You need to install the corresponding engine plug-in, you can refer to: <a href="/docs/latest/deployment/install-engineconn">Engine Installation Guide</a></p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q18-turn-off-resource-checking"></a>Q18: Turn off resource checking<a class="hash-link" href="#q18-turn-off-resource-checking" title="Direct link to heading">#</a></h2><p>Error reporting phenomenon: Insufficient resources
Linkismanager service modify this configuration: wds.linkis.manager.rm.request.enable=false
You can clean up resource records, or set smaller resources
or turn off detection
Linkismanager service modify this configuration: wds.linkis.manager.rm.request.enable=false</p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q19-executing-the-script-reports-an-error"></a>Q19: Executing the script reports an error<a class="hash-link" href="#q19-executing-the-script-reports-an-error" title="Direct link to heading">#</a></h2><div class="codeBlockContainer_J+bg"><div class="codeBlockContent_csEI"><pre tabindex="0" class="prism-code language-undefined codeBlock_rtdJ thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#F8F8F2"><span class="token plain">GatewayErrorException: errCode: 11012 ,desc: Cannot find an instance in the routing chain of serviceId [linkis-cg-entrance], please retry ,ip: localhost ,port: 9001 ,serviceKind: linkis-mg-gateway</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><p><img src="/assets/images/q39_1-764160ac65a63a31c9cc9e2ea6f36f5c.png"></p><p>A: Please check whether the linkis-cg-entrance service starts normally.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q20-scriptis-execute-script-timeoutexception"></a>Q20: ScriptIs execute script TimeoutException<a class="hash-link" href="#q20-scriptis-execute-script-timeoutexception" title="Direct link to heading">#</a></h2><p><img src="/assets/images/q41_1-fc7fee9a7949d93925853282d6b34519.png"></p><p>In linkis-cg-linkismanager.log, Need a ServiceInstance(linkis-cg-entrance, localhost:9104), but cannot find in DiscoveryClient refresh list is repeatedly printed.
This is because the instance is forcibly shut down, but the persistence in the database thinks that the instance still exists. The solution is to clear the two tables linkis_cg_manager_service_instance and linkis_cg_manager_service_instance_metrics after stopping the service.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q21-engine-timeout-setting"></a>Q21: Engine timeout setting<a class="hash-link" href="#q21-engine-timeout-setting" title="Direct link to heading">#</a></h2><p><img src="/assets/images/q43_1-2eab821fc930ab31beb6990a6c5db7a1.png"></p><p>① The parameter configuration of the management console can correspond to the engine parameters, and the timeout time can be modified. After saving, kill the existing engine.
②If the timeout configuration is not displayed, you need to manually modify the linkis-engineplugins directory, the corresponding engine plugin directory such as spark/dist/v2.4.3/conf/linkis-engineconn.properties, the default configuration is wds.linkis.engineconn.max.free.time =1h, means 1h overtime, and can have units m and h. 0 means no timeout, no automatic kill. After the change, you need to restart ecp, and kill the existing engine, and run a new task to start the engine to take effect.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q22-when-creating-a-new-workflow-it-prompts-504-gateway-time-out"></a>Q22: When creating a new workflow, it prompts &quot;504 Gateway Time-out&quot;<a class="hash-link" href="#q22-when-creating-a-new-workflow-it-prompts-504-gateway-time-out" title="Direct link to heading">#</a></h2><p><img src="/assets/images/q44_1-bf45a4798e444d605dacf795803cbd9c.png"></p><p>Error message: The instance 05f211cb021e:9108 of application linkis-ps-cs is not exists. ,ip: 5d30e4bb2f42 ,port: 9001 ,serviceKind: linkis-mg-gateway, as shown below:</p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q23-scripts-execute-python-scripts-the-content-of-the-script-is-very-simple-print-and-execute-successfully-normally-it-can-also-be-executed-successfully-through-the-task-scheduling-system-it-can-also-be-executed-successfully-through-the-job-flow-editing-job-script-page-but-an-error-is-reported-through-job-flow-execution"></a>Q23: Scripts execute python scripts (the content of the script is very simple print) and execute successfully normally. It can also be executed successfully through the task scheduling system. It can also be executed successfully through the job flow editing job script page, but an error is reported through job flow execution.<a class="hash-link" href="#q23-scripts-execute-python-scripts-the-content-of-the-script-is-very-simple-print-and-execute-successfully-normally-it-can-also-be-executed-successfully-through-the-task-scheduling-system-it-can-also-be-executed-successfully-through-the-job-flow-editing-job-script-page-but-an-error-is-reported-through-job-flow-execution" title="Direct link to heading">#</a></h2><p>Error message:</p><div class="codeBlockContainer_J+bg"><div class="codeBlockContent_csEI"><pre tabindex="0" class="prism-code language-undefined codeBlock_rtdJ thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#F8F8F2"><span class="token plain">You can go to this path(/opt/kepler/work/engine/hadoop/workDir/9c28976e-63ba-4d9d-b85e-b37d84144596/logs) to find the reason or ask the administrator for help ,ip: host1 ,port: 9101 ,serviceKind: linkis-cg-linkismanager ,ip: host1 ,port: 9104 ,serviceKind: linkis-cg-entrance</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Exception in thread &quot;main&quot; java.lang.NullPointerException</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    at com.webank.wedatasphere.linkis.rpc.sender.SpringCloudFeignConfigurationCache$.getClient(SpringCloudFeignConfigurationCache.scala:73)</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    at com.webank.wedatasphere.linkis.rpc.sender.SpringMVCRPCSender.doBuilder(SpringMVCRPCSender.scala:49)</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=250m; support was removed in 8.0</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Exception in thread &quot;main&quot; java.lang.NullPointerException</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    at com.webank.wedatasphere.linkis.rpc.sender.SpringCloudFeignConfigurationCache$.getClient(SpringCloudFeignConfigurationCache.scala:73)</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    at com.webank.wedatasphere.linkis.rpc.sender.SpringMVCRPCSender.doBuilder(SpringMVCRPCSender.scala:49)</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    at com.webank.wedatasphere.linkis.rpc.BaseRPCSender.newRPC(BaseRPCSender.scala:67)</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    at com.webank.wedatasphere.linkis.rpc.BaseRPCSender.com$webank$wedatasphere$linkis$rpc$BaseRPCSender$$getRPC(BaseRPCSender.scala:54)</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    at com.webank.wedatasphere.linkis.rpc.BaseRPCSender.send(BaseRPCSender.scala:105)</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    at com.webank.wedatasphere.linkis.engineconn.callback.service.AbstractEngineConnStartUpCallback.callback(EngineConnCallback.scala:39)</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    at com.webank.wedatasphere.linkis.engineconn.callback.hook.CallbackEngineConnHook.afterEngineServerStartFailed(CallbackEngineConnHook.scala:63)</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    at com.webank.wedatasphere.linkis.engineconn.launch.EngineConnServer$$anonfun$main$15.apply(EngineConnServer.scala:64)</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    at com.webank.wedatasphere.linkis.engineconn.launch.EngineConnServer$$anonfun$main$15.apply(EngineConnServer.scala:64)</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    at com.webank.wedatasphere.linkis.engineconn.launch.EngineConnServer$.main(EngineConnServer.scala:64)</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    at com.webank.wedatasphere.linkis.engineconn.launch.EngineConnServer.main(EngineConnServer.scala)</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><p>Solution: The conf in the /opt/kepler/work/engine/hadoop/workDir/9c28976e-63ba-4d9d-b85e-b37d84144596 directory is empty. lib and conf are detected by the system (linkis/lib/linkis-engineconn-plugins/python) when the microservice starts, and the python engine material package zip changes, and are automatically uploaded to the engine/engineConnPublickDir/ directory. The temporary solution to the problem is to copy the lib and conf content under linkis/lib/linkis-engineconn-plugins/python to the corresponding directory of engine/engineConnPublicDir/ (that is, the external link referenced in workDir/9c28976e-63ba-4d9d-b85e-b37d84144596 Under contents. The official solution needs to solve the problem that the change of the material package cannot be successfully uploaded to engineConnPublicDir.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q24-after-installing-exchangis050-click-on-the-dss-menu-to-enter-a-new-page-and-prompt-sorry-page-not-found-f12-view-has-404-exception"></a>Q24: After installing Exchangis0.5.0, click on the dss menu to enter a new page and prompt &quot;Sorry, Page Not Found&quot;. F12 view has 404 exception<a class="hash-link" href="#q24-after-installing-exchangis050-click-on-the-dss-menu-to-enter-a-new-page-and-prompt-sorry-page-not-found-f12-view-has-404-exception" title="Direct link to heading">#</a></h2><p>Error message: F12 view vue.runtime.esm.js:6785 GET <a href="http://10.0.xx.xx:29008/udes/auth?redirect=http%3A%2F%2F10.0.xx.xx%3A29008&amp;dssurl=" target="_blank" rel="noopener noreferrer">http://10.0.xx.xx:29008/udes/auth?redirect=http%3A%2F%2F10.0.xx.xx%3A29008&amp;dssurl=</a> http%3A%2F%2F10.0.xx.xx%3A8088&amp;cookies=bdp-user-ticket-id%3DM7UZXQP9Ld1xeftV5DUGYeHdOc9oAFgW2HLiVea4FcQ%3D%3B%20workspaceId%3D225 404 (Not Found)</p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q25-there-is-an-infinite-loop-in-configuring-atlas-in-hive-resulting-in-stack-overflow"></a>Q25: There is an infinite loop in configuring atlas in HIVE, resulting in stack overflow<a class="hash-link" href="#q25-there-is-an-infinite-loop-in-configuring-atlas-in-hive-resulting-in-stack-overflow" title="Direct link to heading">#</a></h2><p>You need to add all the content jar packages and subdirectories under ${ATLAS_HOME}/atlas/hook/hive/ to the lib directory of hive engine, otherwise AtlasPluginClassLoader cannot find the correct implementation class and finds the one under hive-bridge-shim class, resulting in an infinite loop
However, the current execution method of Linkis (1.0.2) does not support subdirectories under lib, and the code needs to be modified. Refer to:
<a href="https://github.com/apache/incubator-linkis/pull/1058" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-linkis/pull/1058</a></p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q26-linkis10x-compiles-based-on-spark3-hadoop3-hive3-or-hdp314-please-refer-to"></a>Q26: Linkis1.0.X compiles based on spark3 hadoop3 hive3 or hdp3.1.4, please refer to:<a class="hash-link" href="#q26-linkis10x-compiles-based-on-spark3-hadoop3-hive3-or-hdp314-please-refer-to" title="Direct link to heading">#</a></h2><p><a href="https://github.com/lordk911/Linkis/commits/master" target="_blank" rel="noopener noreferrer">https://github.com/lordk911/Linkis/commits/master</a>
After compiling, please recompile DSS according to the compiled package, keep the same version of scala, and use the family bucket for web modules</p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q27-linkis-cannot-get-user-name-when-executing-jdbc-task"></a>Q27 Linkis cannot get user name when executing jdbc task<a class="hash-link" href="#q27-linkis-cannot-get-user-name-when-executing-jdbc-task" title="Direct link to heading">#</a></h2><p>2021-10-31 05:16:54.016 ERROR Task is Failed,errorMsg: NullPointerException: jdbc.username cannot be null.
Source code: com.webank.wedatasphere.linkis.manager.engineplugin.jdbc.executer.JDBCEngineConnExecutor Received val properties = engineExecutorContext.getProperties.asInstanceOf[util.Map[String, String]] No jdbc.username parameter</p><p>Solution 1:
Documentation: Temporary fixes for JDBC issues.note
Link: <a href="http://note.youdao.com/noteshare?id=08163f429dd2e226a13877eba8bad1e3&amp;sub=4ADEE86F433B4A59BBB20621A1C4B2AE" target="_blank" rel="noopener noreferrer">http://note.youdao.com/noteshare?id=08163f429dd2e226a13877eba8bad1e3&amp;sub=4ADEE86F433B4A59BBB20621A1C4B2AE</a>
Solution 2: Compare and modify this file
<a href="https://github.com/apache/incubator-linkis/blob/319213793881b0329022cf4137ee8d4c502395c7/linkis-engineconn-plugins/engineconn-plugins/jdbc/src/main/scala/com/webank/wedatasphere/linkis/manager/engineplugin/jdbc/" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-linkis/blob/319213793881b0329022cf4137ee8d4c502395c7/linkis-engineconn-plugins/engineconn-plugins/jdbc/src/main/scala/com/webank/wedatasphere/linkis/manager/engineplugin/jdbc/</a> executor/JDBCEngineConnExecutor.scala</p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q28-after-changing-the-hive-version-in-the-configuration-before-installation-the-configuration-of-the-management-console-still-shows-the-version-as-233"></a>Q28: After changing the hive version in the configuration before installation, the configuration of the management console still shows the version as 2.3.3<a class="hash-link" href="#q28-after-changing-the-hive-version-in-the-configuration-before-installation-the-configuration-of-the-management-console-still-shows-the-version-as-233" title="Direct link to heading">#</a></h2><p><img src="/assets/images/q50_1-da3747bd98c7849f807cf006c80fc4d0.png"></p><p>Solution 1: It is determined that there is a bug in the install.sh script, please change the hive-1.2.1 here to hive-2.3.3 and reinstall it.
Solution 2: If you don’t want to reinstall, you need to change all the values ​​of hive-2.3.3 in the label_value in the linkis_cg_manager_label table to the desired hive version
Note: You are welcome to submit a PR to the github Linkis project to fix this problem, and then let us know, we will review and merge it into the code as soon as possible (currently unfixed, Deadline November 30, 2021)</p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q29-when-linkis-cli-submits-a-task-it-prompts-group-by-clause-sql_modeonly_full_group_by-error"></a>Q29: When linkis-cli submits a task, it prompts GROUP BY clause; sql_mode=only_full_group_by error<a class="hash-link" href="#q29-when-linkis-cli-submits-a-task-it-prompts-group-by-clause-sql_modeonly_full_group_by-error" title="Direct link to heading">#</a></h2><div class="codeBlockContainer_J+bg"><div class="codeBlockContent_csEI"><pre tabindex="0" class="prism-code language-undefined codeBlock_rtdJ thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#F8F8F2"><span class="token plain">_8_codeExec_8 com.webank.wedatasphere.linkis.orchestrator.ecm.exception.ECMPluginErrorException: errCode: 12003 ,desc: uathadoop01:9101_8 Failed  to async get EngineNode MySQLSyntaxErrorException: Expression #6 of SELECT list is not in GROUP BY clause and contains nonaggregated column &#x27;dss_linkis.si.name&#x27; which is not functionally dependent on columns in GROUP BY clause; this is incompatible with sql_mode=only_full_group_by ,ip: uathadoop01 ,port: 9104 ,serviceKind: linkis-cg-entrance</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><p>Reason: This error occurs in mysql version 5.7 and above. Problem: Because the configuration strictly implements the &quot;SQL92 standard&quot;, the solution: enter the /etc/mysql directory to modify the my.cnf file and add code under [mysqld]:
sql_mode = STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION</p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q30-an-error-is-reported-when-the-flink-engine-starts-and-the-tokencache-is-found"></a>Q30: An error is reported when the flink engine starts and the TokenCache is found<a class="hash-link" href="#q30-an-error-is-reported-when-the-flink-engine-starts-and-the-tokencache-is-found" title="Direct link to heading">#</a></h2><p>ERROR [main] com.webank.wedatasphere.linkis.engineconn.computation.executor.hook.ComputationEngineConnHook 57 error - EngineConnSever start failed! now exit. java.lang.NoClassDefFoundError: org/apache/hadoop/mapreduce/security/TokenCache
Reason: The jar package hadoop-mapreduce-client-core.jar is missing under flink-enginecon lib, just copy a copy from hadoop lib.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q31-when-starting-the-flink-enginespark-engine-the-engine-entrance-reports-an-error-orgjson4sjsonastjnothing-cannot-be-cast-to-orgjson4sjsonastjstring"></a>Q31: When starting the flink engine/spark engine, the engine-entrance reports an error org.json4s.JsonAST$JNothing$ cannot be cast to org.json4s.JsonAST$JString<a class="hash-link" href="#q31-when-starting-the-flink-enginespark-engine-the-engine-entrance-reports-an-error-orgjson4sjsonastjnothing-cannot-be-cast-to-orgjson4sjsonastjstring" title="Direct link to heading">#</a></h2><p>The reason is that linkis-manager reports an error in the yarn queue to obtain an exception
Solution: Modify the linkis_cg_rm_external_resource_provider table and modify the yarn queue information corresponding to config</p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q32-classnotfoundexception-is-reported-when-the-function-script-is-executed"></a>Q32: ClassNotFoundException is reported when the function script is executed<a class="hash-link" href="#q32-classnotfoundexception-is-reported-when-the-function-script-is-executed" title="Direct link to heading">#</a></h2><p><img src="/assets/images/q55_1-dcb8abee478065f48183c591ca54f431.png"></p><p>Reason: linkis creates a function by adding the path of the function to the classPath, and then executing create temporary function. . . Statement, when submitting tasks to the yarn cluster in this way, the jar package of the function will not be uploaded to hdfs, resulting in class loading failure!</p><p>Solution: Modify the method of generating function statements, or use HiveAddJarsEngineHook to solve it. Here, modify the constructCode method of JarUdfEngineHook. After packaging replace all</p><div class="codeBlockContainer_J+bg"><div class="codeBlockContent_csEI"><pre tabindex="0" class="prism-code language-undefined codeBlock_rtdJ thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#F8F8F2"><span class="token plain">  override protected def constructCode(udfInfo: UDFInfo): String = {</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    &quot;%sql\n&quot; + &quot;add jar &quot; + udfInfo.getPath + &quot;\n%sql\n&quot; + udfInfo.getRegisterFormat</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  }</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q33-failed-to-start-bean-webserverstartstop-when-linkis-executes-spark-task-in-cdh-environment"></a>Q33: Failed to start bean &#x27;webServerStartStop when Linkis executes Spark task in CDH environment<a class="hash-link" href="#q33-failed-to-start-bean-webserverstartstop-when-linkis-executes-spark-task-in-cdh-environment" title="Direct link to heading">#</a></h2><p>Detailed log:</p><div class="codeBlockContainer_J+bg"><div class="codeBlockContent_csEI shell"><pre tabindex="0" class="prism-code language-shell codeBlock_rtdJ thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#F8F8F2"><span class="token plain">Caused by: java.lang.IllegalStateException</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">at org.eclipse.jetty.servlet.ServletHolder.setClassFrom</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">ServletHolder.java:300</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> ~</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">jetty-servlet-9.4.48.v20220622.jar:9.4.48.v20220622</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">at org.eclipse.jetty.servlet.ServletHolder.doStart</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">ServletHolder.java:347</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> ~</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">jetty-servlet-9.4.48.v20220622.jar:9.4.48.v20220622</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">at org.eclipse.jetty.util.component.AbstractLifeCycle.start</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">AbstractLifeCycle.java:73</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> ~</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">jetty-util-9.4.48.v20220622.jar:9.4.48.v20220622</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">at org.eclipse.jetty.servlet.ServletHandler.lambda</span><span class="token variable" style="color:rgb(189, 147, 249);font-style:italic">$initialize</span><span class="token variable" style="color:rgb(189, 147, 249);font-style:italic">$0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">ServletHandler.java:749</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> ~</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">jetty-servlet-9.4.48.v20220622.jar:9.4.48.v20220622</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">at java.util.stream.SortedOps</span><span class="token variable" style="color:rgb(189, 147, 249);font-style:italic">$SizedRefSortingSink</span><span class="token plain">.end</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">SortedOps.java:357</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> ~</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">?:1.8.0_292</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">at java.util.stream.AbstractPipeline.copyInto</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">AbstractPipeline.java:483</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> ~</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">?:1.8.0_292</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">at java.util.stream.AbstractPipeline.wrapAndCopyInto</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">AbstractPipeline.java:472</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> ~</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">?:1.8.0_292</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">at java.util.stream.StreamSpliterators</span><span class="token variable" style="color:rgb(189, 147, 249);font-style:italic">$WrappingSpliterator</span><span class="token plain">.forEachRemaining</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">StreamSpliterators.java:313</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> ~</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">?:1.8.0_292</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">at java.util.stream.Streams</span><span class="token variable" style="color:rgb(189, 147, 249);font-style:italic">$ConcatSpliterator</span><span class="token plain">.forEachRemaining</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">Streams.java:743</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> ~</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">?:1.8.0_292</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">at java.util.stream.ReferencePipeline</span><span class="token variable" style="color:rgb(189, 147, 249);font-style:italic">$Head</span><span class="token plain">.forEach</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">ReferencePipeline.java:647</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> ~</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">?:1.8.0_292</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">at org.eclipse.jetty.servlet.ServletHandler.initialize</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">ServletHandler.java:774</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> ~</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">jetty-servlet-9.4.48.v20220622.jar:9.4.48.v20220622</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">at org.springframework.boot.web.embedded.jetty.JettyEmbeddedWebAppContext</span><span class="token variable" style="color:rgb(189, 147, 249);font-style:italic">$JettyEmbeddedServletHandler</span><span class="token plain">.deferredInitialize</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">JettyEmbeddedWebAppContext.java:46</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> ~</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">spring-boot-2.3.12.RELEASE.jar:2.3.12.RELEASE</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">at</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><p>Reason: This is due to the conflict between the classPath and Linkis that CDH-Spark depends on at the bottom.
Solution: On the machine where linkis is deployed, you can check the classPath in spark-env.sh, comment it out, and run it again. For details, please refer to <a href="https://github.com/apache/incubator-linkis/issues/3282" target="_blank" rel="noopener noreferrer">3282</a></p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="q34-an-error-is-reported-when-running-the-flink-task-failed-to-create-engineconnplugin-comwebankwedataspherelinkisenginepluginhivehiveengineconnpluginjavalangclassnotfoundexception-comwebankwedataspherelinkisenginepluginhivehiveengineconnplugin"></a>Q34: An error is reported when running the flink task: Failed to create engineConnPlugin: com.webank.wedatasphere.linkis.engineplugin.hive.HiveEngineConnPluginjava.lang.ClassNotFoundException: com.webank.wedatasphere.linkis.engineplugin.hive.HiveEngineConnPlugin<a class="hash-link" href="#q34-an-error-is-reported-when-running-the-flink-task-failed-to-create-engineconnplugin-comwebankwedataspherelinkisenginepluginhivehiveengineconnpluginjavalangclassnotfoundexception-comwebankwedataspherelinkisenginepluginhivehiveengineconnplugin" title="Direct link to heading">#</a></h2><p><img src="/assets/images/q53_1-e521f8846a09ef0f3e8bf245174ca474.png"></p><p>Reason: The configuration file in the conf in the flink engine directory is empty, and the default configuration is read (by default, the hived engine configuration is read), delete the conf about flink in the configuration table and restart ecp</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/apache/incubator-linkis-website/edit/dev/faq/main.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_mS5F" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_mt2f"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"></div><div class="pagination-nav__item pagination-nav__item--next"></div></nav></div></div><div class="col col--3"><div class="tableOfContents_vrFS thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#q1-the-ps-cs-service-log-of-linkis-reports-this-error-figservletwebserverapplicationcontext-559" class="table-of-contents__link">Q1: The ps-cs service log of linkis reports this error: figServletWebServerApplicationContext (559)</a></li><li><a href="#q2-linkis-eureka-debugging-instructions" class="table-of-contents__link">Q2: Linkis-eureka debugging instructions</a></li><li><a href="#q3-eureka-automatically-stops-when-it-starts-for-the-first-time-and-needs-to-be-restarted-manually" class="table-of-contents__link">Q3: eureka automatically stops when it starts for the first time, and needs to be restarted manually</a></li><li><a href="#q4-linkis-entrance-logwriter-missing-dependencies" class="table-of-contents__link">Q4: Linkis Entrance LogWriter missing dependencies</a></li><li><a href="#q5-when-linkis10-executes-tasks-the-ecp-service-throws-the-following-error-caused-by-javautilnosuchelementexception-noneget" class="table-of-contents__link">Q5: When Linkis1.0 executes tasks, the ECP service throws the following error: Caused by: java.util.NoSuchElementException: None.get?</a></li><li><a href="#q6-linkis1x-general-troubleshooting-method-for-insufficient-resources" class="table-of-contents__link">Q6: Linkis1.X general troubleshooting method for insufficient resources</a></li><li><a href="#q7-linkis-startup-error-nosuchmethoderrorgetsessionmanagerlorgeclipsejettyserversessionmanager" class="table-of-contents__link">Q7: linkis startup error: NoSuchMethodErrorgetSessionManager()Lorg/eclipse/jetty/server/SessionManager</a></li><li><a href="#q1-linkis10-execution-task-report-select-list-is-not-in-group-by-clause" class="table-of-contents__link">Q1: Linkis1.0 execution task report: select list is not in group by clause</a></li><li><a href="#q2-when-executing-scripts-after-deployment-executing-commands-and-collecting-results-i-encountered-such-an-error-ioexception-file-header-type-must-be-dolphin" class="table-of-contents__link">Q2: When executing scripts after deployment, executing commands, and collecting results, I encountered such an error, IOException: File header type must be dolphin:</a></li><li><a href="#q3-the-json4s-package-conflict-caused-by-inconsistent-spark-versions-the-error-is-as-follows-error-message-caused-by-javalangnosuchmethoderror-orgjson4sjacksonjsonmethod" class="table-of-contents__link">Q3: The json4s package conflict caused by inconsistent Spark versions, the error is as follows: Error message: caused by: java.lang.NoSuchMethodError: org.json4s.jackson.jsonMethod$</a></li><li><a href="#q4-when-linkis1x-submits-spark-sql-tasks-in-version-cdh5161-how-to-troubleshoot-404-problems" class="table-of-contents__link">Q4: When Linkis1.X submits spark sql tasks in version CDH5.16.1, how to troubleshoot 404 problems</a></li><li><a href="#q5-running-error-report-missing-package-matplotlib" class="table-of-contents__link">Q5: running error report missing package matplotlib</a></li><li><a href="#q6-when-the-microservice-linkis-ps-cs-is-started-debuggclasswriter-overrides-final-method-visit-is-reported" class="table-of-contents__link">Q6: When the microservice linkis-ps-cs is started, DebuggClassWriter overrides final method visit is reported</a></li><li><a href="#q7-when-the-shell-engine-schedules-execution-the-engine-execution-directory-reports-the-following-error-binjava-no-such-file-or-directory" class="table-of-contents__link">Q7: When the shell engine schedules execution, the engine execution directory reports the following error /bin/java: No such file or directory:</a></li><li><a href="#q8-when-the-hive-engine-is-scheduled-the-error-log-of-engineconnmanager-is-as-follows-method-did-not-exist-sessionhandler" class="table-of-contents__link">Q8: When the hive engine is scheduled, the error log of engineConnManager is as follows: method did not exist: SessionHandler:</a></li><li><a href="#q9-when-the-hive-engine-is-executing-the-following-error-is-reported-lcomgooglecommoncollectunmodifiableiterator" class="table-of-contents__link">Q9: When the hive engine is executing, the following error is reported Lcom/google/common/collect/UnmodifiableIterator:</a></li><li><a href="#q10-during-engine-scheduling-the-following-error-is-reported-python-processes-is-not-alive" class="table-of-contents__link">Q10: During engine scheduling, the following error is reported: Python processes is not alive:</a></li><li><a href="#q11-when-the-spark-engine-is-running-the-following-error-is-reported-noclassdeffounderror-orgapachehadoophiveqlioorcorcfile" class="table-of-contents__link">Q11: When the spark engine is running, the following error is reported: NoClassDefFoundError: org/apache/hadoop/hive/ql/io/orc/OrcFile:</a></li><li><a href="#q1-the-database-on-the-left-side-of-the-script-cannot-be-refreshed" class="table-of-contents__link">Q1: The database on the left side of the script cannot be refreshed</a></li><li><a href="#q2-the-right-side-of-scriptis-cannot-refresh-the-database-and-it-has-been-refreshing-it-should-be-noted-that-the-metadata-of-linkis-does-not-support-docking-sentry-and-ranger-for-the-time-being-only-supports-native-permission-control-of-hive-error-message-the-front-end-database-tab-has-been-refreshing-state" class="table-of-contents__link">Q2: The right side of Scriptis cannot refresh the database, and it has been refreshing (it should be noted that the metadata of linkis does not support docking sentry and Ranger for the time being, only supports native permission control of hive), error message: the front-end database tab has been refreshing state</a></li><li><a href="#q3-scriptisworkspace-when-i-log-in-to-scriptis-the-root-directory-does-not-exist-and-there-are-two-root-directories-workspace-and-hdfs-error-message-after-logging-in-the-front-end-pops-up-the-following-message-the-users-local-directory-does-not-exist-please-contact-admin-to-add" class="table-of-contents__link">Q3: [Scriptis]Workspace When I log in to Scriptis, the root directory does not exist, and there are two root directories: workspace and HDFS: Error message: After logging in, the front end pops up the following message (the user’s local directory does not exist, please Contact admin to add)</a></li><li><a href="#q4-management-consolesettings-how-to-adjust-the-yarn-queue-used-by-the-task-error-message-when-executing-the-sql-task-it-is-reported-that-1-obtaining-the-yarn-queue-information-is-abnormal-or-user-xx-cannot-submit-to-the-queue" class="table-of-contents__link">Q4: [Management Console]Settings How to adjust the yarn queue used by the task? Error message: When executing the sql task, it is reported that 1. Obtaining the Yarn queue information is abnormal or user XX cannot submit to the queue</a></li><li><a href="#q5-when-hive-queries-it-reports-can-not-find-zk-related-classes-such-as-orgapachecurator-error-message-when-executing-hive-tasks-the-log-report-cannot-find-the-class-starting-with-orgapachecurator--classnotfound" class="table-of-contents__link">Q5: When Hive queries, it reports: Can not find zk-related classes such as: org.apache.curator.<em>, error message: When executing hive tasks, the log report cannot find the class starting with org.apache.curator.</em> , classNotFound</a></li><li><a href="#q6-how-does-linkis-support-kerberos" class="table-of-contents__link">Q6: How does Linkis support kerberos</a></li><li><a href="#q7-regarding-linkis-besides-supporting-deployment-user-login-can-other-user-logins-be-configured" class="table-of-contents__link">Q7: Regarding Linkis, besides supporting deployment user login, can other user logins be configured?</a></li><li><a href="#q8-how-to-open-linkis-management-console-administrator-page-ecm-and-microservice-management" class="table-of-contents__link">Q8: How to open Linkis management console, administrator page ECM and microservice management?</a></li><li><a href="#q9-when-starting-the-microservice-linkis-ps-publicservice-kjdbcutilsgetdriverclassname-npe" class="table-of-contents__link">Q9: When starting the microservice linkis-ps-publicservice, kJdbcUtils.getDriverClassName NPE</a></li><li><a href="#q10-when-scheduling-the-hive-engine-the-following-error-is-reported-engineconnpluginnotfoundexception-errorcode-70063" class="table-of-contents__link">Q10: When scheduling the hive engine, the following error is reported: EngineConnPluginNotFoundException: errorCode: 70063</a></li><li><a href="#q11-when-the-hive-engine-schedules-execution-the-following-error-is-reported-opertion-failed-nullpointerexception" class="table-of-contents__link">Q11: When the hive engine schedules execution, the following error is reported: opertion failed NullPointerException:</a></li><li><a href="#q12-when-the-spark-engine-starts-an-error-is-reported-get-the-queue-information-excepiton-obtaining-yarn-queue-information-exception-and-http-link-exception" class="table-of-contents__link">Q12: When the spark engine starts, an error is reported get the queue information excepiton. (obtaining Yarn queue information exception) and http link exception</a></li><li><a href="#q13-pythonspark-scheduling-execution-error-initialize-python-executor-failed-classnotfoundexception-orgslf4jimplstaticloggerbinder" class="table-of-contents__link">Q13: pythonspark scheduling execution, error: initialize python executor failed ClassNotFoundException org.slf4j.impl.StaticLoggerBinder</a></li><li><a href="#q14-common-package-conflicts" class="table-of-contents__link">Q14: Common package conflicts:</a></li><li><a href="#q15-the-mysql-script-running-scripts-reports-an-errorsql-engine-reports-an-error" class="table-of-contents__link">Q15: The Mysql script running Scripts reports an errorsql engine reports an error</a></li><li><a href="#q16-the-waiting-time-for-scriptis-to-execute-the-script-is-long" class="table-of-contents__link">Q16: The waiting time for scriptis to execute the script is long</a></li><li><a href="#q17-scriptis-executes-jdbc-script-and-reports-an-error" class="table-of-contents__link">Q17: scriptis executes jdbc script and reports an error</a></li><li><a href="#q18-turn-off-resource-checking" class="table-of-contents__link">Q18: Turn off resource checking</a></li><li><a href="#q19-executing-the-script-reports-an-error" class="table-of-contents__link">Q19: Executing the script reports an error</a></li><li><a href="#q20-scriptis-execute-script-timeoutexception" class="table-of-contents__link">Q20: ScriptIs execute script TimeoutException</a></li><li><a href="#q21-engine-timeout-setting" class="table-of-contents__link">Q21: Engine timeout setting</a></li><li><a href="#q22-when-creating-a-new-workflow-it-prompts-504-gateway-time-out" class="table-of-contents__link">Q22: When creating a new workflow, it prompts &quot;504 Gateway Time-out&quot;</a></li><li><a href="#q23-scripts-execute-python-scripts-the-content-of-the-script-is-very-simple-print-and-execute-successfully-normally-it-can-also-be-executed-successfully-through-the-task-scheduling-system-it-can-also-be-executed-successfully-through-the-job-flow-editing-job-script-page-but-an-error-is-reported-through-job-flow-execution" class="table-of-contents__link">Q23: Scripts execute python scripts (the content of the script is very simple print) and execute successfully normally. It can also be executed successfully through the task scheduling system. It can also be executed successfully through the job flow editing job script page, but an error is reported through job flow execution.</a></li><li><a href="#q24-after-installing-exchangis050-click-on-the-dss-menu-to-enter-a-new-page-and-prompt-sorry-page-not-found-f12-view-has-404-exception" class="table-of-contents__link">Q24: After installing Exchangis0.5.0, click on the dss menu to enter a new page and prompt &quot;Sorry, Page Not Found&quot;. F12 view has 404 exception</a></li><li><a href="#q25-there-is-an-infinite-loop-in-configuring-atlas-in-hive-resulting-in-stack-overflow" class="table-of-contents__link">Q25: There is an infinite loop in configuring atlas in HIVE, resulting in stack overflow</a></li><li><a href="#q26-linkis10x-compiles-based-on-spark3-hadoop3-hive3-or-hdp314-please-refer-to" class="table-of-contents__link">Q26: Linkis1.0.X compiles based on spark3 hadoop3 hive3 or hdp3.1.4, please refer to:</a></li><li><a href="#q27-linkis-cannot-get-user-name-when-executing-jdbc-task" class="table-of-contents__link">Q27 Linkis cannot get user name when executing jdbc task</a></li><li><a href="#q28-after-changing-the-hive-version-in-the-configuration-before-installation-the-configuration-of-the-management-console-still-shows-the-version-as-233" class="table-of-contents__link">Q28: After changing the hive version in the configuration before installation, the configuration of the management console still shows the version as 2.3.3</a></li><li><a href="#q29-when-linkis-cli-submits-a-task-it-prompts-group-by-clause-sql_modeonly_full_group_by-error" class="table-of-contents__link">Q29: When linkis-cli submits a task, it prompts GROUP BY clause; sql_mode=only_full_group_by error</a></li><li><a href="#q30-an-error-is-reported-when-the-flink-engine-starts-and-the-tokencache-is-found" class="table-of-contents__link">Q30: An error is reported when the flink engine starts and the TokenCache is found</a></li><li><a href="#q31-when-starting-the-flink-enginespark-engine-the-engine-entrance-reports-an-error-orgjson4sjsonastjnothing-cannot-be-cast-to-orgjson4sjsonastjstring" class="table-of-contents__link">Q31: When starting the flink engine/spark engine, the engine-entrance reports an error org.json4s.JsonAST$JNothing$ cannot be cast to org.json4s.JsonAST$JString</a></li><li><a href="#q32-classnotfoundexception-is-reported-when-the-function-script-is-executed" class="table-of-contents__link">Q32: ClassNotFoundException is reported when the function script is executed</a></li><li><a href="#q33-failed-to-start-bean-webserverstartstop-when-linkis-executes-spark-task-in-cdh-environment" class="table-of-contents__link">Q33: Failed to start bean &#39;webServerStartStop when Linkis executes Spark task in CDH environment</a></li><li><a href="#q34-an-error-is-reported-when-running-the-flink-task-failed-to-create-engineconnplugin-comwebankwedataspherelinkisenginepluginhivehiveengineconnpluginjavalangclassnotfoundexception-comwebankwedataspherelinkisenginepluginhivehiveengineconnplugin" class="table-of-contents__link">Q34: An error is reported when running the flink task: Failed to create engineConnPlugin: com.webank.wedatasphere.linkis.engineplugin.hive.HiveEngineConnPluginjava.lang.ClassNotFoundException: com.webank.wedatasphere.linkis.engineplugin.hive.HiveEngineConnPlugin</a></li></ul></div></div></div></div></main></div></div><footer class="footer"><div class="container"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Linkis</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/docs/latest/introduction">Document</a></li><li class="footer__item"><a class="footer__link-item" href="/faq/main">FAQ</a></li><li class="footer__item"><a href="https://github.com/apache/incubator-linkis/releases" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Releases<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items"><li class="footer__item"><a href="https://github.com/apache/incubator-linkis" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="https://github.com/apache/incubator-linkis/issues" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Issue Tracker<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="https://github.com/apache/incubator-linkis/pulls" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Pull Requests<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div><div class="col footer__col"><div class="footer__title">Subscribe Mailing List</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/community/how-to-subscribe">How to Subscribe</a></li><li class="footer__item"><a href="mailto:dev-subscribe@linkis.apache.org" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Subscribe Mail<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="https://lists.apache.org/list.html?dev@linkis.apache.org" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Mail Archive<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright"><div><img style="height:50px" alt="Apache Software Foundation" src="/img/incubator-logo.svg"><p style="color: #999999;  padding: 0 20px 30px;font-weight:400;text-align:left">Apache Linkis is an effort undergoing incubation at The Apache Software Foundation (ASF), sponsored by the Apache Incubator. Incubation is required of all newly accepted projects until a further review indicates that the infrastructure, communications, and decision making process have stabilized in a manner consistent with other successful ASF projects. While incubation status is not necessarily a reflection of the completeness or stability of the code, it does indicate that the project has yet to be fully endorsed by the ASF.</p><p></p>
             <p style="padding: 0 20px 30px;color: #999999;font-weight: 400;"> Copyright © 2022 The Apache Software Foundation. Licensed under the Apache License, Version 2.0. Apache Linkis, Apache Incubator, Apache, the Apache feather logo, the Apache Linkis logo and the Apache Incubator project logo are trademarks of The Apache Software Foundation.</p>
             <div></div></div></div></div></div></footer></div>
<script src="/assets/js/runtime~main.c21e8335.js"></script>
<script src="/assets/js/main.1e191043.js"></script>
</body>
</html>